{"paper1":{"title":"The Design of Microwave Planar Power Dividers and Couplers with Distinct Power Division Ratio in Two Different Frequency Bands","authors":["Valeriy Oborzhytskyy","Ivan Prudyus"],"conference":"2016 International Conference Radio Electronics & Info Communications (UkrMiCo)","abstract":"This paper presents the application of equivalent replacement method for the development of dual-band microwave power distribution devices with arbitrary power division ratio in different frequency bands. The expressions allowing to replace with the equivalent circuits of T-type and P-type a transmission line segment having a different values of characteristic impedance and electrical length at different frequencies are received. For the obtaining of such expressions the method based on the input reactances of reduced partial circuits of even-mode and odd-mode excitation is offered for using. Expression for determination of parameters of the loaded line segment which is used for realization of a reactance with two values of resistance at different frequencies is received also. The examples of calculation and simulation of a dual-frequency power divider and rat- race coupler with power division ratio equal to 2 and 1 at frequencies 2 and 3.6 GHz are given.","keywords":["dual band","microstrip divider","rat-race coupler","unequal division of power","equivalent replacement method"],"content":"I. INTRODUCTION\rPerspective direction of development of the modern radio engineering systems and first of all telecommunication systems consists in simultaneous use of several frequency bands according to available standards. During realization of such systems there are problems, related to considerable complication of structure, with growth of overall sizes and cost. The way of their decision consists in application of dual band microwave devices, which provides the set values of operating parameters on central frequencies of two different frequency bands. Much attention is paid at that to design of passive devices of microwave integrated circuits. Relatively narrow frequency bands of telecommunication standards allow to go out from the values of central frequencies fi (i=1, 2) of these bands at the calculations of passive devices and to consider their as the dual-frequency devices. The frequency coefficient which is determined through the relation of frequencies kf=f2\/f1 (f2>f1 as a rule) is one of basic parameters of such device. Microwave devices of a signal distribution such as power dividers, directional couplers and hybrids, belong to the most important components of high-frequency networks. In the majority of famous publications the design methods of such devices with identical working parameters in each of frequency bands are considered. It concerns, for example, the equal [1] and unequal [2, 3, 4] dual-band power dividers, branch-line couplers [5, 6], ring hybrids [5]. The miscellaneous division of power in bands is considered only for different versions of two-branch couplers [7 \u2013 9] and for one option of the rat-race coupler [8]. Therefore it is very important to solve the task of a different distribution of power in different bands of frequencies.\rRealization of dual-frequency devices foresees the use of circuit elements which are dependent upon frequency. Due to the change of electrical parameters of them at transition from the first frequency f1 to the second f2 the necessary operating parameters of device are arrived at. One of approaches to development of dual-frequency devices is based on the use of method of equivalent replacement. By this method the circuit of the similar single-frequency device is chosen and stage-by- stage replacement of its elements dependent upon frequency (basic elements) by new dual-frequency elements is carried out. These elements provide the same parameters, as the basic elements, but only at two frequencies. Such approach is used for power dividers and also for branch-line and rat-race couplers where each quarter wave segment of line is replaced by equivalent segment of the connected lines [3] or by equivalent T-network formed by segment of line with stub in centre [4, 5]. The P-network formed by segment of the line with stubs at the edges is used in [8, 9] for a two-branch-line coupler in process of the equivalent replacement of its basic segments with different values of characteristic impedance for different frequencies, but with identical electrical length, equal only to 900, what creates the restrictions at the design of dual- band devices with arbitrary power division ratios.\rIn work the approach allowing to develop the methods of calculation of various symmetric circuits for the equivalent replacement of a segment of line with different characteristic impedances and different electrical lengths for two different frequencies is offered, the using of which it is shown on the examples of T- and P-network. Such circuits are necessary for design of dual-band devices with arbitrary division of power in different frequency bands. II. DEVELOPMENT OF EQUIVALENT DUAL-FREQUENCY CIRCUITS\rFor effective use of equivalent replacement method it is necessary to have a number of various variants of circuits together with calculation formulas.\rA. Principleofbasicsegmentreplacementwiththecircuit\rThe deduce of expressions for calculation of equivalent circuit is considerably simplified, if it is a reactive symmetric two-port network, what allows to use the method of input impedances of reduced partial circuits received by even-odd excitation [10]. In this case, unlike a well-known approach with equating of matrixes elements (conductivities or resistances) of the basic segment and equivalent circuit, the equating of corresponding input impedances is required. At such equivalent replacement the expressions for input reactances Xe1, Xo1 of partial circuits of even-odd excitation of equivalent circuit at the first operating frequency f1 are equated with the expressions for input reactances Xte1, Xto1 of partial circuits of even-odd excitation of basic segment at this frequency, and the expressions for input reactances Xe2, Xo2 at the second operating frequency f2 are equated with expressions for input reactances Xte2, Xto2, which correspond to frequency f2. Farther the expressions for calculation of two values of electrical parameters of elements of equivalent circuit for frequencies f1 and f2 via parameters of basic segment are deduced from these equalities.\rB. Circuitsforreplacementoftransmissionlinesegment\rBy a basic element depending from frequency in the structure of unequal microstrip devices for distributing of power there is transmission line segment, the values of characteristic impedance and electrical length of which depend on the power division ratio. Each such basic segment is the symmetrical two-port network with different values of characteristic impedance Zti and of electrical length  ti at two frequencies fi (i=1, 2). Its partial circuits of even-mode and odd-mode decomposition are formed by open and short halves of segment. The input reactances of these partial circuits can be obtained as follows:\rXtei=-Zti\/tti, Xtoi=Ztitti, (1) were t =tan(  \/2). The same input reactances have to be provided at partial components of the equivalent circuit.\rEquivalent T-type circuit consists of two transmission line segments with the electrical length  i at frequencies fi and with characteristic impedance Z and also of shunt reactance Xi, included between lines. In this case the expressions for input reactances of partial circuits of even-mode and odd-mode excitations write down as Xei = Z(2Xi +Zti )\/(Z \u22122Xiti ),\rX oi = Zt i , were ti=tan( i). From equating these expressions to expressions (1) the final design equations were derived:\rXi = Z(Zti + Ztitti )\/[2(Ztiti \u2212 Ztti )], A=tan(\u03b8 )\/tan(\u03b8 )=tan(\u03b8 )\/tan(k \u03b8 ),\r(2) (3)\r121f1\rwere A = Z t \/(Z t ), k \u2013 the frequency coefficient. t1 t1 t2 t2 f Here it is accepted that  2=kf 1 from condition that kf=f2\/f1  2\/ 1 where  i are phase constants of line at frequencies fi. The value of  1 may be found from (3) by iterative calculations.\rEquivalent P-type circuit consists of transmission line segment with the electrical length  i at frequencies fi and with characteristic impedance Z and also of two shunt reactances Xi which are included to the edges of line. Expressions for input reactances of partial circuits of even-odd excitations write down as Xei =XiZ\/(Z\u2212Xiti), Xoi =XiZti \/(Xi +Zti),\rwere ti =tan( i\/2). The equating of these expressions to expressions (1) gives two expressions for Xi:\rXi =ZZti \/(Ztiti \u2212Ztti), Xi =ZZtititti \/(Zti \u2212Ztitti). (4) And the equating of expressions for Xi gives the formula for Z: Z=Zti sin(\u03b8ti)\/sin(\u03b8i). (5)\rThe equating at different values of index i of formulas (5) gives the equality:\rA = s i n (\u03b8 1 ) \/ s i n (\u03b8 2 ) = s i n (\u03b8 1 ) \/ s i n ( k f \u03b8 1 ) ,\rwere A=Zt1sin(\u03b8t1)\/[Zt2sin(\u03b8t2)]. As in previous case the\rvalue of  1 may be found from (6) by iterative calculations.\rDual-frequency reactance accepting different values Xi at different frequencies fi enter (one or several) to each of equivalent circuits given above. Such elements may be realized by circuit the input reactance of which is equal to Xi values. This circuit most often is formed by discrete elements, a transmission line stub or connection of line's segments. The electrical parameters depending from frequency are necessary in circuit to provide the different values of input reactance at the changing of frequency. When the line stub (characteristic impedance Zs, electrical length  si) loaded by the Xki reactance is used, its electrical length  s1 may be found from the equality: tan\u03b8 (X \u2212X )(Z +X X ) s1 = 1 k1 s 2 k2 , (7) tan(kf\u03b8s1) (X2 \u2212Xk2)(Zs +X1Xk1) where value Zs is chosen so that to satisfy simultaneously the equality tan\u03b8 =Z (X \u2212X )\/(Z2 +X X ). When Xki=0 s1 s 1 k1 s 1 k1 (the shorted end of stub) then Zs=X1\/tan( s1) and when Xki=  (the open end of stub) then Zs=-X1tan( s1). When the combinations of values X1, X2 give negative value of Zs the stepped impedance section can be used, where the load Xki is the input reactance of second segment of line.\rIII. PROCEDURE OF EQUIVALENT REPLACEMENT AND EXAMPLES OF ITS USING\rProcess of design of a microstrip device by a method of equivalent replacement is carried out in such sequence. 1) The known version of the single-frequency circuit of power distribution device similar to the required device is chosen. 2) Based from preset values of division power ratios ki at frequencies fi the parameters Zti,  ti of all basic line segments in composition of chosen circuit are calculated for both fi by using known methods of single-frequency circuit calculation. 3) The electrical parameters of elements of the chosen equivalent circuit for every basic segment are calculated: the value  1 is found by iteration method from (3) or (6) and after that the values Z and Xi are calculated from (2) or (4) and (5). -10 4) For the chosen variant of load with reactance Xki the segment parameters providing the input reactance Xi are found from (7).The dependences of scattering parameters of symmetrical circuit from the input impedances of its partial components are -30 shown [11], that the change of sign of reactive components of input impedances leads to change of the phase's signs of scattering parameters, but their modules do not change and the distributing of power between outputs of circuit remains the same. Therefore, if to change a sign of  ti at one of frequencies, it causes the change of signs Xtei, Xtoi and leads to other variant of the electrical parameters of equivalent circuit's elements, at which preset operating parameters of device will be provided. The design of unequal Wilkinson power divider and of rat- race ring coupler is considered as the examples of using the offered method. Different values of power division ratios ki=2.0\/1.0 at average frequencies fi=2.0\/3.6 GHz of two bands were preset for these devices. The equivalent replacement by P-type circuits was carried out for power divider. In case of rat-race coupler the equivalent replacement by T-type circuits with change of  t2 sign, that is  t2=- \/2, was used. Fig. 1 shows the scattering parameters received at the power divider simulation. The results of dual-frequency rat-race coupler simulation are shown in Fig. 2. It is visible that the change of a sign  t2 changes on opposite a sign of output signal phase at f2, but division of power corresponds to set. Characteristics of basic single-frequency devices calculated for k1=2.0 at first frequency f1 are shown by dotted line in Figures. Fig. 1. Scattering parameters of power divider. IV. CONCLUSION The received expressions allow to design the dual-band devices with an arbitrary distribution of power in frequency bands. The expressions for other variants of dual-frequency equivalent two-pole circuit which are necessary for replacement of line segment with different values of electrical parameters at two frequencies can be written down by offered way. Fig. 2. Scattering parameters of rat-race coupler.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?tp=&arnumber=7739593","bibtex":"@INPROCEEDINGS{7739593, \rauthor={V. Oborzhytskyy and I. Prudyus}, \rbooktitle={2016 International Conference Radio Electronics Info Communications (UkrMiCo)}, \rtitle={The design of microwave planar power dividers and couplers with distinct power division ratio in two different frequency bands}, \ryear={2016}, \rpages={1-3}, \rkeywords={microwave devices;power dividers;waveguide couplers;characteristic impedance;dual band microwave power distribution devices;equivalent replacement method;even-mode excitation;frequency 2 GHz;frequency 3.6 GHz;microwave couplers;microwave planar power divider design;odd-mode excitation;power division ratio;reduced partial circuits;transmission line segment;Couplers;Dual band;Frequency conversion;Microwave circuits;Microwave integrated circuits;Power dividers;dual band;equivalent replacement method;microstrip divider;rat-race coupler;unequal division of power}, \rdoi={10.1109\/UkrMiCo.2016.7739593}, \rmonth={Sept},}"},"paper2":{"title":"High-Frequency Class-E Power Amplifier with Shunt Filter","authors":["D. G. Makarov","Ju. V. Rassokhina","V. G. Krizhanovski","Grebennikov Andrei"],"conference":"2016 International Conference Radio Electronics & Info Communications (UkrMiCo)","abstract":"In this paper, an experimental investigation of high-frequency Class-E power amplifier with shunt filter is presented. Using of a shunt LC filter in the load network instead of a series LC tank used in conventional Class-E power amplifier allows the maximum operating frequency and second-harmonic suppression level to be increased compared to conventional Class-E power amplifier. The parasitic resistive elements of the load network were taken into account in simulation of Class-E power amplifier with shunt filter and contribution of each parasitic resistive element has been simulated and calculated.","keywords":["Class E","power amplifier","shunt filter","parasitic elements"],"content":"I. INTRODUCTION\rModern wireless communication systems require high operation efficiency and high harmonic suppression level resulting in lower power consumption, size and cost. This can be achieved by using switching-mode Class-E [1] or Class-F [2] power amplifiers (PAs). The single-ended switching-mode power amplifier with a shunt capacitor was introduced and described in details as a Class-E power amplifier in mid-70s and has found widespread application due to its design simplicity and high operation efficiency [1, 3]. There are different configurations of Class-E PA: conventional Class E with shunt capacitance, Class E with finite dc-feed inductance, or Class-E with quarterwave transmission line [4]. Besides, there are many implementations of broadband Class-E PA using reactance-compensation or filtering techniques [5, 6]. All types of Class-E PA are based on common optimum switching conditions regardless of their schematic realization: drain voltage and drain voltage derivative become zero prior to the moment of switch \"ON\". Thus, there is no power dissipation in the switch and efficiency ideally reaches 100%.\rEach type of Class-E PA consists of the active device operating as a switch and the load network that can be designed in different configurations. In this paper, an experimental investigation of high-frequency Class-E PA with shunt filter and lumped-element load network is presented. Actually, the first results for switching-mode Class-E PA with shunt filter were demonstrated in the late 60-s for particular case and based on vacuum tube as an active device [7]. Much more detailed approaches to design Class-E power amplifier with shunt filter including transmission-line implementation and broadband capability within frequency band from 1.7 to 2.7 GHz achieving the output power of about 40.6 dBm and average drain efficiency of 68% were described in [8]. But generally there is a lack of practical implementations of the lumped-element load-networks for Class-E PA with shunt filter.\rThus, the objective of this work is to experimentally test Class-E PA with shunt filter at high frequency with lumped- element load network and to determine main sources of power loss and their contribution to overall efficiency.\rII. CIRCUIT ANALYSIS\rThe circuit schematic of a Class-E PA with shunt filter is shown in Fig. 1. It consists of the switching power MOSFET device VT1, RF choke (Lchoke) that allows only dc current to flow through dc voltage source, shunt capacitance C, series inductance L, dc-blocking capacitor Cb, which is assumed to have relatively high capacitance to present very low impedance at operating frequency, shunt L0C0 filter, series reactance X, and load resistance Rload [8].\rSeries reactance X can be negative \u2013 capacitance, equal to zero, or positive \u2013 inductance, and also can represent some type of parasitic elements of load-network real components. The shunt L0C0 filter is tuned to the fundamental frequency having infinite impedance at this frequency and zero- impedance at harmonics. As in conventional Class-E power amplifier, the shunt capacitance C consists of internal drain- source capacitance of active device and external capacitance.\rIn theoretical analysis, it is assumed that\r- the transistor has zero saturation voltage, zero saturation resistance, infinite off-resistance, and its switching is instantaneous and lossless\r- the series reactance X = 0\r- the shunt capacitance is assumed to be constant\r- the shunt L0C0 filter has zero impedance at harmonics - there is no loss in the circuit except the load Rload\r- 50% duty ratio is used. Fig. 1.\rAs a result of the theoretical analysis, the following design equations for the load-network parameters are obtained:\rV2 Rload   0.4281 dc P\rout\r L R   1.4836  CR   0.261\rL0   R   Q \rwhere Q is the quality factor of shunt L0C0 filter and     2 f is\rthe radian operating frequency [8]. In this case, the optimum switching Class-E conditions are satisfied at operating frequency, and the maximum amplitude of the drain voltage is 3.677 times of dc-supply voltage that is similar to that in conventional Class-E PA. Besides, for each active device with its intrinsic drain-source capacitance Cout, the maximum operating frequency is 1.91 times greater than that for conventional Class-E PA and can be obtained by [8]\rf   0.097 P\rmax outC V2\rThe initial operation conditions are as follow: operating frequency of 13.56 MHz, dc-supply voltage of 12 V, output power of 5 W, and load-network quality factor Q = 5. An active device is power MOSFET IRF520 with its internal output capacitance of about 150 pF and the maximum allowable drain-source voltage of 100 V [9].\r40\r30\r20\r1,0\r0,5\r0 . 1 5 u F   \rC 0   6,0 L0 LR 5,5\rOutput power, W\rDrain voltage, V\rDrain current, A\rDrain efficiency, %\rFig. 2.\rThe values of the load-network elements calculated\raccording to the design equations are written as Rload = 12.3  , L=214.7nH,C=100pF,L0 =28.9nH,C0 =4760pF. The circuit schematic of the load network of Class-E PA with parasitic elements is shown in Fig. 2.\rHere, the parasitic resistances of inductor L and dc- blocking capacitor Cb are presented by Rseries. The parasitic resistances of the elements of shunt L0C0 filter are given by RL0 and RC0, and the parasitic inductance of the load resistor is shown by LR.\rIII. SIMULATION OF AMPLIFIER\rSimulation of Class-E PA with shunt filter was fulfilled using ADS harmonic balance simulator with the switch model for active device having the on-state resistance of 0.15  . The simulated drain voltage and current waveforms at operating frequency are shown in Fig. 3. Fig. 3. Drain voltage and current simulated waveforms.\rAs can be seen, the drain voltage and drain voltage derivative are zero prior to the moment of switch \"ON\", thus demonstrating that the idealized optimum or nominal Class-E switching conditions are satisfied.\rThe output power and drain efficiency versus frequency are shown in Fig. 4. As can be seen, the simulated output power is 4.96 W with 98.8% drain efficiency at operating frequency of 13.56 MHz. Fig. 4.\rSimulated output power and efficiency. \rIV. EXPERIMENTAL INVESTIGATION\rFrom design equation for the series and shunt inductances it follows that their values in Class-E PA with shunt filter are much less than the corresponding values for conventional Class-E PA at the same operating frequency and they are equal to 214.7 nH and 28.9 nH, respectively. In this case, to make design more convenient in implementation and to minimize losses and parasitic elements shown in Fig. 2, special technical approaches were used in experimental prototype under its practical implementation. The experimentally obtained drain and load voltage waveforms are shown in Fig. 5. It can be seen that the drain voltage waveform corresponds to the nominal switching Class-E conditions. Fig. 5.\rExperimental drain and load voltage waveforms. In practical implementation, first the load resistor was made of 40 SMD resistors connected in parallel to minimize the series lead inductance. These resistors are attached to the small-size PCB on one side, while the mica capacitor C0 is connected to the load by wide conductors at the other side. Mica capacitor type is \"KSO\" and its ESR at operating frequency is not more than 0.07   [10]. The inductance L0 represents one turn of silver-coated wire and placed close to C0 between its leads to minimize additional inductances. Hence, the L0C0Rload filter is realized as a separate unit, as shown in Fig. 6, and its parameters can be adjusted individually. L0C0Rload filter construction. The capacitor C is connected directly to the drain of the power MOSFET and inductor L is connected to the same point. Similar to inductor L0 it is made from silver-coated wire as well. The output power and drain efficiency versus frequency are shown in Fig. 7. Fig. 7.\r11,5\r12,0\r12,5\r13,0 Frequency, MHz\r14,0\r14,5\r100 90 80 70 60 50 40 30\r15,0 15,5\r13,5\rExperimentally obtained output power and efficiency versus frequency. At operating frequency of 13.56 MHz this Class-E PA delivers 3.33 W to the load with 92.5% drain efficiency. As can be seen, the measured output power is slightly less than expected from simulation, which means that the simulation using simplified nonlinear model of MOSFET IRF520 cannot properly describe the behavior of nonlinear capacitances [9].\rIn addition, the parasitic resistances of shunt filter and inductance LR are included in PA simulation. Because the inductor L0 consists of one turn only, its parasitic resistance was not taken into account, and the parasitic resistance Rseries of inductor L is calculated to be 0.08  . As for parasitic inductance LR, it can be estimated to be equal to 20 nH due to technical features of experimental prototype. The simulation results of the output power and drain efficiency versus frequency are shown in Fig. 8. It can be seen that these dependencies remain almost the same although with slightly lower values. Fig. 8. Simulated output power and drain efficiency using MOSFET model and lossy elements. In this case, Class-E PA delivers 3.7 W of output power with 85% drain efficiency.\rNext step is to calculate loss contribution of each parasitic resistance RC0 and Rseries. The loss due to RL0 was not taken into account because of very low inductance L0, with estimation that resistance is lower than 0.01  . The dissipation power in each resistor was simulated by measuring rms current flowing through each resistor according to\rP RI2 diss diss RMS\rAs the result, the final values of the dissipation powers are as follow: PRC0 = 0.536 W and PRseries = 0.032 W. It can be seen that, to minimize power losses in Class-E PA with shunt filter, the capacitor C0 should have extremely low tg .\rThus, it can be concluded that the series parasitic resistances of shunt L0C0 filter significantly decrease the output power and efficiency of Class-E PA with shunt filter and it becomes difficult to implement it at high frequencies using lumped-element load network. However, as an advantage, the simulation has shown 50 dB of second harmonic suppression at the load. Consequently, it could be useful to further investigate the lumped-element Class-E PA with shunt filter involving the parasitic reactive elements in its schematic.\rThe next step is to investigate power amplifier behavior with nonzero reactance X shown at Fig. 1, which can be negative or positive and represent capacitive or inductive part of load impedance. This can be parasitic parameters of load or reactive load like antenna. Several capacitors and inductors were put in experimental prototype schematic and their values were chosen to obtain X impedance from 0 to 2R for inductors and from -2R to 0 for capacitances. Then output power and drain efficiency were measured at central frequency of 13.56 MHz versus X\/R ratio. Also simulation of such dependences were obtained using ADS. These dependences are shown at Fig. 9.\rAs can be seen experimentally obtained and simulated data have similar dependences, except simulated output power in capacitive X part. It can be explained with using of ideal switch and ideal passive elements models without resistive losses in simulation.\rThus the range of reactance X in which there is none of significant amplifier characteristics decreasing is determined and it is approximately from -0.5R to 0.5R. So in the case of reactance X presence we have another one parameter that can be included in theoretical analysis of such scheme and can be deeply investigated.\rV. CONCLUSION\rIn this paper, the high-frequency Class-E power amplifier with shunt filter was designed, simulated, and experimentally tested. Special design features were used to minimize the values of the parasitic resistive parameters of the lumped- element load network. Experimental prototype of Class-E PA with shunt filter demonstrated 3.33 W of output power with 92.5% drain efficiency at 13.56 MHz. It is shown in simulation that even relatively small values of the parasitic resistive elements significantly decrease the quality factor of shunt L0C0 filter and entire characteristics of PA. In this case, the main source of losses is the capacitor of the shunt L0C0 tank and it is very important to choose this capacitor with as lower tg  as possible. Experimental investigation with reactance X 0 was held and the range of X\/R values in which characteristics of amplifier don\u2019t decrease was determined. So further investigation can include effects of nonzero X in theoretical analysis of class-E power amplifier with shunt filter. Fig. 9.\rSimulated and measured output power and drain efficiency versus X\/R ratio.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7739594","bibtex":"@INPROCEEDINGS{7739594, \rauthor={D. G. Makarov and J. V. Rassokhina and V. G. Krizhanovski and G. Andrei}, \rbooktitle={2016 International Conference Radio Electronics Info Communications (UkrMiCo)}, \rtitle={High-frequency Class-E power amplifier with shunt filter}, \ryear={2016}, \rpages={1-4}, \rkeywords={HF amplifiers;LC circuits;circuit simulation;radiofrequency filters;radiofrequency power amplifiers;high frequency Class-B power amplifier;parasitic resistive element;second-harmonic suppression level;shunt LC filter;Capacitance;Capacitors;Power amplifiers;Power generation;Power harmonic filters;Shunts (electrical);Switches;Class E;parasitic elements;power amplifier;shunt filter}, \rdoi={10.1109\/UkrMiCo.2016.7739594}, \rmonth={Sept},}"},"paper3":{"title":"Microwave Filter Based on Crystal-like Reactive Elements","authors":["Evgeniy Nelin","Yana Zinher"],"conference":"2016 International Conference Radio Electronics & Info Communications (UkrMiCo)","abstract":"The models of reactive elements, called  -models, that unlike conventional models without frequency limitations are proposed. On the basis of  -models of low-pass filters mi- crostrip filters based on 3D electromagnetic-crystal inhomogenei- ties, which combine inductive and capacitive elements is pro- posed. Calculated and experimental characteristics of the filter illustrating characteristic improvement and reduction in size compared with conventional solution are presented.","keywords":["microstrip structure","electromagnetic crystal inhomogeneity","lowpass filter"],"content":"I. INTRODUCTION\rMicrostrip filters are using in many radioelectronic sys- tems including wireless [1]. Modern development of the mi- crostrip technology largely link with the using of artificial materials \u2014 metamaterials and artificial structures with spe- cial characteristics [2]\u2013[4]. Such structures include crystal-like structures with band properties similar to crystals. Microstrip crystal-like structures named electromagnetic crystals, as well as separate electromagnetic-crystal inhomogeneities (ECIs) are used in microstrip devices. In [5] the using of 3D ECIs as quasi-lumped reactive elements with a significant reactivity increasing compared with the conventional structures is pro- posed.\rOn the basis of quasi-lumped reactive elements lowpass filters (LP) are constructed. Since for the given reactivities values the ECIs are smaller, their using in LP filters will re- duce filters sizes. Furthermore, as shown below, and the sup- pression of signals in stopband is improved significantly.\rIn this paper the models of reactive elements, called \u03b4-models, is offered and microstrip LP filter based on 3D ECIs is proposed and examined.\rII. COVENSIONAL MODELS OF\rREACTIVE ELEMENTS\rModels of lumped reactive elements use in the circuit with\rlumped parameters. In such circuit the inductance and capaci- tance are lumped on separate section with the length l. In cir- cuit with distributed parameters (transmission line (TL)) under certain conditions reactive elements can be considered as qua- si-lumped on separate sections of TL.\rFig. 1 shows  - and T-like circuits, by which these sections are modeling. Quasi-lumped reactive element is realized by TL section.\rUnder conditions l    , Z    Z0 and Z    Z0 , where  \ris the wavelength; Z and Z0 are the wave impedances of TL\rsection and TL, in  - and T-like circuits capacitors and induc- tors are not taken into account, respectively. As a result, it remains only series inductor and shunt capacitor.\rFig. 1  - (a) and T-like (b) equivalent curcuits of TL section\rFor the given values of inductance L and capacitance C TL\r sections lengthes are defined as [2]: 1 L1\rlL   arcsin( c ), lC   arcsin( cCZC), kc ZL kc\r   where indexes \u00abL\u00bb and \u00abC\u00bb denoting inductance and capaci- tance; k 2 \/ ,   2 f ,fisthefrequency;index\u00abc\u00bbde- noting cutoff frequency.\rIII.\rDELTA MODELS OF REACTIVE ELEMENTS AND LOW-PASS FILTERS\r 978-1-5090-4409-2\/16\/$31.00 \u00a92016 IEEE\rFrequency limitation for\relements by ratio l      is\rconventional   of reactive\r due to the finite dimensions of the\r elements. Model without such limitation is proposed below.\rLet\u2019s consider the equivalent circuit for infinitely short TL section with the length dl (Fig. 2\u0430). The values dL, dC, dr and dg are lumped infinitely small elements: inductance, capaci-\rmodels\rtance, resistance and conductance, respectively. Resistance and conductance account losses. The inductance and capaci- tance equal:\rdL L dl, dC C dl, (1) where L  and C  are inductance and capacitance per unit\rlength.\rFig. 2 Equivalent circuit of infinitly short TL section (a), finite dimension impedance barrier and well (b),  -barrier and  -well (c)\rWave impedance and wave\u2019s phase velocity \u03bd in TL with- out losses are defined by expressions:\rZ  L , v  1 . (2) C  L C \r  aredeterminedbyratios    zl and    z 1l andin \u0421LhCl\rEqs (4) sign \u00ab=\u00bb changes to sign \u00ab \u00bb.\rConsider the using of reactive elements  -models for LP\rfilters modeling. Fig. 3a demonstrates LP prototype filters with a ladder network structure and their dual (Fig. 3b) [2, 6]. Value gi for i=1 to n represents the inductance of series in-\rductor or capacitance of shunt capacitor, n is the number of reactive elements. If g1 represents capacitance or inductance, then g0 is defined as the source resistance or conductance. Similarly, if gn \u2015 capacitance or inductance, then gn+1 corre- sponds to the load resistance or conductance. Element values are normalized thus, that g0  1 and the angular cutoff fre-\rquency  c  1.\rThe values of the filter inductances and capacitances are\rrelated with the values gi by formulas [5]:\rL Z0 cg, C g0 cg. (5)\r ig iiZ i 0c 0c\r    Taking into account Eqs (4) from Eqs (5) we get     gi .\rThus, reactive elements  -models are directly related with the prototype-filter model.\rFig. 3 LP prototype filters\rFig. 4 depicts the  -models for filters with Butterworth (a) and Chebyshev (b) characteristics for circuit from Fig. 3b when n   3 and n   5 . Distance between reactive elements\r    With regard to Eqs (2) from Eqs (1) we get dL Zdl, dC dl.\rFig. 2b shows finite dimension impedance barrier and well, where small symbols denoting the normalized to Z0 im- pedances; indexes \u00abh\u00bb and \u00abl\u00bb denoting high and low (regarding to 1) impedances.\ri kc\r  v Zv\r(3)\r Letting l dl, zh    zl  0 and besides that z dl    and z 1dl    , where   and   are constants.\rhLl\u0421L\u0421\rIn this case the finite dimension barrier and well will take the form of  -barrier and  -well (Fig. 2c), in Eqs (3) values dL and dC are finite and equal L and C, then Eqs (3) will take the form\rL  LZ0, C  C . (4) v Z0v\rAs we see from Eqs (4),  -barrier is equivalent to series inductor and  -well \u2015 shunt capacitor. We will call these models of reactive elements as  -models. Unlike conventional approximate models of reactive elements,  -models are exact and without frequency limitation.\rConstant   determines the measure of reactivity of reac- tive element. In graphic image of the reactive element  -model the length of the  -function is directly proportional to the con- stant   value, so and to the inductance or capacitance values. Thus, unlike conventional graphical symbols of reactive ele- ments,  -models allows to represent the relations between cir- cuit reactances values.\rFor finite dimension reactive elements constants  L and\r -models are shown imaginary. Values of gi are given in Table 1 [2].\rfor these cases\r    Fig. 4 Delta-models for LP filters with Butterworth (a) and Chebyshev (b) characteristics\rTABLE1VALUESOF gi\rIV. LOWPASSFILTER\rConventional ECIs are 2D structures formed as holes or slits of different shapes in the metalized surface or in the sig- nal conductor [2, 7]. 3D ECIs are significantly more effective [4]. Inductive 3D ECI is formed by through hole in metalized surface and dielectric with a small-diameter conductor above it, and capacitive \u2015 metalized non-through hole on signal conductor side or on metalized surface. Such ECIs represent first order LP filters.\rThe inductor and capacitor can be combined in the con- struction of ECI, thereby increasing the order of the filter. Combined ECI are formed by non-through holes: ECI I \u2015 on the signal conductor side (Fig. 6a), ECI II \u2015 on the metalized surface (Fig. 6b, substrate is not shown) with a partial metali- zation of the hole. Signal conductor of non-metalized part of the hole is narrow. At ECI II the form of the signal conductor over the metalized part of the hole is the same as hole bottom form. Non-metalized and metalized parts of the hole corre- spond to inductor and capacitor. Combined ECI is equivalent to the second order filter (Fig. 7).\rab Fig.6 Combined ECIs\rFig.7 Combined ECI equivalent circuit\rFig. 8 presents transfer characteristics of ECI with full- metalized hole (capacitance ECI, curve 1), and combined ECIs I and II (curve 2 and 3, respectively). Curve 4 \u2015 transfer characteristic of ECI I with hole etched in metalized surface under the non-metalized part of the ECI. The parameters of ECIs: square-shaped hole with rounded angles, the length of square side is equal 5 mm, hole depth t = 0.9 mm, signal con- ductor width w = 0.1 mm, half hole is metalized. Substrate material \u2015 Rogers RO3010, dielectric thickness 1.28 mm, relative dielectric constant 10.2, dielectric loss tangent 0.0023 at the frequency 10 GHz, metalization thickness 0.035 mm.\rAs ECIs are 3D structures with complex geometry and boundary conditions it is necessary to use 3D electromagnetic simulators. In the paper simulation was held in software pack-\r          LP character- istic type\rn g1 g2 g3 g4 g5\r         3 1.0000   2.0000 1.0000\rButterworth   5 0.6180   1.6180 2.0000 1.6180 0.6180\r                  Chebyshev, ripples 0.1dB\r3 1.0316   1.1474 1.0316\r          5 1.1468\r1.3712 1.9750 1.3712 1.1468\r   Fig. 5 shows transfer characteristics of the Chebyshev fil-\rter formed by finite dimension reactive elements (curve 1) and\rits  -model (curve 3) for circuit from Fig. 3b. Parameters of\rthe -model:  zl,  p ,p g\/g,z pz 1, ClLC12hl\rzl   0.4 . For finite dimension reactive elements the relation\rl \/ v is the same. Taking into account the influence of capaci- tance in  -like circuit and inductance in T-like, which are not\rincluded in quasi-lumped elements models, the impedances of the finite dimension reactive elements are corrected: inductive reduced by 6.7 % and capacitive increased by 13.6 %. In the passband the transfer characteristics coincide (see the inset in Fig. 5).\r  0 -10\r-20\r1 32\r0\r-0,10 1\r                               ,dB\r       H\r                    -30 02468\rf\/f c\rFig. 5 Transfer characteristics of the Chebyshev filter\rTo improve the selectivity of the filter formed by the finite dimension reactive elements the reactive elements should be closer to their  -models. Curve 2 in Fig. 5 \u2015 filter transfer characteristic when zl =0.2. For correction the impedances of inductive elements are reduced by 8.6 % and the impedance of capacitive is increased by 2.9 %. The frequency of transfer characteristic minimum in the stopband fmin increases from\r3.44 fc to 6.85 fc , and the minimum value Hmin decreases\rfrom  15.6 to  34.1 dB. Wherein the size of the filter twice smaller.\r                    \rage \u0421ST Microwave Studio. This package can be considered as a \u00abvirtual\u00bb vector network analyzer with possibility of structure's fields visualization.  Fig. 9 illustrates the construction of the third order LF fil-\r         ter based on the II combined rectangular shape\r         angles (substrate is not shown)\rto the two inductors and capacitor. Under a wide segment the\r0 -5 -10 -15 -20\r1 2 4\r3\r. Signal conductor is\rECI\rcorresponding\r1 23\rwith\r  rounded\r formed by two narrow and one wide segments,\r  hole is metalized.\r                                                                                                      0 2 4 6 8 10\rf, GHz Fig. 8 Transfer characteristics of ECIs\rFig. 9 LP filter based on combined ECI\rFig. 10 shows calculated transfer characteristic of the con- ventional structure filter (curve 1), experimental (curve 2) and calculated (curve 3) transfer characteristics of the filter based on combined ECI with parameters: n   3, Chebyshev charac-\rteristic, fc =1 GHz, passband ripple 0.1 dB, which correspond\rto L =8 .209 nH and C = 3.652 pF.\rConventional structure filter is examined in [2]. For con-\rventional and proposed structures the values of lL, wL , lC and\rwC are equal 9.81, 0.2, 7.11, 4.0 mm and 7.4, 0.2, 2.4 and\r5.0 mm, respectively; t = 0.9 mm; substrate material   Rog- ers RO3010. Excluding external segments of signal conductor, lengthes of the conventional and proposed structure filters equal 26.7, and 17.2 mm, respectively, which corresponds to the decrease of the filter\u2019s length by 1.6 times. Fig. 10 Transfer characteristics of the LP filter\rThe experimental values of fc and ripple coincide with\rcalculated. The calculated and experimental values of fmin and Hmin are equal 7.1 and 6.9 GHz,  33.5 and  33.0 dB, re-\rspectively.\rCONCLUSION\rProposed  -models of reactive elements unlike conven- tional models do not have frequency limitation. Delta-models \u00absuggest\u00bb decisions, which allow increasing the LF filter se- lectivity.\rMicrostrip filters based on 3D ECIs provide a significant reduction in size, increasing bandstop cutoff frequency and reducing signal level in bandstop. Combined ECI allows inte- grated realiziation of the different character reactivities.\rSignificant interest has got a research of based on ECIs higher orders LF filters and filters with other type characteristics.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7739595","bibtex":"@INPROCEEDINGS{7739595, \rauthor={E. Nelin and Y. Zinher}, \rbooktitle={2016 International Conference Radio Electronics Info Communications (UkrMiCo)}, \rtitle={Microwave filter based on crystal-like reactive elements}, \ryear={2016}, \rpages={1-4}, \rkeywords={crystals;low-pass filters;microstrip filters;microwave filters;\u03b4-models;3D electromagnetic-crystal inhomogeneities;capacitive elements;crystal-like reactive elements;inductive elements;low-pass filters;microstrip filters;microwave filter;Finite element analysis;Inductance;Integrated circuit modeling;Low-pass filters;Microstrip filters;Microwave filters;Three-dimensional displays;electromagnetic crystal inhomogeneity;lowpass filter;microstrip structure}, \rdoi={10.1109\/UkrMiCo.2016.7739595}, \rmonth={Sept},}"},"paper4":{"title":"The Research of L-type Matching Filter Parameters","authors":["A. Movchanyuk","V. Fesich","I. Sushko"],"conference":"2016 International Conference Radio Electronics & Info Communications (UkrMiCo)","abstract":"The work of L-type matching filters for different methods of detecting the PLL feedback signal and the filter parameter influence on oscillation amplitude of piezoelectric transducer are investigated in this research. The equivalent scheme of quadripole with mechanical resistance load composed of filter and a piezoelectric transducer is designed.\rThe expressions for the input filter impedance and oscillation amplitude of piezoelectric transducer on the input voltage amplitude are obtained by the analysis of the T-shaped equivalent circuit. Relations for matching filter inductance for piezoelectric transducer working at the serial and parallel resonance frequencies are calculated analyzing the expression for the input impedance.\rInappropriate use of L-type filter for piezoelectric transducer working at the serial resonance frequency is shown theoretically.\rThe possibility of piezoelectric transducer operating mode with independent of the mechanical load amplitude of mechanical vibrations is given. The adequacy of mathematical modeling is confirmed by the experimental results.","keywords":["ultrasonic","piezoelectric transducer","matching filter","resonance frequency","feedback signal","PLL","quadripole."],"content":"I. INTRODUCTION\rUltrasonic equipment and technologies with high ultrasonic intensity are widely used in the microelectronics, in radio components and devices manufacturing [1 \u2013 3]. The electromechanical transducer on the piezoceramic base \u2013 piezoelectric transducers are the most frequently used to obtain high-intensity ultrasound. Piezoelectric transducers have high electroacoustic efficiency, relatively low mechanical losses and enough mechanical strength after reinforcement. The piezoceramic devices are used as functional electronics devices \u2013 piezoelectric ultrasonic motors [4], piezoelectric transformers [5], actuators for positioning [6].\rThe main disadvantages of piezoceramics are the presence of a large static capacitance, the dependence of parameters on time and temperature and mechanical load. The electromechanical piezoelectric transducer must operate at the frequency close to the mechanical resonance frequency to\robtain high amplitudes of mechanical oscillations. In this case the electronic control circuit has not only to support the amplitude of piezoceramic transducer oscillations at a predetermined level, but also to compensate for its parameters changing. The reactance component of piezoelectric transducer impedance must be compensated to obtain high total efficiency of the electronic control circuit.\rII. PROBLEMSTATEMENT\rUltrasonic equipment consists of ultrasonic generator, ultrasonic piezoelectric transducer and matching filter. The analysis of the piezoelectric transducer equivalent circuit (fig.1) [7] shows that there are two resonant frequencies: the frequency of mechanical resonance frequency and parallel resonance frequency.\rThe amplitude of the oscillations is proportional to the currentthroughtheresistorofmechanicalbranchRM .\rL\u041c\r\u04210 \u0421\u041c R\u041c\rFig.1. The piezoelectric transducer equivalent circuit on the lumped elements.\rwhere C0 \u2013 capacitance of the clamped transducer; LM \u2013 mass component; CM \u2013 mechanical compliance; RM \u2013 mechanical loss resistance\rMutual compensation of reactance takes place in the mechanical branch of equivalent circuit at mechanical resonance:\rf 11. R 2  LMCM\r                          978-1-5090-4409-2\/16\/$31.00 \u00a92016 IEEE\r                            The input impedance of piezoelectric transducer becomes active at parallel resonance frequency and the parallel resonance frequency can be calculated as\rof piezoelectric transducer. Accordingly, feedback signal is\rI\r proportional to I\r1 if it is equal to the input current of matching\r 1\r1\r2 if the differential transformer is used to detect a\r   feedback signal. LM0 L1\rf 11. P 2  CC\rL1\rfilter or to I I2\r              M CM  C0\r      The mechanical branch parameters of equivalent circuit can be calculated according to the results of the static capacitance measurements, resonance and parallel resonance frequency:\r  f 2  1\rCM  C0   P    1 , LM   2 .\rU1\r\u04211\r   U1\r                T1\rT1\rFig. 2. Scheme of L-type matching filter. (a) feedback signal is proportional to input current. (b) feedback signal is proportional to current through mechanical branch.\rQuadripole is described by     A \u2013matrix. The elements of A \u2013matrix relate the currents and voltages at input and output\rterminals of the quadripole:\r       Ufb\r    fR      2  f   C\r  RM (a) (b)\r  Matching filter is mounted between ultrasonic generator output and piezoelectric transducer. It performs two functions simultaneously: makes active transducer impedance on the operating frequency and serves as the source of a feedback signal to maintain a resonant operating mode of the piezoelectric transducer.\rSignal proportional to the current through the input\rterminals of the matching filter (fig.2a) or through mechanical\rbranch using differential current transformer T (fig.2b) can be\rused to detect a feedback signal U . fb\rIt could be considered that a static capacitance of piezoelectric transducer is equal to double value of capacity and current transformer does not introduce significant phase shifts and can be excluded analyzing schemes of matching filters with differential transformer. Maximum current criterion or criterion of phase shift between feedback signal and output voltage of ultrasonic generator could be used as criteria of resonant mode maintain since the current through the mechanical branch is proportional to the amplitude of the transducer oscillations.\rDifferent PLL implementations and matching filter schemes [8, 9] are used designing the ultrasonic generators [10\u201413]. So question of feedback signal detection and its influence on the piezoelectric transducer operation is uncovered insufficiently.\rThe goal of the research is studying the feature of feedback signal detection in L-type matching filter and its influence on the amplitude of piezoelectric transducer oscillations.\rIII. MAIN PART\rThe piezoelectric transducer and matching filter are replaced on quadripole with T-type equivalent scheme (fig.3) for matching filter analysis.\rThe equivalent mechanical resistance RM is used as\r  U \r \r 1\r I1  a\r2\r21\r1\r1\r2\r2\r2\r2\r2\rUfb\r       1  U 11\rI I2 .\ra\ra\ra\ra\rU\rU\rU\rU\ra\ra\rI\r,\r  1\r,\r1\r1\r1\r12\r2\r        a2\rU2  a2\r21\r22\r1\r2\rU\r         Expressions for the matching filter input impedanceZIN , I\r the input current I\r1 and the current through the mechanical\r    branch I\r1\rcould be written assuming that the output impedance\rI\r2\r1\r 2\r of the ultrasonic generator on transistors is much smaller than the input impedance of matching filter:\r  Z\r1\rZ\rI\rI\ra21RM  a22\rN \ra\rR\r  1\ra\rR a\rM 12 ; (1)\r1\r1\r1\r           N\r  a\r a\r1M22; (2)\r   I \ra\r1\rR\rI\rU\r2\rR\r    1\rU\r1\r2\r        1\r1 a11RM  a12\r   In turn the elements of A \u2013matrix can be calculated through elements of T-shaped equivalent circuit:\r(3) ; (4) (5) (6)\r     Z a  1  B\rZ\r B Z A\r;\r;\ra12  ZB  ZC  Z\r; a21   1 ;\r     11\r    Z\rZ\rZ A\rZ\r   C\rZ\rC\rC\rC\rB\rZ\rB\rB\r;\r          quadripole load. The input voltage U\r1 coincides with output\rI\rcurrent of ultrasonic generator and also with input current of\rI\rU1 1\ra22  1 Z\rZ\rZ\rA\rA\r  Z\rC\r   voltage of ultrasonic generator, current I\r1 \u2013 with output\r.\rmatching filter. Current I\rmechanical branch and proportional to mechanical amplitude\r1\rC Z A\r.\r  1\r   2 is equal to the current through\r 2\r \r I1\rI2\rU2 R\u041c\rThe following is obtained by substituting (8\u201310) into (4\u20137), with the (11\u201314)\r      U1\r   2 a11  1      ;\r     EL  \r          2 \r      Zb\rZc\r   a21   j C0;\r(a)\ra12 j L j M    R  1     ;   R        EL    \r             U1 ZaU2R\u041c\r 2   2 a P .\r  22  2   2 PR\r  (b)\rFig. 3. The equivalent A-matrix (a) with T-shaped equivalent circuit (b).\rT-shaped equivalent circuit of quadripole for scheme on fig.2 takes the form in fig.4.\rThe following is obtained by substituting these expressions into (1)\r       2        1      RM   j L  j M  \r          2  \r \r  R   1      \r \r  EL       R  2   2\r       EL         .\r  \r      \r      Z\rA \rZ\rA\r(7)\rZ \rZ\rN\r    I\rI\rI\rN\r  j CR   P\r0M  2  2\r As the result:\rPR\rIt is obtained after transformations:\r   \r1\r1 ; j C0\r    Z\rB  B\rj \r;\r; (8)Z\r(9)\r0 M  \r  R\r     1  \r  EL\r     \rA\r         2    2   2    \r    M   \r    \r  R  1      \r      \r 2           \r 1    P R   CR   L  \r            2   2   M     EL      P R  \r2   2   2     CR     P  \r     Z\rj\rL\rL\rL\r \r \r \r      Z \r        I\rI\rN\rI N2\r   L   1 .\r0M22   P   R  \r Z \rC M\rZ\r L\rj\rj\rj\rL M\r       2     2   2             2PR\r           .\r      M j C I1LL\u041c \u0421\u041cI2\r  C0RM  1         2 2    L  M         EL       P   R      R\r     2  \r    M\r      \r  EL  \r      j       2  2  2 2\r      CR     P   0M  2  2\r                PR \r    U1 \u04210 U2R\u041c\rIt is necessary that the imaginary part of the input impedance is equal to zero to match the piezoelectric transducer with ultrasonic generator.\r          Fig. 4.\rSome additional notations:\r0 M          2  2       EL       P R   \rM                R      EL     \rThe equivalent scheme of L-type matching filter.\r EL   1 LC0\rThe expression analysis describes that the imaginary part of the input impedance is equal to zero for two different values of the matching filter inductance at two frequencies:\r;\r R   L C ; (11) C R2\r(10)\r1  2CP\r       2     2   2     CR2  1     P   L  \r              2   \r  R  1     0.\r  L  1 ,at   ; P0\r   MM L  0M ,at   .  2C2R2  1 R\r  P  \r1 R0M\rC C ; LM0\rMC C M0\r(12) As seen, the inductance value of matching filter does not depend on load impedance of mechanical branch in the first case. The matching filter input impedance and input current from (2) at the frequency  P are equal:\r(13)\r       LM .\rM\rCM\r\u0410\rC LC\rfig.5 \u2013 fig.9. Chokes 2mH for L-filter and 1 mH for L-filter with differential transformer were used as the filter inductance.\rL\r \r1 0\r  U1\rThus, the current through mechanical branch calculated by\r(3) is:\rAs a result, the current through mechanical branch and the amplitude of mechanical oscillations do not depend on the mechanical load on the piezoelectric transducer. The feedback signal and the ultrasonic generator output voltage will be\rshifted on  2 detecting the feedback signal using differential current transformer.\rIn the second case, the input impedance and current at the frequency  R are equal to:\r2\r ,I\r.\rI1 1\rU\rRM\r.\r0\r0 0R\r    Z\rN \rZ\rI\rM\r           I\rI\rN\rRM C0\rL\r       I U\r1\r.\r1\r  I\r2\rU\r.\r    2 1\r1 j PL\r     R \rR  M,I\rC\rR\r \rC2\rR2  1\rFig. 5.\rof L-filter working at parallel resonance\rCalculated (\u2013) and experimental (+) values of input current amplitude\r     Z \rI  1\rZ\rU\r1\rM. R0M M\r1\rUR\r0\rM\rR 0\r0\rM\r             1 The current through mechanical branch\r  I\r 2C2R2  1 1\rI\rN\rN\rR R2  1                     1\r      2\r \rC2 C\rR\r 1 2             1 R2 MR0MM\r  2C2 . R 0\r    I\rU\rU\rI\r2\r1\r U\rR\r U\rR\r0\r0 0M,I\rI\rU\r              2\r1R j CR2\r         Therefore, current through mechanical branch is practically linearly dependent on the load in the mechanical branch. Thus, detection of feedback signal is impossible through dependence of phase shift on load resistance in mechanical branch. Also the matching degree of generator and piezoelectric transducer will change. Matching can be achieved only at a certain load resistance.\rAs a result it can be concluded that scheme with L-type matching filter should be used for transducer operation at the parallel resonance frequency. Normal operation of PLL system is possible only at a fixed value of the mechanical load working at the mechanical resonance frequency. Mismatch changing the mechanical load will take place working with ultrasonic generator with external excitation at mechanical resonance frequency.\rA comparative analysis of experimental and theoretical data for the piezoelectric transducer with operating frequency of 44 kHz is conducted.\rThe experimental values of static capacitance, resonance and parallel resonance frequencies are calculated data for the equivalent circuit.\rC0  5.6nF; fR  44.05kHz; fP  44,65 kHz. CM  0,13714 nF; LM  95,4mH; RM  30Ohm.\rDigital generator RIGOL DG1022 was used as generator of test signal. Currents and voltages were measured by digital oscilloscope ATTEN ADS1102CML in averaging mode for 64 samples. Calculated by (1\u20133) and experimental data is given on\rFig. 6.\rworking at parallel resonance\rCalculated (\u2013) and experimental (+) values of input phase of L-filter\r Fig. 7.\rof L-filter with differential transformer working at parallel resonance.\rCalculated (\u2013) and experimental (+) values of input current amplitude\r Fig. 8.\rwith differential transformer working at parallel resonance.\rIV. CONCLUSIONS.\rIt is shown by the calculated and experimental data analysis that it is advisable to use a signal proportional to the input current filter as feedback signal for both types of filters. The mechanical branch current is shifted on 90 degrees relatively the input voltage. However, around the resonance frequency of the transducer high slope of the phase characteristic cannot be obtained. Therefore, using the feedback voltage proportional to the mechanical branch current as the feedback signal is not appropriate. The hypothesis of increasing the oscillation amplitude of the piezoelectric transducer with increase of static capacitance and reducing of filter inductances was confirmed by the results of experiments.\rThe increasing of oscillation amplitude was controlled visually by the liquid spray to the surface of the piezoelectric transducer. A drop of liquid was applied to the end of transducer. Then the input voltage for spraying was picked up. Then the capacitor was connected in the parallel with input terminals of the piezoelectric transducer and inductor values of the filter were changed. After that the voltage observing the start of liquid spraying was chosen. As a result it was found that spraying started at significantly lower input voltage increasing the static capacitance.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7739596","bibtex":"@INPROCEEDINGS{7739596, \rauthor={A. Movchanyuk and V. Fesich and I. Sushko and Y. Vistyzenko}, \rbooktitle={2016 International Conference Radio Electronics Info Communications (UkrMiCo)}, \rtitle={The research of L-type matching filter parameters}, \ryear={2016}, \rpages={1-5}, \rkeywords={equivalent circuits;matched filters;phase locked loops;piezoelectric transducers;signal detection;vibrations;L-type matching filter parameter;PLL feedback signal detection;T-shaped equivalent circuit analysis;input filter impedance;mathematical modeling;mechanical resistance load;mechanical vibration;parallel resonance frequency;piezoelectric transducer oscillation amplitude;quadripole equivalent scheme;Acoustics;Generators;Impedance;Matched filters;Oscillators;Piezoelectric transducers;Resonant frequency;PLL;feedback signal;matching filter;piezoelectric transducer;quadripole;resonance frequency;ultrasonic}, \rdoi={10.1109\/UkrMiCo.2016.7739596}, \rmonth={Sept},}"},"paper5":{"title":"IMMERSIVE VISUALIZATION OF GEOPHYSICAL DATA","authors":["Philippe A. Cerfontaine","Anne-Sophie Mreyen","Hans-Balder Havenith"],"conference":"2016 International Conference on 3D Imaging (IC3D)","abstract":"In this application paper we\u2019ll explain the work flow we use to create immersive visualizations and spatial interaction for geophysical data with a head mounted device (HMD). The data that we analyze consists of two dimensional geographi- cal map data and raw geophysical measurements with devices like seismometers, Electrical Resistivity Tomography (ERT) and seismic tomography profiles as well as other geophysi- cal and geoscientific data. We show the tool chain that we use while explaining the choices that we made along the way. The technical description will be followed by a brief assessment of the added benefit of rendering our data in virtual reality (VR). After the technical description we conclude this paper with some outlook on the (likely) future use of VR in geosciences.","keywords":["virtual reality","visualization","geology","geo- physics","geography"],"content":"1. INTRODUCTION AND MOTIVATION\rIn modern geophysics the three dimensional (3D) analysis of processed measurement data is very important. Being able to integrate data sets from different sources into a single spa- tial visualization thus becomes primordial to finding, under- standing and analyzing the geology of the site where the mea- surements were made. The multitude, variety and density of the resulting integrated data often overlap when they are pro- jected to a flat screen. The need for increased depth percep- tion drove us towards the recently rediscovered VR technol- ogy. Combined with the limited and often cumbersome inter- action capabilities provided by simple desktop environments with a mouse and keyboard we had several reasons to test our data in VR. Our approach is supported by recent technologi- cal advancements, both in the field of hardware and software development. Further this technology has become affordable through widespread use in the entertainment industry, so we are able to use it for the visualization of our geological mod- els.\r2. REFERENCES AND RELATED WORK\rVisualization of geophysical data traditionally involves a lot of 2D representations: Maps, profiles, seismograms and other graphs. Just as most types of geoscientific exploration, geo- physics requires 3D to investigate the relationship between all those 2D data. To stay on topic we won\u2019t go into details and specifics of 2D visualization in geophysics but we\u2019d like to reference some of the major developments for 3D repre- sentation in geology. GOCAD [9] is one example of a once academic development in this field that has long since then become a commercial product. Due to the necessity for geo- logical data visualization in geothermal, oil, gas or mining in- dustries (among others) it is no wonder that commercial soft- ware solutions like CoViz [2] have long since taken the lead in that domain and are still being developed. Even though such software mostly includes modules that allow represent- ing complex 3D data sets, including time-dependent informa- tion that can also be jointly interpreted, these solutions rarely offer support for VR technology. In most cases this was prob- ably because of the complexity and high price of VR equip- ment. This publication [6] shows that the recent developments in VR technology (hard- but also software) are changing that paradigm. Through the use of the Unreal Engine from EPIC [3] that already had its first VR application in [7]. Two good examples of immersive visualization applied to surface and fault data in geo-sciences using VR would be [8] and [11] respectively.\r3. 3D VISUALIZATION OF GEOPHYSICAL DATA\rFor us the first step to enable the spatial analysis of geo- physical data is to integrate the available data. Combining data from various measurement methods performed in the field into a single 3D model with real world coordinates is necessary to get useful information on the subsurface struc- ture of studied sites. The measurements cover most applied geophysical techniques, from electrical profiling to ambient noise recordings (including seismics, electro-magnetic mea- surements, among others). Once all the pieces have been ac- curately assembled, visualization matters like uniform color mapping across several profiles or transparencies have to be addressed. After these problems have been solved the second step will be to use the created model in VR.\rThis simplistic two stage approach is more important for us than it may seem at first sight. Because it lets us split the workload more evenly. Ideally stage one could be ac- complished by an expert in geology and geophysics modeling while in stage two the results would be further post-processed by someone who\u2019s expertise is in the field of visualization and virtual reality.\r3.1. Geographic raster data\rTypically, the first available data for a new site are aerial im- agery (ortho images), a digital elevation model (DEM) and in some cases a geological or topographic map. With geographic information systems (GIS) that rely on two dimensional infor- mation representation, we use the DEM to extract the depth information of the terrain to propagate it to otherwise flat data.\rThe DEM is a georeferenced raster image where each pixel contains an elevation value. It is essential for building our 3D model as it will be used as basic input representing to- pography, and as support for many types of textures including remote imagery.\rTo generate a 3D surface from the DEM we convert the raster image to point data with XYZ information. The result- ing point cloud can either be read in Paraview [1] by export- ing the attribute table from the GIS or directly importing the shape file to Blender [5] using the appropriate BlenderGIS plug-in. Both tools provide algorithms to compute a triangu- lated surface geometry. Because we want to render our model in VR (which is more demanding for the graphics hardware) the surface geometry could be decimated in flat areas if nec- essary.\rThe main purpose of the surface model is to give the viewer a sense of orientation. A good way to even further the sense of orientation is to apply aerial imagery or topographic map data as a texture to the obtained surface geometry. The Blender GIS plug in has functionality to accurately map georeferenced raster images to geometries in the same coordinate system (see Figure 1). One thing to keep in mind with geographic raster data is that it often contains huge amounts of pixels and might not have dimensions that exactly match a power of two, some early adjustments using tools in the GIS will generally saves a lot of trouble down the line when dealing with large data sets.\rAs surface data might hinder viewing subsurface infor- mation, it might be required to make them partly transpar- ent. Which is certainly a good idea while you don\u2019t have to manage a lot of transparency layers in the scene. But as we will see in the next section adding multiple intersecting profile data can rapidly make it tedious to manage transparency ef- fects while avoiding to confuse the viewer. Another technique is to draw the surface points only. The problem is that simple points don\u2019t provide depth information to the viewer. Rep-\rFig. 1. The textured surface model helps viewers to get a sense of orientation in the scene.\rresenting the surface in wire frame mode might provide the viewer with more depth cues but it also clutters the display a lot, depending on the resolution of the surface model. This is especially true for surfaces that have not been subject to mesh decimation but at least their regular structure improves depth perception, while decimated meshes are generally too chaotic to be visualized in wire frame, especially without re- triangulation.\rAnother way to enable the viewer to see through the sur- face model while preserving graphics compute power is to use contour lines. This widespread method for representing height on maps actually works quite well in 3D. They can easily be computed based on the initial DEM using the GIS. Due to their familiarity and the implicit depth cue of each line representing a certain height with constant vertical spacing between lines, in our experience, they give the viewer a good indication of the terrain while minimizing display clutter (see Figure 2). The only downside of contour lines is that they don\u2019t have surface normals and thus no illumination, like we already explained for points.\r3.2. Geographic vector data\rNow that we have seen how to integrate imagery and raster data from the GIS we still have to transfer measurement points and profile lines in short vector data to the 3D scene. Points can be imported in exactly the same way that we already ex- plained for the DEM point cloud in the previous section. To represent them in the scene we use simple geometries like spheres or cylinders as glyphs with different colors or color scales to symbolize different values. The glyph\u2019s size can also be used as a means to convey additional information to the viewer. Glyphs that are located on the surface model should be big enough to reach trough it. This way they can serve as\r landmarks to help the viewer get his bearings from above and below ground level.\rLine data can be represented with tubes instead of simple lines since they can be illuminated for better depth perception. Lines that we want to represent on the surface should be sub- divide to prevent the tubes from completely cutting through the geometry. N.B.: This technique might be applied to the aforementioned contour lines but as for glyphs it raises the polygon count in the scene.\rPolygon data, especially large area polygons, have to un- dergo a special procedure when they have to be represented in 3D. To avoid the generated 3D polygons from cutting through the model\u2019s surface we have to insert additional points inside each polygon. In order to match the DEM surface for each polygon we want to render as geometry we use the clipping functionality from the GIS to create point subsets from the DEM point cloud that we then have to triangulate using the Delaunay algorithm. The procedure is very similar to the one we used to generate the surface model only in smaller chunks. The result might become blocky because it is bound to the original DEM grid\u2019s resolution. Considering that we have to repeat the whole procedure for every polygon it might be more efficient to use the GIS to export the complete map as a georeferenced raster image and drape it on the surface geometry as a texture.\r3.3. Profile data\rGenerally speaking profile data is a set of two-dimensional graphs that represent seismic velocities or electrical resistiv- ity. Those were obtained through 2D inversion of measure- ment data with the adequate software. These graphs may or may not intersect one another (see Figure 2). Once the pro- file images are ready to be exported in high resolution they have to be placed in the 3D scene as accurately as possible. We\u2019re going to import these profiles as images on planes with transparent background. Next we\u2019ll need at least two refer- ence points on the surface geometry and the profile that we have to match together. Point data imported from the GIS as described in the previous subsection might be a good choice for that purpose. First the image has to be scaled accordingly before placing it with respect to the surface geometry. This can become a tedious and repetitive task when sitting at the desktop.\r4. IMMERSIVE VISUALIZATION AND INTERACTION WITH GEOPHYSICAL DATA\r4.1. Visualizing geophysics in VR\rFrom the hardware perspective we are currently working with the Vive system from HTC [10] mostly because of the precise and responsive tracking system that allows us to track a room of 3.5 by 4.5 meters (in our case) with bi-manual 6dof inter- action. On the software side we use the Unreal Engine 4 from\rFig. 2. Intersecting profile data visualized with contour lines as surface representation.\rEPIC [3], because it is powerful and has out of the box support for our current VR hardware while the provided functionality also has a layer of abstraction from it.\rTo setup the scene we built in the 3D modeler it is bet- ter to think about the way we want to present the data to the final viewer instead of using a monolithic copy. Consider- ing the frequently overlapping data from different geological measurements or features it is better to partition the scenes objects so that the different parts can be enabled and disabled individually in the final scene. For instance one way to deal with this is to export all the seismic profiles as one object and to create another object with all electric profiles thus bringing more structure to the final scene. With the scene graph\u2019s hier- archy being organized around the surface model as backbone we usually attach all other parts of the model to the ground level. Light sources used to emphasize certain features in the scene should also be anchored in a way that makes sure that they move according to what they\u2019re supposed to highlight (see Figure 3).\rBefore we go over to the interaction part of this section we still have settings and adjustments to make to the final scene\u2019s materials. Since transparencies don\u2019t automatically carry over from the 3D modeling tool. The settings have to be adapted material-wise to achieve the desired effect. Translucency of several overlapping profiles can rapidly become confusing for the viewer with additive transparency settings. Another way to solve this problem is to mask out the profile background using the image\u2019s alpha channel and reserve translucency for a see through effect on the surface model thus minimizing overlapping transparencies in the process.\r4.2. Interacting with geophysical data in VR\rSince version 4.12 the Unreal Engine comes with a built in VR editor mode that lets you use motion controllers to build and tweak the 3D scene with 6dof interaction while wearing\r theVRhelmet.\rFig. 3. Subsurface view of geophysical data with the Unreal Engine 4.\r5. BENEFITS AND IMPROVEMENTS USING\rWe tried it with the tedious task of placing the profile data according to the surface model. After you\u2019ve become familiar with the controls, a profile could theoretically be placed with one move. In practice this requires several adjustments and tweaks. Although its fast, precision is an issue when manipu- lating far away objects with two picking beams to control the position, orientation and scale in one fell swoop as depicted in Figure 4.\rOnce you get used to the VR editor it becomes a very useful extension to prepare and inspect the final scene with the geophysical data. The problem that we had when showing our content in the VR editor was that the controls tend to be overwhelming for one time users. The obvious solution to this problem is to implement more simple interaction methods that would allow us to show and inspect our data without the fear of scaring people off or having them break the model every time they aim at and then click something with a motion controller.\rWe decided to use a simple fly through metaphor where pushing the trigger while holding the motion controller in one direction would move the viewer towards exactly that direc- tion. The trigger button makes the most sense because it is a one dimensional analog input signal that we can use to adjust the movement speed.\rIMMERSIVE VISUALIZATION AND INTERACTION\rBefore we get to the main part of this section where we com- pare the desktop and VR visualization of our models we are going to give the reader a brief rundown of the main differ- ences between a HMD based VR system versus systems like a CAVE [4] or other large tiled display systems e.g. power walls.\rBesides the obvious price tag difference that we already mentioned in the introduction there are other more practi- cal considerations. On one hand, compared to lightweight tracked filtering glasses used in a CAVE, HMDs are less com- fortable to wear. While the technology has certainly been im- proved weight and wiring are still problematic issues.\rOn the other hand, HMDs are clearly a more economic solution not only from a financial, but also from the spatial point of view as less room space is required to set up the sys- tem. Furthermore, HMD systems require less maintenance and related human resources.\rWhile it is far from perfect at least large scale VR installa- tions give you the possibility to let a few other viewers stand as closely as possible to the user that\u2019s benefiting from user centered projection and moving parallax. With HMDs there\u2019s no such option the only thing that\u2019s left for bystanders is to look at a screen view of the scene, although it could easily be projected to a wall.\r Fig. 4. Placing geophysical profile data using picking beams in VR editor mode.\rOur last point is about visual quality. One thing to keep in mind when comparing the field of view of HMD based systems to a system with glasses, is that the visibility field with stereo vision is restricted by the glasses size. The most outstanding argument in favor of the visual quality provided by HMD systems is the implicit edgeless, squared 360 degree panoramic view that is impossible to match even with high end CAVE systems. For people that are subject to motion sickness seeing your own body helps so it is an inconvenience with HMDs.\rFinally we focus on the fundamental question about the upgrade from desktop to VR visualization for geophysical data and its pros and cons. Even though the VR hardware has become much cheaper, it is still more expensive than a simple desktop system without a HMD. The high hardware specifications are certainly due to the fact that during render- ing a lot of data has to be processed twice until the final im- ages for the left eye and the right eye are ready (at frame rates that don\u2019t induce motion sickness). As expected it is only natural that higher frame rates can be achieved with the same data and system but on the monitor. Better depth perception is certainly one of the biggest assets of VR. The added bene- fit from first and foremost moving parallax and stereo vision are really stunning for new users. Depth perception for sim- ple points and lines on desktop systems is impossible without moving the model. In VR you get depth cues without fur-\rther ado. The only thing you have to do is moving your head around the scene. This makes it easier than with a mouse and keyboard where you first have to learn the key bindings and button mapping before you understand what sort of trackball interaction you are dealing with. Even with a 6dof mouse the desktop can\u2019t compete because the device is designed for experts.\rOnce the model is scaled beyond the size of the tracking space the user needs a way to get around the scene. So VR interaction has to be included and custom tailored to the appli- cation, the available hardware and last but not least the differ- ent users. We consider this to be the make or break point for VR applications and demonstrations. This is where VR nav- igation helps if you can keep the interaction simple enough for presentation purposes. While from a developer\u2019s perspec- tive it might be tempting to try to use all available degrees of freedom with all buttons the casual user is thank full for less. We found that once it has been tailored to the viewers de- gree of expertise it is faster to pick up than with basic desktop interaction. But the desktop visualization also has some ad- vantages. One advantage that should not be neglected is that usually people don\u2019t get motion sickness just by looking at a monitor. Motion sickness only affects a small percentage of users, nevertheless it is still an issue for VR. A smaller prob- lem is the over all readability of 2D text content in VR. Jitter and blur often hinder the viewer from reading comfortably. Readability can be improved through 3D font extrusion but in return this increases the polygon count. So it is only an option for short texts but then again it is probably a bad idea to read long texts with in VR anyway. To conclude this section we\u2019ll briefly touch the topic of sharing the visualization experience with others. When analyzing geophysical data more than one expert is usually required to get the most insight out of the 3D model. This reflects the different kinds of data sets and measurement methods that are involved. A collaborative VR solution would be required to adequately address this prob- lem.\r6. CONCLUSION AND OUTLOOK\rThe improved availability and quality of entry level VR sys- tems should convince hesitant or reluctant potential users to try this technology before deciding to pursue or condemn VR for their application\u2019s needs. One thing in particular that we found out is that representing geophysical models with a HMD and only head tracking has become an easy task. Further we learned that user interaction with motion controllers can become complicated to learn. For now we have reduced it to simple scene navigation but we\u2019ll certainly add more in- teraction mechanisms over time. The next thing that we\u2019d like to investigate in that context is shifts of different inter- action paradigms based on user perspective and the model to user size ratio. Finally we noticed that distributed and\/or co-located real time collaborative VR is necessary. This is certainly a complicated task that involves network commu- nication and data sharing topics [2]. In our opinion it is the only valid option to circumvent the problems we had trying to share the VR experience with a HMD based system. It is probably safe to assume that this is an issue for disciplines other than geophysics as well.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823445","bibtex":"@INPROCEEDINGS{7823445, \rauthor={P. A. Cerfontaine and A. S. Mreyen and H. B. Havenith}, \rbooktitle={2016 International Conference on 3D Imaging (IC3D)}, \rtitle={Immersive visualization of geophysical data}, \ryear={2016}, \rpages={1-6}, \rkeywords={data visualisation;electrical resistivity;geophysical techniques;seismology;virtual reality;electrical resistivity tomography profiles;geophysical measurements;geosciences;head mounted device;seismic tomography profiles;seismometers;two dimensional geographical map data visualization;virtual reality;Data visualization;Geology;Geophysical measurements;Solid modeling;Surface topography;Three-dimensional displays;geography;geology;geophysics;virtual reality;visualization}, \rdoi={10.1109\/IC3D.2016.7823445}, \rmonth={Dec},}"},"paper6":{"title":"THE EFFECTS OF DEPTH WARPING ON PERCEIVED ACCELERATION IN STEREOSCOPIC ANIMATION","authors":["Sidrah Laldin","Laurie M. Wilcox","Robert S. Allison"],"conference":"2016 International Conference on 3D Imaging (IC3D)","abstract":"Stereoscopic media produce the sensation of depth through differences between the images presented to the two eyes. These differences arise from binocular parallax which in turn is caused by the separation of the cameras used to capture the scene. Creators of stereoscopic media face the challenge of depicting compelling depth while restricting the amount of parallax to a comfortable range. To address this tradeoff, stereoscopic warping or depth adjustment algorithms are used in the post-production process to selectively increase or decrease the depth in specific regions. This process modifies the image\u2019s depth-to-parallax mapping to suit the desired parallax range. As the depth is adjusted using non-linear parallax re-mapping functions, the geometric stereoscopic space is distorted. In addition, the relative expansion or compression of stereoscopic space should theoretically affect the perceived acceleration of an object passing through that region. Here we evaluate this prediction and determine if stereoscopic warping affects viewers' perception of acceleration. Observers judged the perceived acceleration of an approaching object (a toy helicopter) moving in depth through a complex stereoscopic 3D scene. The helicopter flew at one of two altitudes, either ground level or camera level. For each altitude, stereoscopic animations were produced under three depth re-mapping conditions (i) compressive, (ii) expansive, and (iii) zero (no re-mapping) for a total of six test conditions.\rWe predicted that expansive depth re-mapping would produce a bias toward perceiving deceleration of the approaching helicopter, while compressive depth re- mapping would result in a bias toward seeing acceleration. However, there were no significant differences in the amount or direction of bias between the re-mapping conditions. We did find a significant effect of the helicopter altitude, such that there was little bias in acceleration judgements when the helicopter moved at ground level but a significant bias towards reporting acceleration when the helicopter moved at camera level. This result is consistent with the proposal that observers can make use of additional monocular (2D) cues in the ground level condition to improve their acceleration estimates. The lack of an effect of\rdepth re-mapping suggests that viewers have considerable tolerance to depth distortions resulting from stereoscopic post-processing. These results have important implications for effective post-production and quality assurance for stereoscopic 3D content creation.","keywords":["motion in depth","acceleration","depth warping","stereoscopic 3D media"],"content":"1. INTRODUCTION\rDepth from disparity in a stereoscopic image reflects the layout of the scene but is also affected by a number of artistic, on-set production, post-production, and display parameters. A tool that filmmakers often use to determine stereoscopic parameters is the disparity range or parallax budget. This range is usually limited and carefully controlled to ensure comfortable viewing or to respect quality assurance standards. Thus, one of the most important stereoscopic 3D (S3D) filmmaking parameters is the interaxial distance between the two cameras, as it controls the mapping of the range of depths in the scene to the range of parallax in the image. This parallax is usually expressed in terms of the number of pixels of lateral shift between the images of the same object in the left and right camera view, or alternatively the size of this shift as a percentage of the image width. Assuming a \u2018typical\u2019 viewing geometry, the parallax budget controls the desired range of retinal disparities that will be presented to the user.\rIn native capture stereoscopic filming, the camera interaxial separation affects the parallax to depth relationship throughout the entire scene. With the single parameter of interaxial, parallax cannot be increased at one range of distances and simultaneously decreased or held constant at another. Once the content has been captured at a particular camera baseline, it is generally not possible to change the interaxial. Furthermore, during capture, the parallax range is typically optimized for a particular screen size; when the footage is subsequently displayed on a screen with a different size, the predicted depth experienced by the audience will change. For example, according to stereoscopic geometry, if a S3D image optimized for a theatre-sized screen is viewed on a small screen (at a nearer\r978-1-5090-5743-6\/16\/$31.00 \u20ddc2016 IEEE\rdistance so it subtends the same visual angle) then geometry predicts a decrease in depth and a flattening of objects in the scene. Conversely, an image designed for a small screen can be uncomfortable to view when scaled up. Although S3D content producers have various capturing techniques, such as multi-view acquisition and multiple retakes, at their disposal to adapt the content to different viewing conditions, these methods can be computationally complex, time- consuming, and costly. Content producers can also employ labour intensive tools such as manual editing of disparities by compositing scenes from multiple stereo rigs with varying baselines.\rTo cope with these issues, S3D content producers can use stereoscopic warping or depth adjustment in the post- production phase. This process permits modification of the image\u2019s depth-to-parallax mapping to suit the desired parallax range. In addition, depth-to-disparity modelling is performed such that the parallax can be selectively retargeted for a defined region of interest in the image. This results in modified depth in specific parts of the 3D image leaving the depth in other regions unchanged. This kind of region-defined disparity manipulation is known as non- linear parallax mapping or non-linear disparity mapping by some researchers, and provides the content producers with a robust means to manipulate and control depth in isolated parts of the image for technical and\/or artistic reasons [1]\u2013 [6].\rNon-linear depth adjustment can be beneficial when a large range of distances need to be portrayed in a scene that also contains a volumetric object in the foreground. If this scene is shot with a restricted parallax budget, then all depths within the scene will be compressed, and the foreground object may appear significantly compressed and flattened relative to its width. If the depth budget is expanded, to give the foreground object(s) more volume, the director risks introducing too much parallax in the background which may introduce divergence. Region- specific disparity manipulation through non-linear disparity mapping provides a solution as it permits enhancement of the depth of the foreground, while leaving the depth in the background unchanged [2].\rThe non-linear parallax mapping process involves several steps including: (i) generation of parallax or depth maps, (ii) adjustment of the function between zin (distance from the camera in the original stereo pairs) and zout (the geometrically predicted distance from the viewer of the display), and (iii) creation of a new stereoscopic image based on the depth maps and the new zout function. The key element is that the user or 3D content producer can specify the in-out distance or parallax maps based on the desired creative and technical effects. It is a common practice that once the left view has been acquired, the parallax manipulations are performed on the right view based on the nearest and farthest distances, desired parallaxes at these points, and the parallax remapping function between these end points. As the depth is adjusted using non-linear\rparallax re-mapping functions, the geometrically predicted stereoscopic space is distorted. It no longer adheres to the stereoscopic geometry of the scene that was captured (even if presented orthostereoscopically) or to the monocular depth cues in the perspective images.\rTypically the input-output parallax mapping functions are monotonic and smooth to avoid reversals or discontinuities in the relationship between parallax and depth (discontinuities can be used if no objects appear at or near the distance of the discontinuity). As noted in the figures that follow, unless they are linear, these monotonic parallax remapping functions produce stereoscopic distortions; depths between the different points of the scene are compressed or expanded depending on their distance. In expansive parallax re-mapping functions, the parallax between objects that are near the camera will be smaller than in the original stereoscopic images, while the parallax will be larger than in the original at far distances from the camera. This predicts a perceived expansion of stereoscopic space with increasing distance from the camera. Conversely, with a compressive parallax re-mapping function, compression of space should increase as distance from the camera increases.\rAs well as affecting static depth intervals, this compression or expansion of stereoscopic space should affect the perceived motion of an object traversing the space. That is, if an object moves in depth relative to the camera at constant velocity in the scene, but the mapping from the original to transformed stereoscopic is compressive or expansive, then the object\u2019s apparent velocity should change over time. If the object moves in the direction in which the space expands then it should appear to accelerate. Conversely, it should appear to decelerate if it moves in the opposite direction.\r2. METHODS\r2.1. Participants\rParticipants (n=7) ranged in age from 19 to 35, had normal or corrected to normal visual acuity, and good stereoscopic vision (at least 40 arcseconds assessed using the Randot Stereotest). Three of the observers were female, four were male, and all were paid for their participation.\r2.2. Stimuli\rThe scene was constructed in Maya (Autodesk, version 2012) to simulate a realistic animation clip from an S3D movie complete with multiple monocular and binocular depth cues. Each graphic unit in the modelled scene was equivalent to a centimeter in 3D space and thus we will report all dimensions in cm (see below for the effects of image scaling and stereoscopic presentation). The frame rate of the stereoscopic presentation was 24 fps and we will report all durations, speeds and accelerations in seconds based on this frame rate.\rThe shot consisted of a toy helicopter (38.51 cm x 12.85 cm) flying toward the camera across a school gym (604.52 cm by 998.98 cm). The gym included a stage with curtains in the background, multiple rows and columns of chairs placed in the middle of the gym, as well as various other objects placed throughout the space (Figure 1). The only moving object in the scene was the helicopter, which sported a rotor that turned at a constant speed throughout the clip (so as not to provide an extraneous cue for the acceleration or deceleration of the helicopter).\rIn the z-space (depth direction) of the scene, the helicopter moved from a distance of 650 cm to approximately 150 cm. Because the camera was located at z=25 cm this corresponded to a range of 625 cm to 125 cm relative to the camera. The z-axis motion path specified in Maya was a straight line approach trajectory traversing 500 cm. The helicopter covered this distance in 5 s (120 frames) for a simulated average speed of 100 cm\/s. This average speed was constant across animations although acceleration, and thus the start and end speed varied.\rFig. 1. Helicopter shot \/ gym scene used in the experiments. The helicopter is approaching at floor level in this image.\rFig. 2. The helicopter approaching at eye level.\rThe position of an object relative to the ground plane is a strong monocular depth cue (called \u2018height in the field\u2019). Further, changing the altitude of the object will influence the availability, and influence of, monocular depth cues. Thus we evaluated whether the height of the helicopter above the ground had an impact on the viewer\u2019s perception of acceleration or deceleration. To do so we compared two\r\u2018altitude\u2019 conditions: one where the simulated helicopter flew just above the ground and one where it approached at camera height (43.7 cm). In the latter case the helicopter remained at the same vertical position in the image as it approached, making the height in the field cues uninformative (Figure 2).\rIn all the conditions, the start position of the helicopter was at 625 cm from the camera and the end position was at 125 cm from the camera in the world z-coordinates (i.e., separation in depth along the floor). However, the camera was pitched downward (-3.34o) in the floor level condition for better framing. This resulted in slight differences in distances along the camera z-direction of the start and end positions of the helicopter between the two conditions That is, they were slightly further from the camera (+4 cm) in the ground level condition than in the camera level condition.\rStereoscopic animations were rendered as separate left and right eye views. It was critical to maintain constant parallax at the start and end position of the helicopter throughout all the test conditions. In these experiments the effective unwarped stereoscopic camera rig had parallel cameras with focal length of 35 mm (35-mm equivalent, 54o horizontal field of view) and an interaxial separation of 1.98 cm. The rig was converged (using standard off-axis\/skew frustrum stereoscopic rendering) at 125 cm from the camera so that the animation stopped as the nose of the helicopter reached the screen plane (i.e., had zero screen parallax). Based on these parameters, the parallax at the start position for the ground level was 25.92 pixels and for the eye-level condition was 23.92 pixels across all test conditions. The end disparity of the front tip of the helicopter was zero in all conditions.\rThe non-linear parallax remapping was accomplished based on the rendered camera frames and associated depth maps using Fusion (Blackmagic Design, version 6.3). Based on the input images, and the desired parallax remapping, a new right camera image sequence was rendered. The disparities at the start and end position of the helicopter\u2019s travel were controlled and maintained in Fusion and were verified by comparison with onscreen disparity. In the stereoscopic tools in Fusion, a near and far point can be associated with a range of parallax values (parallax budget) and the desired interaxial distance computed. These values were set to match the stereoscopic rig parameters listed in the previous paragraph.\r2.3. Non-linear parallax re-mapping\rThree non-linear parallax re-mapping functions were used for this study. The parallaxes at both the start (z = 650) and end (z = 150) positions of the helicopter were constant across all conditions and the parallax at these points corresponded to the distance in the modelled scene and the stereoscopic rig parameters. To remap the depth of the selected scene, to the space between these end points (i.e. along the path traversed by the helicopter) the relationship between distance in the modelled scene and the parallax\r  \r specified distance was mapped according to one of three quadratic functions. These functions describe the relationship between the original z position of the helicopter in the modelled scene (zin) and the z position specified by the screen parallax after mapping (zout) (Refer to Figure 3 below):\r1. Linear:   =   In the linear condition the       \rstereoscopic parallax matches the perspective depth. The correspondence is appropriate for the camera baseline and this parallax is not manipulated in the post-production. Note, however, that the stereoscopic camera baseline is smaller than the observer\u2019s eye separation so the predicted stereoscopic depth for the whole scene is somewhat compressed relative to the modelled scene. This stereoscopic distortion in the scene is the natural result of the camera set up and parameters and is typical for stereoscopic movies.\rFig. 3. Non-linear parallax remapping function. Parallax for zin values (x-axis) were remapped to parallax appropriate to a different zout (y-axis).\r2.4. Stereoscopic Display and Viewing\r2. C  ompressive function:      =\r\u2212 .        +  .     \u2212    . ,     \u2264   \u2264    \rThe stimuli were displayed on 54-inch 1080p Panasonic Viera Series TC-P54VT25 3DTV plasma television with dimensions of 119.8 (W) and 67.3 (H) cm and pixel resolution of 1920 (W) and 1080 (H). The stereoscopic images were viewed through active shutter glasses provided with the television. The position of the viewer relative to the screen was adjusted to obtain a horizontal viewing angle of 36 degrees (1.8m from screen). The viewer sat stationary throughout the session.\rThe camera interaxial separation was smaller than the viewer\u2019s interocular distance (which averaged 5.81 cm as measured with a Reichert, Digital PD Meter). Additionally, the CG camera field of view was wider than the display visual angle. These factors are typical of filmed stereoscopic content in commercial productions where lens and interaxial choices are made for reasons of artistic preference and visual comfort rather than to match a specific viewer and viewing position (which is impossible in a theatre setting).\r2.5. Procedure\rThe method of constant stimuli was used to assess the perceived acceleration\/deceleration of the helicopter under each of the disparity mapping and altitude conditions. For each condition, stimuli with 11 levels of acceleration were presented to cover a range of positive and negative accelerations. A set of pilot experiments determined an appropriate range of accelerations needed to adequately sample the psychometric functions (Table 1). On each trial, the observer viewed one helicopter trajectory, and indicated verbally whether the helicopter appeared to be accelerating or decelerating. Each stimulus condition was presented 20 times over 10 sessions (2 repeats per session). For each subject, the proportion of accelerating responses at each stimulus level was tabulated and psychometric functions were fit using a Weibull function. From each psychometric function we estimated the point of subjective equality (PSE) for each mapping function. The PSE in the present case is\r            , otherwise\rwhere stereoscopic depth intervals are compressed relative to the model at distances far from the camera. Given that the object moves towards the camera (z-values decrease) the object should appear to accelerate as it approaches (relative to the linear case). As the helicopter covers the same distance (end points are fixed) in the same time average simulated velocity remains 100 cm\/s. However, the velocity changes with a simulated acceleration of 24.8 cm\/s2, starting from 38 cm\/s at z = 650 and increasing to 162 cm\/s at z = 150.\r3. E xpansive function:      =\r .        +  .     +    . ,     \u2264   \u2264    \r            , otherwise\rwhere stereoscopic depth intervals are expanded relative to the model at distances far from the camera. Since the object moves towards the camera it should appear to decelerate (relative to the linear case). The simulated deceleration is 24.8 cm\/s2, starting from 162 cm\/s at z = 650 and decreasing to 38 cm\/s at z = 150.\rCare was taken so that the rate of change of depth was either strictly increasing or decreasing over the manipulated trajectory (z-values between 650 cm and 150cm). In Fusion, disparity manipulation occurs at the nodal level, in which a node is created to specify how the z-value is manipulated. Thus, the three zin versus zout relations were generated over the range of relevant z values (Figure 3).\r the stimulus level at which the participant is equally like to report that the helicopter was accelerating or decelerating (the 50% point on the psychometric function). Bias predictions were calculated using each observer\u2019s interocular distance. If the re-mapping biases the apparent acceleration in one direction, then the PSE should be shifted in the opposite direction since acceleration in the opposite direction is needed to null this bias.\r    Acceleration (cm\/frame2)\rAcceleration (cm\/s2)\r Initial velocity (cm\/s)\r   Final velocity (cm\/s)\r  0.05\r28.8\r 30.0\r  172.8\r  0.04\r23.04\r 44.3\r  158.6\r  0.03\r17.28\r 58.6\r  144.3\r  0.02\r11.52\r 72.9\r  130.0\r  0.01\r5.76\r 87.2\r  115.7\r  0\r0\r 101.4\r  101.4\r  -0.01\r-5.76\r 116.4\r  87.8\r  -0.02\r-11.52\r 130.7\r  73.5\r  -0.03\r-17.28\r 144.9\r  59.3\r  -0.04\r-23.04\r 159.2\r  45.0\r -0.05\r-28.8\r  173.5\r 30.7\r            Table 1. Acceleration levels used to measure the effects of non-linear parallax remapping.\r3. RESULTS AND DISCUSSION\rFigure 4 shows sample psychometric functions for one observer for the linear, compressive, and expansive non- linear parallax re-mapping conditions. As expected, as acceleration increases from negative to positive, the proportion of positive acceleration responses increases. The observer in Figure 4 demonstrates bias that is dependent on the remapping function. Once again, it should be noted that the predicted PSE (50% point on the fitted curve) is opposite in direction to the bias because oppositely directed acceleration will be needed to cancel the bias. For example, in Figure 4, the observer was biased to perceive the stimulus as accelerating for the compressive function; however, because the PSE reflects the amount of acceleration needed to cancel the positive bias, the PSE value is negative.\rFig. 4. Sample psychometric functions (fit with Weibull functions) for one subject for each mapping condition (compressive, linear and expansive) of the ground condition.\rThe average PSEs for all participants are shown in Figure 5 as a function of the non-linear parallax remapping condition and altitude. This figure also depicts the geometrically predicted PSE values based on the stereoscopic geometry.\rFig. 5. Average PSE values for Eye (Camera) Level and Ground Level conditions. The lines show predicted PSE values for each remapping curve with compressive at -13.9, linear at -7.9, and expansive at -4.2 cm\/s2\rWe predicted that the transition points would vary as a function of non-linear parallax remapping algorithm. Furthermore, when the stereoscopic capture parameters differ from the viewing parameters (as in this study and in most filmed content) geometry predicts a transformation between the geometry of the modeled scene and its appearance in the perceptual display space (Allison, 2004; Woods, 1993). In the current scenario the differences in baseline and magnification predict a nonlinear compression of space relative to the original scene. As a result of the nonlinearity, we would predict\u2014based on parallax alone\u2014 that an object moving at constant velocity in depth would appear to be accelerating at 7.9 cm\/s2. The acceleration predictions for our compressive and expansive re-mappings\rwould also be subject to this compression and hence biased toward acceleration with predicted accelerations of 13.9 and 4.2 cm\/s2, respectively.\rIt is clear that these predictions were not borne out. A repeated-measures ANOVA confirmed that while there was a significant main effect of altitude (F(1,35)=37.438, p= 5.39e-07), there was no effect of depth warping on perceived acceleration (F(2,35)=0.857, p=0.433). The interaction between depth warping and the altitude of the helicopter was also not significant (F(2,35)= 0.046, p=0.955).\rThe impact of helicopter altitude on perceived acceleration is evident in Figure 5. That is, in the ground level condition there was little bias, while in the camera level condition, there was a significant bias toward seeing the helicopter accelerate (deceleration was required to null the bias). This altitude-dependent bias was consistent across the non-linear parallax re-mapping conditions.\rIn summary, the results of the experiment show that although participants\u2019 perception of the transition between perceived acceleration and deceleration differed significantly as a function of helicopter altitude, there was no consistent effect of stereoscopic depth warping at either altitude.\rThe S3D scene contained monocular cues such as looming, perspective, and distance from the horizon in addition to the screen parallax. To isolate the effect of the monocular cues and their influence on perceived motion through the scene we replicated the main experiment but with monocular viewing of the linear mapping stimulus (with non-dominant eye patched) in three participants. We found no significant differences in PSE between eye level and ground condition when viewed monocularly. Two participants displayed bias towards seeing the helicopter accelerate, while the other was biased toward seeing deceleration. The bias toward acceleration was consistent with the results of the main experiment, thus confirming that monocular cues may have played a role in the perception of acceleration in the first experiment.\r4. GENERAL DISCUSSION\rThe three main cues to motion-in-depth that have been studied in the literature are target vergence (absolute screen parallax of the helicopter in our study), relative disparity (difference in parallax, or relative parallax, between the helicopter and other stationary features), and changing image size \/ looming (here looming of the helicopter). One of the earliest studies done on this topic was by Regan and Beverly in 1979. Their experiments investigated the rate of change in disparity that was needed to null perceived motion in depth due to changing target size [8]. They determined that the contribution of disparity to motion in depth perception becomes more pronounced when either velocity or presentation times are increased. Subsequent studies have investigated the added role of target vergence, in addition to\rrelative parallax and looming in perception of motion in depth.\rFor instance, Brenner et al. [9] presented targets with simulated motion in depth based on various combinations of changing absolute screen parallax, changing relative parallax (presence or absence of other stationary features), and looming. Their participants adjusted the lateral velocity of a subsequently presented probe to match the target\u2019s speed of motion in depth. Matched lateral velocity was always less than the simulated motion-in-depth velocity. Changing screen parallax (target vergence) alone did not produce motion in depth (see also [10]) but could influence the matches when other cues were also presented. This suggests that target vergence alone was insufficient to produce motion in depth but could modulate it. Compared to our stimuli, the monocular cues to distance in these displays were minimal. Nevertheless, looming produced the most robust motion in depth and, somewhat unexpectedly as it was ambiguous to absolute distance, image size affected perceived target distance. Changing the screen parallax of the target, relative to static features, produced motion in depth although it was reduced relative to that experienced in the full cue or looming only conditions. Others have shown that the perceptual interpretation of the binocular cues depends on the strength of monocular looming cues. The latter can modulate the magnitude and even the direction of the perceived motion in depth [11], [12].\rCompared to these previous studies, in Experiment 1, rich monocular and binocular cues were available to specify the depth of the helicopter relative to the rest of the scene. Further, changing screen parallax was always accompanied by changing relative parallax. The absolute screen parallax of the helicopter at any time was nonlinearly mapped according to the depth remapping and thus the apparent motion due to target vergence should have manifested the predicted accelerations and decelerations.\rPredictions based on relative parallax are more complicated. Relative to the unwarped distant background (or near foreground), the changing relative parallax of the helicopter was distorted in the same manner as the absolute parallax and thus we would predict acceleration and deceleration of the helicopter under parallax re-mapping. However, note that the entire scene was also subject to the same parallax re-mapping as the helicopter. Therefore, during the motion both the path of the helicopter and the space it moved through were stereoscopically distorted and thus the distortion of local relative parallax was small, particularly for the floor altitude condition. In fact, any object stereoscopically aligned with the helicopter at a given frame in the linear animation would also be aligned with helicopter at the corresponding frame in the re-mapped animations. Thus when comparing the parallax (and predicted relative stereoscopic depth) of the helicopter over time relative to features at similar depth (say on the floor), the progress of the helicopter over the space should be identical regardless of the warping. In this case the\rdistortion of the motion path of the helicopter might depend on the distortion (or lack of) of the depth in the scene itself.\rWe have some evidence that in the presence of strong perspective and other monocular cues such distortions may be minimal. Previous studies on the effect of interaxial separation on perception of stereoscopic depth have shown that observers have a surprising ability to tolerate a wide range of interaxial settings without significant distortion [13]. If the apparent depth and shape of the scene were unperturbed by the re-mapping then we would predict that observer would not see the acceleration expected if the space was apparently distorted. McKee and Welch [14] have demonstrated that observers have poor ability to scale velocity for distance (velocity constancy) compared to size or depth. Therefore, even if stereoscopic space appeared distorted it may not have been evident in the motion profile of the helicopter.\rAnother important aspect of motion in depth studies related to our experiments is the observers\u2019 ability to perceive acceleration in moving objects. In an earlier study we assessed whether asymmetric distortion in the mapping of stereoscopic to real space as a result of varying interaxial has impact on the perceived acceleration of an object moving through that space under orthostereoscopic viewing [15]. Our data suggested that observers were able to discount distortions of stereoscopic space in interpreting object acceleration \/ deceleration [15]. Other previous work has suggested that observers are relatively insensitive to 2D or 3D acceleration. Gottsdanker, Frick, and Lockard [16] asked subjects to discriminate between accelerated and constant-velocity moving targets. They found that performance improved as mean velocity of the object increased, and presentation time decreased. They concluded, as did subsequent studies [17], that rather than being perceived directly, acceleration was detected by comparing early and late velocities. Later studies have shown that observers often misperceive acceleration resulting in overestimation times of arrival time in time-to-contact experiments [18], although the bias was reduced when binocular information was present. More recent studies have demonstrated that observers have high thresholds and systematic biases when judging acceleration based on image looming [19]. This general lack of sensitivity to acceleration of moving objects may underlie the lack of an influence of depth remapping on apparent acceleration that we found in the present study. However, we note that observers were able to discriminate acceleration in our stimuli (Figure 4). The just noticeable differences in the psychometric functions were considerably smaller than the predicted effects of the nonlinear disparity remapping. Thus, participants should been sensitive to the predicted effects of the remapping.\rIn contrast to the lack of an effect of parallax remapping we did find a significant effect of altitude. The small biases seen in the ground level condition suggests that observers were using ground plane cues in these judgements. As\rnoted above, any factor that influences perceived distance should impact the perceived acceleration of an object traversing through that distance. The importance of the ground surface in determining the perceived distance of objects in 3D scenes was noted by Alhazen about 1000 years ago [20]. Much later, Gibson [21] highlighted the importance of the ground surface in perception of 3D scenes. One aspect is the so-called height in the visual field, which reflects the geometric relation between distance and height in the optic array. As the horizon lies at eye level, everything on the ground is projected onto the lower half of the optic array [22]. As the distance from the observer increases, so does the height in the optic array. More recent research by Bian and colleagues [23] has posited a ground dominance effect in determining the perceived relative distances of objects in 3D scenes. They found that when presented with vertical posts in optical contact with either the ground or the ceiling, the observers perceived the one attached to ground surface to be closer.\rThe increased accuracy of acceleration judgements at ground level compared to eye level that we found is similar to improvements in other judgements when ground contact information was available. For example, Philbeck et al. compared walking and verbal report as indicators of distance with angular elevation (eye-level vs. ground level) as a variable. They found that adding height in the field as a distance cue (moving the target from eye-level to ground level) improved the accuracy of walked distance, thus providing evidence of angular elevation as an effective cue to egocentric distance [24].\rOur study has important implications for both content production and quality assurance of stereoscopic 3D media. Content producers face the challenge of increasing depth and maintaining the parallax budget, thus depth warping has become a popular and effective tool. Our research shows that observers have significant tolerance to parallax re- mapping; they generally are not sensitive to the differences in motion that theoretically result from re-mapping functions.\r5. ACKNOWLEDGEMENTS\rThis work was supported by grants to the 3DFlic project from the Ontario Media Development Corporation and the Ontario Centres of Excellence. Thanks to Rob Burton and Arc Productions for expert advice and assistance with the preparation of the stimuli. Also, thanks to Carly Hylton for help with the data collection.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823446","bibtex":"@INPROCEEDINGS{7823446, \rauthor={S. Laldin and L. M. Wilcox and R. S. Allison}, \rbooktitle={2016 International Conference on 3D Imaging (IC3D)}, \rtitle={The effects of depth warping on perceived acceleration in stereoscopic animation}, \ryear={2016}, \rpages={1-8}, \rkeywords={cameras;image coding;image sensors;object detection;optical distortion;stereo image processing;binocular parallax;camera level;geometric stereoscopic space distortion;helicopter altitude;image depth-to-parallax mapping;nonlinear parallax re-mapping functions;object detection;quality assurance;stereoscopic 3D scene capture;stereoscopic animation;stereoscopic depth adjustment algorithms;stereoscopic depth warping algorithms;stereoscopic post-processing;stereoscopic space compression;stereoscopic space expansion;Acceleration;Animation;Cameras;Helicopters;Nonlinear distortion;Stereo image processing;Three-dimensional displays;acceleration;depth warping;motion in depth;stereoscopic 3D media}, \rdoi={10.1109\/IC3D.2016.7823446}, \rmonth={Dec},}"},"paper7":{"title":"A NEW DESIGN AND ALGORITHM FOR LENTICULAR LENSES DISPLAY","authors":["Rene\u0301 de la Barre\u0301","Roland Bartmann","Mathias Kuhlmey","Bernd Duckstein","Silvio Jurk","Sylvain Renault"],"conference":"2016 International Conference on 3D Imaging (IC3D)","abstract":"In this study, a novel autostereoscopic 3D display design based on our patent specification is proposed and demonstrated. In contrast to common autostereoscopic two- view-designs with lenticulars, our approach allows more distance between image splitter and display panel. In comparison to 3D displays showing details at nominal distance, our design projects the images to a near zone in front of the display. As a consequence, the low magnification factor reduces distance errors from relative movements of display panel and image splitter. An image processing algorithm was developed to arrange the content and correct display shortcomings of image allocation caused by optical properties of the lenticulars. In addition, this algorithm allows static adjustment and tracking of observer position in a defined area in front of the display. We integrated the display in a production chain for digitalization and presentation of cultural heritage objects and developed special web-based modules for stereo rendering and user interaction, so that observers may explore cultural content in a natural manner.","keywords":["autostereoscopic","3D display design","lenticular","air-image","low crosstalk","3D visualization","x3dom"],"content":"1. INTRODUCTION\rToday, new desktop and tablet displays show a highly increased resolution. This is an advantage for the design of autostereoscopic displays (ASD). ASD need several interleaved images and with such a high overall resolution it is possible to reproduce a good single view resolution, as well. Due to smaller subpixel structures, the image splitter needs a reduced lens pitch as well as increased accuracy when aligning with the LC-panel. By getting the image splitter as close as possible to the display structure, some properties of image separation are unfavorable. The influence of distance flaws will be increased. The smaller distances affect the lenticular pitch and thereby optic aberrations like the Petzval field curvature [1, 2]. Due to the latter, the quality of the stereo image deteriorates with a wider observing angle. A design approach to reduce angle dependence and panel waviness is an increase of lenticular lens radius. This is familiar from the patent literature [3]. The concept is based on a smaller magnification factor of the proposed 3D system. Thereby the stereo image is composed by several different image strips.\rAlso quite important for perception of good 3D images is the delivered depth. Our novel 3D display can show more depth than conventional flat autostereoscopic designs with smaller lenticulars. The presented depth of an autostereoscopic panel is proportional to the lens pitch [4]. Therefore, the proposed design is equipped with an image splitter with comparatively large lens pitch in the millimeter range. That concept of lower magnification was already used for ASD design [5]. It is significant different to the traditional design approaches known from literature [6].\rWe implemented the idea of aerial-image representation. The intermediate image in the so called air-image plan can be made visible by a physical plane for instance a simple sheet of paper. To get such kind of image, the matrix pixel plane has to be located somewhere between simple and double focal point distance of the lenticular. The 3D model, image algorithms, optic simulations and experimental validation of the novel two-view ASD design will be presented.\rTo bring out the quality of the 3D display, we integrated it in a content production chain for cultural heritage objects combining different scanning techniques, computer graphic methods and 3D web rendering tools. Such models are mostly complex and should be visualized with a lifelike level of details. Additionally, we developed a web compatible display driver and appropriate interaction modules in JavaScript and HTML for the Fraunhofer IGD renderer x3dom, a web-based library allowing a seamless integration of 3D content into a webpage [7].\r2. DISPLAY MODEL\rConventional ASD are often multi-layered flat panel designs. Our novel air-image setup is also based on the lenticular image splitter type. The lens grid separates the stereo strips in the pixel plane of the display. In contrast lenticular parameters and distances are different from the majority of common designs and the underlying principle is described in detail in former literature [3].\rThe geometric conception of the proposed 3D display is depicted in Fig. 1. Different light paths of an exemplarily chosen stereo image section are shown. In detail, several subpixels of the matrix panel MP are emitting light rays of displayed image content to the observer eyes. All image strips b will be projected as optical air-image in AP through a vertical aligned cylindrical lens array BS. The projected air-image consists of different B sections.\r978-1-5090-5743-6\/16\/$31.00 \u20ddc2016 IEEE\r  Fig. 1. Top view scheme of the air-image display with red for left and green for right content, light ray distribution from an exemplary part of matrix pixel panel MP through the image splitter BS to the eyes position (at the middle of projection width C in distance P) in nominal plane NP; b is a single image strip in MP and B the projection in the air plane AP; A, a and D are different distances between AP, BS, MP and NP.\rHowever, optical imaging of real objects is applicable for cylindrical lenses only at the perpendicular profile of cylinder axis [1]. The human eye is able to dim the resulting image intensity and therefore able to separate a single view direction-dependent. The eye pupil of the observer in nominal plane NP selects images in viewing direction out of a variety of stacked projected images in AP and provides a comparable sharp image perception. In our model, stereo sections are calculated based on the geometrical stripe width and distance parameters. For the eye, only image areas are visible inside connection line between the eye pupil periphery and the edge regions of the cylindrical lenses. These sections B in the geometric scheme in Fig. 1 are projections of left and right image content of strips br and bl. The allocation of the single image stripes is depicted in detailinFig.2.\rq=  P (3)  \rFig. 2. Magnified part of image splitter BS and matrix pixel plane MP with red for left and green for right content, different stripe sections and distances are depicted.\rThe image segments b, equals the image strip pair bl and br, are dependent on the ratio of air plane AP and image splitter BS to matrix plane MP. From condition a < A follows b < B with magnification factor \u00df, see equation (2).\rb = \u00df B and \u00df=  (2)\r In MP the distance of the center sections bl and br will be defined as q in equation (3). Thus parameter q is proportional to distance a between BS and MP.\r In equation (4) the periodical block distance Q of one image strip pair to another is the projection of lens pitch L from single eye to matrix plane with q << P.\rQ = L   +   (4)  \r Furthermore the block distance Q is approximately equal to lens pitch L. The gap between two image strips will be defined by parameter s in equation (5) and is dependent on the ratio of distances a and D.\rs=q\u2212b (5)\r B = L    \u2212   (1)\rg  = \u22c5 Q\u2212q  \u2212b ,i=1...4 (6)  =s\u2212 \u22c5g (7)\rThen the subdivision of s in smaller gaps is depicted in Fig. 2 and equation (7). The parameters gi equal to reserve sections 1 to 4, see equati on (6), are symmetric and of the same value here.\r Resulting from the equation (1), the strip width B of projected air-image sections is smaller than lens pitch L. In detail, it is inverse proportional to air-image distance A.\rThus the gaps between image strips b will rise by increasing the air-image distance A.\rThe distances and widths determined by equation (1) to (7) are dependent on different geometric parameters L, A, a, D and P. The first three L, A and a are defined by the arrangement of the 3D display and therefore once set, they\r are constant. The distance D in contrast is dependent on the current position of the observer and P on the interpupillary distance of the observer\u2019s eyes. This results in a change of the widths B and b. The gap sections \u0394 and gi need to be updated by tracking the observer when he changes his distance to the screen.\r3. IMAGE PROCESSING\rA well-known optical purpose of a lenticular array BS in a 3D display is to magnify and split the image information on the pixel plane MP. Thus, it separates the views to eye positions of the observer in NP. A common approach is to get an arrangement, where each lenticular lens magnifies a group of subpixels. By using an exemplary lens pitch of eight subpixel widths, the observers are able to look at up to eight different images. For such a case the allocation of subpixels with appropriate content information decides whether it is used as a single user or multiview system. As another detail, the pixel panel is not exactly located in the focal point. Because of angle dependent aberration effects and unwanted image strip enlargements, the pixel plane MP will be arranged close to the focus of the lenticular lenses.\rIn our case MP lies behind the focal plane of the lenticular lenses, therefore more than one subpixel in its width will be captured.\r3.1 Image generation\rThe image allocation used for image stripe arrangement will be explained in detail, see Fig. 3. These sections are described as bl and br and will be reproduced, laterally reversed on stereo image layer AP. Therefore, the observer only sees a partial section of width b. Inside range c it is possible to locate N pixels and therefrom in section b a subset of it. For the case that the observer distance to the display will be changed, the pixels in an image strip shift from visible areas to invisible areas and vice versa. Meanwhile the observer is still looking at an undisturbed image. Because the width of reserve sections gi is dependent on the change of distance to the panel.\rThe parameter N is the number of all subpixels for a display row, where Q is the number of subpixels for each content block with the index is. The parameter x describes the subpixel address in x-direction for each display row. The variable indices r and l represent right and left content for the respective eyes of the observer. Finally, reserve sections g1 to g4 are currently invisible areas. The basic idea of the algorithm is that with different strip widths br and bl the image information needs to be scaled too. The image information will be distributed over the whole number of subpixels.\rHowever, the section size for lateral movements can be increased by tracking. Then the observer position will be captured and thus affects the allocation of pixels for each image strip. In addition distance changes can be adapted by z-tracking.\rf\r Fig. 3. Schematic, representation of a horizontal pixel row segment in MP (front view), image stripes bi and reserve sections gi, uniform block width Q and resolution N.\rThe arrangement requires a new interleaving pattern and the use of a standard side-by-side stereo image format. This image format has to be converted internally in horizontal anamorph images with a side ratio of approximately 1:3. In our experimental setup, see further details in chapter 4, we used a magnification ratio of 1:2.4. The image has to be divided in stripes that partially overlap. Therefore, redundant information will be inscribed in the peripheral image stripe areas. The position of the stripes inside the partial image can be changed dynamically by controlling the interleaving parameters through a tracking signal. This will enable stereo image adaptation to the tracked observer position.\r 3.2 Image algorithm\rIn mathematical terms, the algorithm implements a\rtransformation of pixel coordinates from an initial image\rwith horizontal resolution N0 to converted pixel coordinates\rdisplay is th e zero point for the x-coordinate, angle  \rof the used image for the 3D display. The edge point of the\rdescribes the viewing angle. A source image with a given\rcoordinates     ,    of an image composed by image strips  , \rhorizontal resolution N0 will be converted to pixel\rthat are merged together in a different order, see equation (8). The resolution of right and left image has to be the same. Otherwise we have to scale it to the same resolution.\r     ,  =   \u22c5[ +  + h   \u00b1    \u22c5  \u22c5 ] (8)  ,           \r   This equation is the mathematical representation of that coordinate transformation where x\u2019 is the x-coordinate of the source image for left or right image. The value N0\/N describes a resolution ratio. Furthermore, xs is a shift value that represents the slope of the grid and also includes the observer position dependent on a tracking value. This value influences the x-position of the image. A tracking of the observer distance will be realized by changing the parameter Q. Equation (4) shows this relation. The value h(x) as block position in equation (9) describes a coordinate mapping within one block. If x is at the right section of content information then Q0 = 0 and if x is at the left section then Q0 = Q\/2.\r + \rh    =    \u2212             \u22c5   (9)\rFig. 4. Exemplarily chosen single lens for simulation of angle dependent light ray distribution between BS and MP, Focal plane FP and Petzval field curvature are depicted, viewing image stripe width is proportional to viewing angle.\rThe width of the image stripes in MP is angle dependent. Hence the observer in viewing distance D perceives different image stripe widths from respective viewing directions. For the experimental setup from Tab. 1 the viewing angle is in the range from 0 to 14\u00b0. The stripe width will be increased due to wider viewing angle and more distance to the focal point of a single lens element. The reasons for that are aberration effects or simplified the Petzval field curvature due to inclined incidence of light [8]. Fig. 4. shows the angle dependent stripe width by use of the optical simulation software OpticstudioTM. For specific arrangements or 3D display setups the relation between angle and strip width has to be investigated in detail. The parabolic dependence of stripe width from viewing angle and wavelength is depicted in Fig. 5. The results were determined by optical simulations and the weighted average data was mathematically fitted by a parabola. Function b(\u03b1) in equation (10) describes that angle dependency, where c0, c1 and c2 represent the constants of regression.\r In addition to equation (8), the disparity value d results from position change of image left against image right. F and b(\u03b1) describe the required scaling factors to corresponding pixel positions of the original image. The scaling compensation factor F corrects the offset error. Function b(\u03b1) is angle dependent and results from a correction of strip enlargement by an increased observation angle \u03b1 that will be described in detail in the following chapter.\r3.3 Light ray simulation and error correction\rThe image processing algorithm was tested and validated by optical modeling software and in the experiment; the used parameters, with pixel pitch p, are listed in Tab. 1.\rTab. 1. 3D display simulation parameters\r    Parameter\r Value\r  Unit\r  P\r 65\r Mm\r  D\r 1,2\r M\r  a\r 69,5\r mm\r  f\r 53,8\r Mm\r  p\r 0,11655\r mm\r  N\r 5120\r Pixel\r       \r  Lenticular array\rVisualizing screen\rIn common autostereoscopic lenticular lenses displays the dimmed stereo image by eye aperture is directly composed on the image splitter plane BS. In contrast, the proposed novel 3D display depicts a real air-image in front of the lenses array at AP composed of stereo image stripes.\rMP BS\rFP AP\rFig. 7. Horizontal light distribution with alternating colored strips in several distances to image splitter BS and pixel plane MP, anterior focal plane FP is barely visible; RGB- pattern is sharply separated in 17 cm distance from BS at air-image plane AP.\rHowever, to prove the projection of the air-image a RGB- test image with three respectively different colored image stripes was created by our adapted image allocation algorithm. All image sections br were filled with one equally\r      Fig. 5. Adaptation of Petzval field curvature, wavelength and angle dependent strip width in pixels was derived from optical simulation data and fitted by a parabola curve.\rFig. 6. 3D display setup with test content.\r4.2 Proof of air-image\/stripe distribution\r     =      +     +      \r(10)\rThe parameters for average adaptation of the Petzval field curvature were c0 = 12.5, c1 = 0.206 and c2 = 0.045. The angle \u03b1 respects the viewing position in front of the display. Therefore, the correction is realizable for tracking as well. The position of the observer can be transformed into this angle. The depicted adaption in Fig. 5 limits the possible viewing space by its range and the feasible maximum stripe width. The application of the proposed strip width adaptation is shown in the image presentation in chapter 4.\r4. EXPERIMENT, RESULTS AND APPLICATION\rAn autostereoscopic air-image 3D display was developed and examined by using a 5K panel with a large-sized lenticular lens grid.\r4.1 Display setup\rThe 27\u201d 3D display mock-up was realized on a test carrier with micro meter drives for distance adjustments. In Fig. 6 the display setup consisting of a vertical lenticular array and a matrix panel in landscape format is depicted. An additional reflecting screen for visualizing of light ray paths was attached too. The used display was a DELL UP2715K and the lenses raster plate was produced in resign on glass technology. The cylindrical lenses have a pitch of L = 5.91 mm. For the display control a Windows 8 PC (ASUS ROG G20AJ-DE045S) with an NVidia 980 GTX graphics card was applied. A GPU based software render tool developed by Fraunhofer HHI was used for image processing and content multiplexing. The algorithm used the acceleration of DirectX11 libraries to generate the displayed pattern. For the documentation of results a CANON EOS-1 Ds Mark II camera and contrast enhancement software were used.\r        \r  distributed RGB-stripe. The emerging ray picture was visualized on a white screen, shown in Fig. 7. In different distances to BS, several centimeters away from focal plane FP, the color distribution is changing. The best color separation was provided by the real air-image in 17 cm distance at air-image plane AP. The perceived RGB-image is merged correctly in that distance only. The RGB-stripe pattern presents either the left or right view of the stereo image. It was demonstrated that periodical superimpositions with alternate changing colors exist in the projected light field.\r4.3 Presentation of stereo images floating in air\rFig. 8. Mock-up of the 3D display showing content from Fraunhofer cultural heritage project. The view is assembled from the overlapping image stripes.\rTo operate and test the new 3D display, we implemented new rendering modules and used scanned objects created by the Fraunhofer cultural heritage project. The main objective of this current project is to create a processing chain for the digitalization and the presentation of cultural objects as sculptures, statues and parts of historical monuments, into consideration on their different materials (sandstone, marble, wood etc.). These objects are very suitable to bring up the stereo capabilities and quality of the display because they have complex forms and contain important details that have to be presented in a lifelike manner. In general, digitalization can be done by scan sensors (new methods are developed and tested in the project) as well as by 3D reconstruction algorithms for static objects and dynamic bodies [9].\rFig. 8 shows first results of scanned objects viewed on the new display and rendered in real-time inside an interactive webpage, here in full screen mode (from left to right: \u201eKreuzblume\u201c by Fraunhofer IPM and Mu\u0308nster Freiburg; \u201eTerracotta Krieger\u201c and \u201eDave\u201c by Fraunhofer IGD). To render the scene in stereo and process user interaction as body gesture, we created some JavaScript and HTML extensions in the x3dom library.\rFig. 9. Magnified image section of Fig. 8, on the left side without and on the right side with applied correction algorithm for image stripes (1-8).\rSpecial display drivers and tracking modules for head and hand tracking were implemented and tested. Interactive web-based 3D scenes for suchlike auto-stereo displays can be done in an efficient way. The content shown in Fig. 9 was rendered via the extended library and processed by image allocation and correction algorithms described in chapter 3. The two pictures demonstrate the differences in the image strip alignments. In table 2, N0 describes the resolution of the source image shown in Fig. 8 where N is the resolution of the pixel panel.\rTab. 2 Constant input parameters of the proposed image processing algorithm equation (8) for Fig. 8 and 9.\rThe values Q and s depend on the viewing distance and system parameters of the 3D display. Due to the chosen setup the slope angle of the grid is zero. The central camera position is fixed in front of the panel, therefore xs is constant zero. In addition, the disparity value d is zero because there is no shift between the images. The parameter Q0 is zero due to the block position for the right views. F as compensation factor results from calibration to improve the image quality.\rTab. 3 Parameter comparison of variable values in pixels by stripe numbers shown in Fig. 9.\r     N0\r N\rQ\r  s\r xs\rd\r  Q0\r  F\r  1920\r 5120\r53,67\r  9\r 0\r0\r  0\r 1,17\r pixel\r pixel\rPixel\r  pixel\r pixel\r pixel\r  pixel\r       No.\r  1\r2\r  3\r 4\r5\r  6\r 7\r  8\r x\r  4373\r4427\r  4480\r 4534\r4588\r  4641\r 4695\r 4749\r\u03b1\r   10,0\r10,3\r  10,6\r 10,9\r11,1\r  11,4\r 11,7\r 12,0\r h(x)\r  0,8\r0,5\r  1,2\r 0,8\r0,5\r  1,2\r 0,8\r 0,5\rb(\u03b1)\r   19,1\r19,4\r  19,7\r 20,0\r20,4\r  20,7\r 21,1\r 21,5\r xr\u2019(x,\u03b1)\r  1646\r1666\r  1687\r 1707\r1726\r  1747\r 1767\r 1786\r xr\u2019(x,0)\r  1650\r1670\r  1691\r 1710\r 1730\r   1751\r  1771\r  1790\r       \rThe results shown in Tab. 3 exemplarily demonstrate the enlargement of the visible image stripes 1 to 8. In detail the angle dependent correction of b(\u03b1), see Fig. 5, and xr\u2019(x,\u03b1) was used. However, the selected area in the image is almost linear and the difference to xr\u2019(x,0) is about four pixels.\r5. SUMMARY 5.1. Discussion of results\rFor this considered single user approach, with a magnification of 2.4:1, a plurality of pixels will be covered by one lens for each row. Several content-dependent subpixels are joined together to constitute an image stripe either related to the left or right observer eye. Invisible from the current observer position, adjacent subpixels ensure reserve zones and allow a wider viewing space. Light rays from the intermediate image in the projection plane were selected by eye aperture. Thus, the observer will perceive a 3D stereo image with good quality and depth.\rCompared with conventional 3D display designs of 100- times magnification and column multiplexing, it is a lot easier to adjust display matrix and lens grid. The influence of display waviness and distance errors of lens grid and panel are minimized. For the experiment a non-corrected lens grid with wide cylindrical lenses was used. It could be proven that the used image processing algorithm allows the dynamic correction of the arrangement and optical errors.\rThe perceptible displayed resolution is defined by the distance-dependent magnification. Compared to the observing position a tracking is necessary for representation of image content with no delay.\rThen, the stereo image will be continuously adjusted by the position of the user\u2019s head. The scene will be explored from different perspectives while the observer moves laterally in front of the display. Furthermore, the user will be able to interact with the virtual content via gesture in a natural manner (e.g. for rotation, selection, etc.) because virtual objects float in the air, close to him.\r5.2. Conclusion\rA new approach for designing two-view ASD was presented. The effects of this design were discussed. Therefore an optical ray tracing model and various image processing algorithms were developed and tested by simulation and experiment. It was proven that a real 3D image with high quality and low magnification is feasible. The low image magnification reduces the influence of misalignment errors between display screen and lenses raster on the viewing quality. Then the enhanced distance is large compared to the waviness of the display panel.\rThe substrate layer for the lenses creates only low intra ocular crosstalk effects. Furthermore the perceptible 3D depth is increased, compared with common 3D displays.\rThe integration of the display and software modules in a production chain for digitization and presentation of museum and cultural objects demonstrates well the feasibility of the techniques. Now, with this web-based solution (x3dom renderer and extensions) 3D content can be presented on such stereo displays over the internet, lifelike, with a high fidelity and depth. Thus, the floating object close to a museum visitor tempts him to interact with it. Of course, the interactive stereo display can also be used for other scenarios e.g. for automotive, construction and medical applications as well as for gaming or entertainment.\r6. ACKNOWLEDGEMENT\rWe would like to thank the Mu\u0308nster Freiburg as well as the Fraunhofer IPM and IGD for the provided image content. Financial support by Fraunhofer-Gesellschaft, Munich (grant no. 6   96) is gratefully acknowledged.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823447","bibtex":"@INPROCEEDINGS{7823447, \rauthor={R. de la Barr\u00e9 and R. Bartmann and M. Kuhlmey and B. Duckstein and S. Jurk and S. Renault}, \rbooktitle={2016 International Conference on 3D Imaging (IC3D)}, \rtitle={A new design and algorithm for lenticular lenses display}, \ryear={2016}, \rpages={1-7}, \rkeywords={lenses;object detection;optical design techniques;optical images;stereo image processing;three-dimensional displays;Web-based modules;autostereoscopic 3D display design;autostereoscopic two-view-designs;cultural heritage object digitalization;cultural heritage object presentation;image processing algorithm;image splitter;integrated optics;lenticular lens display;stereo rendering;user interaction;Image resolution;Lenses;Mathematical model;Observers;Optical imaging;Strips;Three-dimensional displays;3D display design;3D visualization;air-image;autostereoscopic;lenticular;low crosstalk;x3dom}, \rdoi={10.1109\/IC3D.2016.7823447}, \rmonth={Dec},}"},"paper8":{"title":"BODY EXPRESSION RECOGNITION FROM ANIMATED 3D SKELETON","authors":["Arthur Crenn","Rizwan Ahmed Khan","Alexandre Meyer","Saida Bouakaz"],"conference":"2016 International Conference on 3D Imaging (IC3D)","abstract":"We present a novel and generic framework for the recogni- tion of body expressions using human postures. Motivated by the state of the art from the domain of psychology, our ap- proach recognizes expression by analyzing sequence of pose. Features proposed in this article are computationally simple and intuitive to understand. They are based on visual cues and provide in-depth understanding of body postures required to recognize body expressions. We have evaluated our ap- proach on different databases with heterogeneous movements and body expressions. Our recognition results exceeds state of the art for some database and for others we obtain results at par with state of the art.","keywords":["Body Expression Recognition","3D Skele- ton","Features Extraction","Animation"],"content":"1. INTRODUCTION\rMany applications would benefit from the ability to under- stand human emotional state in order to provide more natu- ral interaction e.g video games, video surveillance, human- computer interaction, artistic creation, etc. Emotion is a com- plex phenomena. Expression of an emotion could be personal and subjective as two persons could perceive and interpret dif- ferently the same expression. Its perception changes from one culture to another. Furthermore, human expresses emotional information through several channels, like facial expression, body movement and sound. Several studies from various dis- ciplines have shown that body expressions are as powerful as those of the face to express emotion [16]. Unfortunately, to the best of our knowledge, few publications have focused on this issues.\rThis paper presents a method to detect and classify ex- pression through a sequence of 3D skeleton-based poses. The challenge is to find common informations between all the movements of the same expression. And this, while the ex- pression is embedded in the action performed by the per- son, e.g. nervous person walking pattern contains the ac- tion of walking and the emotional state to be nervous. With the growth and ease of accessibility of devices that track 3- dimensional body like the Kinect [21, 3] or accelerometer-\rbased motion capture system [23], it is easy to get data. Thus many applications will be benefited from real-time analysis of body movements. There are lots of method proposed in literature for facial expressions [5, 15, 12, 14], however, as mentioned above, there is a lack of research for the analysis of body movement to recognize expressions. Early proposed methods for body movements analysis [13, 18] were limited to specific movements or expressions. Taking an approach proposed by choreographer seeking to describe dance move- ments, Truong et al.[24] proposed descriptors calculated from the movements in order to recognize various gestures and ex- pressions. Inspired by Truong\u2019s work and results of psycho- logical research [16], we have looked to quantify the space posture of the body, during an action, by introducing new set of features to characterize an expression. Besides, its com- putational simplicity allowing the recognition of expression on heterogeneous body movements in real-time. Our method has the advantage to be reusable in many domains as move- ment synthesis processes for computer graphics and anima- tion. The paper is organized as follows. Section 2 presents the state of the art of the emotion analysis. Section 3 details the set of features we proposed. Section 4 shows the results obtained on different databases and the comparison with other state of the art\u2019s method. Finally, Section 5 concludes the pa- per.\r2. RELATEDWORK\rIn recent years, researchers have considerably focused on the automatic facial expression recognition (FER) [5, 15, 12]. How- ever, computer vision domain is lacking research on the de- tection of emotion based on human\u2019s posture. In analogy af- terward we use the term \u201dBody Expressions\u201d to refer to emo- tion expressed by body posture. While, psychological studies show that the human\u2019s posture is as powerful as facial expres- sions in conveying emotions [20]. That\u2019s why, psychologists have sought to understand what bodily information is neces- sary for recognizing the affective state of one person. Psycho- logical studies [2, 10] have investigated the part of the body form versus movement in affect perception. These studies conclude that both form and motion information are impor- tant for perceiving emotions from body expressions. Finally, after the form and movement features, psychologists have in-\r978-1-5090-5743-6\/16\/$31.00 \u20ddc2016 IEEE\rtroduced two main levels of bodily details which are high and low-level descriptions [16]. The high level description is of- ten based on the Laban approach which describes body ex- pressions in a global way. The low description is another ap- proach to describe body expression by providing more precise features like distance between joints and angle between body segments. Some techniques related to psychology studies fo- cus on very specific actions. Bernhardt and Robinson [4] pro- posed a framework to detect implicitly communicated affect of knocking actions. Their method is based on segmentation to divide complex motions into a set of automatically derived motion primitives. Then, they analyzed the parsed motion in terms of dynamic features, i.e. velocity, acceleration and jerk. They obtained a range of 50% to 81% correct classification rate and we are also going to compare our method with their results (refer Section 4 for comparison). Karg et al.[13] pro- posed method for gait patterns recognition. They investigated the capability of gait to reveal a person\u2019s affective state. Ac- cording to their study, speed, cadence and stride length are important factors to correctly discriminate different expres- sions of a human gait. Paterson et al.[22] confirms the role of velocity for the discrimination of affect. They performed visual experiments and concluded that speed plays vital role in the perception of affect.\rBelow mentioned are few state of the art articles that pro- posed heterogeneous body movement analysis. Castellano et al.[7] proposed a method for automated video analysis of body movement and gesture expressiveness. They used move- ment expressiveness to infer emotions. They also present two methods for the classification. The first one is based on a direct classification of time series whereas the second one uses a meta-features approach. A meta-feature is the statis- tical calculation on a set of features in order to abstract the time. They obtained correct classification accuracy of 61% on their database which contains four expressions (Anger, joy, pleasure and sadness). Kleinsmith et al.[18] proposed a sys- tem that incrementally learns to recognize the body expres- sion. Their system is based on form features (i.e. distance from one joint to another one) and motion features. They ob- tained a correct classification rate of 79%, we will compare our method with their results (refer Section 4 for compar- ison). Truong et al.[24] proposed a new set of 3D gesture descriptors based on the laban movement analysis model for gestures expressiveness. They obtained high recognition rates for action recognition (F-Score: 97%) on Microsoft Research Cambridge-12 dataset [9]. They also tested their classifica- tion approach on their own proprietary database which con- tains 882 gestures and achieved best F-Score of 56.9%.\rWe can take inspiration from the domain of animation in computer graphics as they also analyze body movement and gesture expressiveness. The researchers working in the domain of animation generation proposed different methods for modification of animation using skeleton transformations. This modification of expression can be achieved by chang-\ring the timing, speed and the spatial amplitude of the motions of body part joints. [1, 11]. Some researchers [6, 25] have also used methods from signal processing (Fourier transfor- mation, motion wave-shaping, time-warping, etc.) in order to modify the expression of animations. Recently, with the same goal of synthesis expressive animations, Forger and Takala [8] proposed an approach based on the motion signals by using frequency components.\r Fig. 1: Overview of our framework.\r3. PROPOSEDFRAMEWORK\rOur motivation was to propose an efficient recognition method of body expressions on heterogeneous movements that rivals state of the art and in real time on a standard computer. Fol- lowing the foot prints of several previous works [18, 24, 8], we propose a set of novel descriptors based on geometry, mo- tion and frequency-based of body part joints. The overview of our approach is given in Figure 1 and in the following three steps. The rest of this Section is dedicated to the presentation of the features proposed by our method.\r1. Our framework computes low-level features for each frame of the motion capture data. We decomposed our features in three types: geometric features, motion fea- tures and Fourier features. The details of these features are described below.\r2. Startingfromalltheselow-levelfeaturesobtainedfrom a time sequence, we compute the meta-features i.e. mean and standard deviations for each feature. Since meta- features are independent of the time, we avoid the com- putationally complex time-warping step for synchro- nizing two animations.\r3. The resulting values of these meta-features are fed to the classifier which provides the expression classifica- tion of the input motion capture data.\rThe low-level features we propose are based on distances between body joints, area of triangle defined by specific joints, angles, velocity, acceleration and frequency of movement of different joints. Table 1 describes the set of all our basic fea- tures. We have a total of 68 features. Then, for each feature we compute the meta-features. Thus, our approach leads to a total number of 136 features in order to recognize the body expression. Figure 2 shows the set of meta-features for three expressions. For visual purpose, we are only showing first 76 features from the total of 136 dimensional feature vector. It can be observed in the referred figure that our features have a discriminative trend for these expressions. This discrimi- native ability is used in the proposed framework to robustly classify body expressions.\r(a) Histogram of our features extracted from happy knocking\rstudies, i.e., form and movement, high and low-level descrip- tions of bodily detail.\rThe first feature V, is the global space occupy by the skele- ton. We use the size of the bounding box of the skeleton in the three directions. The second feature \u03b8 is the angle defined between the vertical axis y and the axis binding the center of the hip to the neck. According to studies in psychology, a per- son tends to extend his body for positive expression whereas for negative expressions a person takes a more compact and forward tilt posture.\rThe following features are dedicated to correctly capture the configuration of each body part. Distance D between two joints is important feature to analyze body expression. For in- stance, we use the distance between hand to the shoulder on the same side. This distance gives indirect information about the elbow. We use distances between hands to hips, hands to shoulders and elbows to hips. Furthermore, a new idea intro- duced by our approach is the use of triangles A to correctly discriminate multiple expressions. The body being symmet- ric, we take joints on each side of the body, right and left side. The last point of the triangle is chosen on the axis of the body, i.e. either neck or hips. Since the majority of move- ments is anti-symmetric, the triangle gives lot of information about the expression. For each triangle, we compute its area and the three angles formed by these different triangles. With these four values, we extract information related to shape of the body. For instance one of these triangle-based feature is the area and the angles of the triangle defined by the neck and shoulders. Figure 3 shows the variation of triangles for two different body expressions taken at the same timing of walk- ing. We compute the area and the angle of triangles defined by the hands and the neck; the shoulders and the neck; the elbows and the neck; the hands and the hips.\rAs mentioned in the state of the art, psychologists have showed that motion is an important characteristic to discrim- inate expressions for different gestures. Thus, we add to the feature vector the motion features: the velocity \u20d7v and the ac- celeration \u20d7a of the hands, shoulders, hips, head and elbows joints. The velocity is the first derivative of the position of the current joint. Acceleration being the second derivative of this position.\rFinally, we use the fast Fourier transformation F to ob- tained the frequency component of our selected joints. We compute the discrete Fourier transform with the fast Fourier transform algorithm on the signal of the joint angle. Section 4 compares the results by using only geometric features, only motions feature or only the Fourier features on the different databases.\r4. RESULTSANDANALYSIS\rWe have tested our method on three databases (refer table 2 for summary of databases) which are presented below. Two of them are acted databases, while the last database consists\r   (b) Histogram of our features extracted from sad knocking\r(c) Histogram of our features extracted from angry knocking\rFig. 2: Features used for different expressions. These his- tograms show the discriminative property of our features.\rAccording to the studies of [13] and [4], we have decided to use the following joints for feature extraction: head, pelvis, elbows, shoulders and hands. Experimental results obtained by [13] showed that head, pelvis, elbow and shoulder joints embed most of the emotion of a movement. Results in [4] proved that hands are also important in conveying expressions for several action. Thus, analysis of these five joints is most important to recognize body expressions. All the features ex- tracted for body expression analysis are presented in Table 1 and are described below. We used many features to correctly fit to aspects of body expression described by psychological\r   Id.\r Type of feature\r Description\r  V\r Space of the skeleton\r Size of the bounding box of the skeleton\r  \u03b8\r   Angle\r  The three angles induces by the triangle formed by both shoulders and neck\rAngle between the vertical direction y and the axis binding the center of the hip and the head\r   D\r           Distance\r      Right hand to the hips\rLeft hand to the hips\rRight hand to the right shoulder Left hand to the left shoulder Right elbow to the hips\rLeft elbow to the hips\r       A\r       Area\r    Triangle defined by both hands and neck Triangle defined by both shoulders and neck Triangle defined by both hands and hips Triangle defined by both elbows and neck\r     \u20d7v\r         Velocity\r     Hands Shoulders Hips Head Elbows\r      \u20d7a\r         Acceleration\r     Hands Shoulders Hips Head Elbows\r      F\r           Frequency\r      Change of angle with respect to time of the following joints : Hands\rShoulders\rHips\rHead Elbows\r             Table 1: List of set of features that we propose to extract from the human postures for the classification of body expressions.\r  DataBase\r  Number of movements\r Number of expressions\r UCLIC [17]\r  183\r 4\r Biological [19]\r  1356\r 4\r SIGGRAPH [26]\r  572\r 8\rof synthetic animations generated by the method of Xia et al.[26]. We will refer the last database as SIGGRAPH database in the rest of this paper.\r1. Biological Motion [19]: this database consists of 1356 motions, especially knocking actions for 4 expressions (angry, neutral, happy, sad).\r2. UCLIC Affective Body Posture and Motion [17]: this acted database contains 183 acted animations for 4 ex- pressions (fear, sad, happy, angry).\r3. SIGGRAPH database [26]: the SIGGRAPH database is synthetic database generated from 11 minutes of mo- tion capture data. It contains 572 animations with 8 expressions or style (angry, childlike, depressed, neu- tral, old, proud, sexy, strutting). In above mentioned databases, the SIGGRAPH database includes the largest range of movements: jump, run, kick, walk.\rTable 2: Description of the databases used for the test of our method.\rSummary of results obtained by our methods using the different sets of features are presented in Table 3. Our pro- posed approach can run in real-time as it runs at  \u030350 fps (on PC running on i7-4710MQ with 8GB of RAM). Extraction of proposed features, except Fourier features, takes 10ms for each frame while the extraction of Fourier features takes 500ms for a sequence of 27 seconds with 1657 frames. Before the classification, we are scaling each attribute of our features to the range [1,+1] to avoid numerical difficulties during the cal-\r    \r   DataBase\rSet of features used\r  Results\r  UCLIC\rAll features\r  78%\r  UCLIC\rOnly geometric features\r  66%\r  UCLIC\rOnly motion features\r  52%\r  UCLIC\rOnly Fourier features\r  61%\r  SIGGRAPH\rAll features\r  93%\r  SIGGRAPH\rOnly geometric features\r  92%\r  SIGGRAPH\rOnly motion features\r  75%\r  SIGGRAPH\rOnly Fourier features\r  90%\r  Biological\rAll features\r  57%\r  Biological\rOnly geometric features\r  56%\r  Biological\rOnly motion features\r  48%\r  Biological\rOnly Fourier features\r  46%\r               (a) First frame of the depressed walking animation.\r(b) First frame of the proud walking animation.\rTable 3: Our results on the different databases and with dif- ferent set of features. Classification method is SVM.\rscenario for machine learning algorithm. Proposed frame- work achieved best results on the synthetic database (SIG- GRAPH) which is due to the fact that synthetic animations presents exaggerate expressions with high inter-class varia- tions.\rTable 4 shows that our method is competitive against state of the art on the UCLIC database with similar recognition rate. In the Biological Motion Database, movements are mainly knocking at a door (\u2248 1200 animations out of 1356 anima- tions). The state of the art approach [4] uses this particularity to compute the average movement of knocking at a door and then subtracting this movement before running the recogni- tion in order to emphasis the expression. Their recognition rate for this biased method is 81%. Nevertheless, this trick is possible for very specific movements, when you assume that all movements are similar. They learn a movement\u2019s specific bias. Since purpose of our proposed framework is to be ro- bust against heterogeneous movements, we can not apply this assumption. We believe that to evaluate [4] and our approach, their unbiased recognition rate of 50% is to be compared with our approach that obtained correct classification accuracy of 57%. Finally, to the best of our knowledge, no method in liter- ature on body expression analysis has tested the SIGGRAPH database.\rTable 4: Comparison of our methods using all features men- tioned in this paper to the state of the art methods.\rFig. 3: Frames of the same action but with different body expressions. This figure show the variations of the triangle area formed by the both shoulders and the neck. Figure is color coded for the ease of visualization i.e. triangle formed by same joints in two figures are shown in similar color.\rculation. Presented results have been obtained using the sup- port vector machine with a radial basis function kernel (SVM - RBF kernel) method for the classification using a 10-fold cross validation technique. The parameters of the classifier were determined empirically. Proposed features (refer Sec- tion 3 for the detailed discussion on proposed features) can be categorized in the following sub-categories.\r1. Geometric features: distances, area and angles of trian- gles.\r2. Motion features: velocity and accelerations.\r3. Fourier features: magnitude of the spectra for different joints.\r4. All features: combines all features mentioned above.\rThe best result is obtained with the combination of the different proposed features (see Table 3). It is interesting to observe the fact that, correct classification accuracy does not degrade significantly when we used only geometric features.\rOn the two databases, SIGGRAPH and Biological Mo- tion, geometric features achieved only one percent less cor- rect classification accuracy than the best achieved results. On UCLIC database the recognition rate for the geometric fea- tures is 12% less than the combination of all features. This is probably due to the fact that this database is significantly smaller in sample size (183 actions) and presents worst case\r   DataBase\rBest results from state-of-the-art\r  Our results\r  UCLIC\r79% [18]\r  78%\r  Biological\r50% unbias [4]\r  57%\r  SIGGRAPH\r\u2013\r  93%\r    \r5. CONCLUSION\rWe have presented novel approach for automatic recognition of body expressions through 3D skeleton provided by mo- tion capture data. Taking inspiration from psychology do- main state of the art, we have proposed simple and represen- tative features to detect body expression from temporal 3D postures, even in complex cases: jump, run, kick etc. We have evaluated our approach on three databases that contain heterogeneous movements and expressions and obtained re- sults that exceeds state of the art. Secondly, our proposed approach runs in real time due to its computation simplicity. Thus, opening up possibilities for human-computer interac- tion applications. One such application example is new gen- eration of video games that can benefit from real time body expression analysis to adapt its content on run time.\rAs future work, we will aim at extending the proposed method to use more semantic meta-features reinforcing the recognition of body expressions. For instance, we will seek to fit the low-level features on analytic curves in order to use the curve parameters in the classification. And, we will com- pute incrementally the low-level features to achieve continu- ous recognition over time. An improvement of the validation will be to test our approach on multi-simultaneous actions and by cross-validating our method on several databases.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823448","bibtex":"@INPROCEEDINGS{7823448, \rauthor={A. Crenn and R. A. Khan and A. Meyer and S. Bouakaz}, \rbooktitle={2016 International Conference on 3D Imaging (IC3D)}, \rtitle={Body expression recognition from animated 3D skeleton}, \ryear={2016}, \rpages={1-7}, \rkeywords={computer animation;emotion recognition;feature extraction;pose estimation;psychology;stereo image processing;animated 3D skeleton;body expression recognition;body postures;heterogeneous movements;human postures;pose sequence;psychology domain;visual cues;Databases;Elbow;Feature extraction;Hip;Neck;Psychology;Shoulder;3D Skeleton;Animation;Body Expression Recognition;Features Extraction}, \rdoi={10.1109\/IC3D.2016.7823448}, \rmonth={Dec},}"},"paper9":{"title":"PERCEIVED QUALITY OF ANGULAR RESOLUTION FOR LIGHT FIELD DISPLAYS AND THE VALIDY OF SUBJECTIVE ASSESSMENT","authors":["Peter A. Kara","Maria G. Martini","Peter T. Kovacs","Sandor Imre","Attila Barsi","Kristof Lackner","Tibor Balogh"],"conference":"2016 International Conference on 3D Imaging (IC3D)","abstract":"Angular resolution plays a vital role in the perceived quality when displaying visual content on autostereoscopic 3D displays, since it affects the motion parallax effect. In this paper, we present the results of a subjective quality assessment carried out on a light field display, investigating the perceptual quality of visual content with different angular resolutions. We also address the question of subjective assessment validity, since the visual experience of suboptimal, reduced angular resolution is currently a completely new phenomenon for test participants.","keywords":["Quality of Experience","Perceived Quality","Image Quality","Light Field Display","Angular Resolution","Subjective Assessment of Visual Quality"],"content":"1. INTRODUCTION\r2D displays show the same image to different directions. Drilling down to the pixel level, this is because each pixel shows the same color and intensity to all directions (disregarding intensity falloff at steep angles). However, in case of autostereoscopic 3D displays, emitting different colors to different viewing directions is a required property by principle, as this is the only way to show different images to the two eyes of the human observer without using additional devices for view separation (e.g., glasses). The number of different directions that can be separately controlled and emitted from the display\u2019s surface on a unit angle is referred to as angular resolution. Angular resolution not only determines the quality of the motion parallax effect [1] [2], but it also has a consequence on the depth range of the 3D display.\rSince continuous motion parallax is an essential quality factor of autostereoscopic 3D displays, it is necessary to provide the given service with an acceptable extent of angular resolution [3]. The motivation of this paper is to extend the knowledge on the perceptual effects of angular resolution, in the aspect of subjective quality perceived from the user\u2019s point of view.\rAnother motivation of this paper is the fact that currently Light Field Displays (LFDs) are far from being common in everyday usage \u2013 many people have not even heard about this type of display technology \u2013 and the phenomenon of reduced angular resolution is unknown to many. This concern leads to question of validity (or invalidity) regarding subjective results obtained from test participants; the collected subjective data may be inconsistent and misleading.\rThe two main sets of research questions of this paper are the following: (1) How do human observers perceive the changes in angular resolution on LFDs? How much does angular resolution affect the perceptual quality of visual content? What is an acceptable number of views for a given field of view? (2) Can current subjective test participants be considered a reliable source of quality assessment regarding angular resolution? Are subjective scores collected during such evaluations expected to be consistent? Is a simple training phase before subjective assessment sufficient to introduce this new phenomenon to the test participants?\rThe paper is structured as follows: Section 2 provides an overview of the related work. The parameters of the experiment carried out are detailed in Section 3, results of which are presented in Section 4, including a discussion on the validity of the collected data. This is followed by the conclusions of findings in Section 5, also pointing out potential continuations of the work.\r978-1-5090-5743-6\/16\/$31.00 \u20ddc2016 IEEE\n2. RELATED WORK\rThe Society for Information Display\u2019s (SID) International Display Measurement Standard provides guidelines for measuring parameters such as angular resolution on 3D displays [4]. McIntire investigated the relationship between the amount of disparity and task performance [5]. The maximum amount of disparity that can be presented is directly related to the angular resolution of the display used. High angular resolution could also be achieved by using pico projectors [6], where users directly observe the array of projectors, which are practically functioning as angularly- varying pixels. In such case, angular resolution is affected by the resolution of the projectors. The synthesis of intermediate views can be used to increase angular resolution, as it boosts the number and thus the density of views. Such reconstruction of light field [7] supports smooth motion parallax, however, depending on the solution, the reduced visual quality of the reconstructed image may damage the user experience [8].\rBeyond angular resolution, several other attributes contribute to the Quality of Experience (QoE) as well. The ratio of the Field of View (FOV) and the number of views displayed in the given FOV determines angular resolution, however, FOV on its own affects QoE [9]. When it comes to resolution, traditional spatial resolution of the source visual content also plays a role in the overall experience of the human observer [10], however, degradations in resolution appear to the user differently as light rays hit the projection- based LFD screen at irregular positions.\rIn the work presented in this paper, angular resolution was the only variable between test conditions. More precisely, the number of views used to display the given visual content varied while the FOV was constant for all test cases.\r3. RESEARCH CONFIGURATION 3.1. Experimental setup\rTo investigate our set of research questions, we designed an experiment in which human observers had to assess the quality of still images with varying angular resolution, displayed on a projection-based LFD. We used a 3-meter- wide glasses-free HoloVizio 3D cinema system [11] to present the images to the test participants. The horizontal viewing angle of the selected display was 40 degrees and its brightness was 1500 cd\/m2. The subjective quality assessment measurement took place in a closed laboratory environment, with lighting conditions of 25 lx.\rDuring the measurement, the test participant viewing and evaluating the stimuli was 4.6 meters away from the display, which was explicitly given by setting up a cord on a straight line (see Figure 1). This was a 2.5H viewing distance [12], as the height of the selected display was 1.8\rmeters. There was also a direct instruction to move along the cord (1 meter in each direction) before evaluating the quality in order to view the displayed images from different angles, and thus observe the continuity of horizontal motion parallax in the process.\rFig. 1. Experimental setup of the measurement.\rA total of 20 test participants (11 men and 9 women) evaluated the quality of the displayed images. The average age of the participants was 26, and they belonged to an age interval running from 19 to 50.\r3.2. Measurement stimuli\rAs it was stated earlier in the paper, the only variable of the subjective measurement was angular resolution. This variation was achieved by directly rendering the test stimuli with different number of views. The number of test cases was 10 and each test case was identified by the stimulus\u2019 number of views. The lowest value was 15 and was incremented by 15 until reaching 150, so the test cases were 15, 30, 45, 60 etc. We chose the uniform difference in the number of views between adjacent test cases to be 15 based on the outcomes of measurements prior to the actual experiment, determining it to be above Just Noticeable Difference (JND) for angular resolutions below 3 view\/degree for the given display.\rThree different 3D contents were rendered for the measurement (see Figure 2), and naturally all 3 were rendered 10 times with the given number of views. The stimuli had plain background color and contained an object, complexity of which was different for each. Stimulus A was a collection of shapes with plain colors and clear, sharp edges, stimulus B had fine structural details and stimulus C contained detailed textures. The resolution of the displayed source images was 1024x576.\r \n Fig. 2. Stimulus A (top), B (middle) and C (bottom).\rThe reason why we chose to only use still images as source stimuli instead of videos was to exclude perceptual phenomena like frame rate, thus reduce cognitive load and allow a greater focus on the single attribute at hand. Also, rendering the images for the research enabled a precise control over angular resolution. Of course, in future research, we aim to include videos and natural stimuli.\rThe 30 test cases were displayed in a random order, but they were separated by content; first the 10 randomized test cases of source stimulus A were shown to the participants, followed by B and then C. At the beginning of each content sequence, a reference was shown, which was actually test case 150, so the measurement contained a hidden reference for evaluation, because the reference quality was included in the test conditions without the knowledge of the test participants.\r3.3. Subjective assessment task\rThe evaluation of test cases was carried out on an Absolute Category Rating (ACR) scale [13] from 1 to 10, where 1 was the lowest possible score and 10 represented the reference quality. The task was to rate the perceived quality of the displayed objects, focusing on angular resolution. We chose to have an ACR scale larger than the usual 5-point one in order to allow participants distinguish the small differences\rbetween adjacent test cases. Degradation Category Rating (DCR) [13] was considered as well during the design of the research, however, we rejected it because of the so-called \u201cwow effect\u201d; test participants are less likely to express annoyance towards new, visually appealing technology. Using a bidirectional comparison scale could also have been an option, but having a complete set of comparison with the gives test conditions would have resulted in a total of 165 comparisons.\rBefore the assessment itself, a learning phase took place, during which test participants were introduced to the measurement task itself, the display, the stimuli and the phenomenon of angular resolution through examples. We only had nai\u0308ve test subjects; none of the participant had seen light field displays before the measurement, and experiencing changing angular resolution was new to all of them.\rThe test cases were initially displayed for 10 seconds, and the test participants had to assess the quality right after each test case in a time window of 10 seconds, during which the image was still displayed. The duration of the measurement was approximately 11-12 minutes.\r4. RESULTS\r4.1. Mean scores\rWe expect mean subjective scores to increase along with the number of views, since a higher number of views on a given FOV means higher angular resolution, and thus smoother horizontal motion parallax.\rFig. 3. Mean Opinion Scores.\rThe results obtained for the Mean Opinion Score (MOS) of the measurement (see Figure 3) \u2013 containing the evaluation of all stimuli, provided by the 20 participants \u2013 show a slight inconsistence, due to the fact that test case 75 received a higher mean score than 90, even though this difference appears not to be significant with a 0.95 Confidence Interval\r \n(CI). Up to test case 75, the differences of adjacent test cases are indeed significant, but not beyond that.\rThe reason why test case 75 was so highly evaluated is a curious case, and more details shall be found if the mean scores of the 3 stimuli are displayed individually (see Figure 4). As it is apparent, the biggest contribution to the aforementioned shape of the overall MOS is due to stimulus C. For test case 75, it has a mean score, which is higher than any other (including the hidden reference), followed by test case 90 with much lower scores (a difference of 1.75), and the 4 test cases after that fluctuate. Stimulus A has a similar tendency regarding test case 75 and 90, but mean scores of test cases with higher numbers of views increase. This increase also applies to stimulus B, however, the phenomenon of inconsistent mean scores is shifted up by 1 test case (90 and 105) and it is also much smaller.\rFig. 4. Mean Opinion Scores, separately for each stimulus.\rThe preference of source stimuli can be clearly observed from their mean scores, but this is also reflected in the distribution of scores (see Figure 5). While the most frequent score used to evaluate test stimulus B is 7, it is 8 for A and 9 for C. The distribution is close to uniform on the lower half of the scale, and shows that no unit of the evaluation scale was neglected.\rFig. 5. Distribution of scores for each stimulus.\r4.2. Test case 75, 90 and 105\rThis part of the analysis presents the actual assessment scores obtained by test case 75, 90 and 105 for each source stimulus. In the figures, each cut represents the evaluation of one individual, and the cuts are arranged clockwise by the deviation of these scores.\r  Fig. 6. Assessment of test case 75, 90 and 105 for stimulus A. Each cut represents the scores of a test participant.\rIn case of stimulus A (see Figure 6), scores appear in many variations of ranking order. 3 participants did not distinguish the perceived quality of these test cases and 6 evaluated two of them equally. Test case 75 was rated the highest, closely followed by test case 105. 8 test participants provided test case 90 lower scores than to the other two. The participant having the highest deviation in these scores gave test case 75 a score of 8, but only 2 to 90 and 10 (score equivalent of reference quality) to 105. We can even observe the opposite of what we would theoretically expect for the quality of the test cases, in a clear, uniform manner (10, 8 and 6 for test case 75, 90 and 105, respectively).\r  Fig. 7. Assessment of test case 75, 90 and 105 for stimulus B. Each cut represents the scores of a test participant.\r\nThe mean score provided for stimulus B is the lowest among the subjective assessment results, however, it can also be said that the scores do not deviate much (see Figure 7). Even though only 2 test participants avoided differentiation in scores completely, the quality of two test cases was rated identical 12 times. Unlike stimulus A, here test case 90 was much appreciated and test case 75 was the least favored.\rFig. 8. Assessment of test case 75, 90 and 105 for stimulus C. Each cut represents the scores of a test participant.\rCompared to the previous two stimuli, stimulus C received clearly higher scores in general (see Figure 8), as can also be seen in the mean scores of the stimuli (see Figure 4). Only a single participant gave the same scores to all 3 test cases, however 10 test participants rated 2 test cases equally. These 10 cases can be clustered into 3 groups: 90 and 105 are the same but 75 is better by 1 unit (2 occurrences), 75 and 105 are the same but 90 is worse by 1 (4 occurrences) and by 2 units (4 occurrences). Test case 75 being the best from these 3 test cases, followed by test case 105 and then 90 is also a popular scoring pattern; 7 test participants assessed accordingly. There are also examples for test case 90 and 105 gaining the highest score, 1 each.\r4.3. Inconsistent subjective assessments \u2013 discussion\rIn this final part of the results, we intend to address the research question of validity, pointing out a topic \u2013 or rather an issue \u2013 to the scientific community. Today, at the time of this paper, are test participants truly capable of evaluating a phenomenon like angular resolution? While image resolution is a phenomenon that consumers encounter on a daily basis while for instance watching streaming videos on the Internet, the majority of the people have not even seen autostereoscopic displays in person. In case of our study, 75% of the selected test participants had not even heard of such displays prior to the measurement. The lack of prior knowledge and experience is of course beneficial, since it\rmay introduce specific types of cognitive bias, e.g., the labelling effect [14]. The role of the training phase is to familiarize the observers with the task and what they will see, but can it be considered to be sufficient to provide consistent, valid results in the end?\rThe 60 rating sequences (20 participants assessed 3 stimuli, and 1 sequence is the series of 10 scores for a given stimulus) are 90% inconsistent, if we in this case define a sequence to be consistent in the following way: for any Vi, Si+n must be greater than or equal to Si, where S is the subjective score allocated to V, which denotes the given number of views, i is the index of a test case and n is an arbitrary positive integer. We hereby present 3 series of inconsistent subjective assessment sequences for each stimulus, provided by different test participants.\rFor stimulus A (see Figure 9), the first sequence might be familiar, since it has already been pointed out earlier (see Figure 6), for having the highest deviation for test case 75, 90 and 105. The other two sequences are similar in this sense, however, in case of the second sequence, it is test case 105 that received a sudden low score compared to the prior test case. These two sequences put together show a radical contrast for test case 90, since \u2013 while it obtained the lowest possible score in the third sequence \u2013 it was assessed to be an equal of the reference in the second one.\rFig. 9. Examples of inconsistent scores for stimulus A.\rIn stimulus B (see Figure 10) the first evaluation sequence determines test case 15 to have the best perceptual quality, even though it displays the content in such a way that not only discrete image borders are visible, but that the adjacent views are also continuously present semi- transparently, regardless of viewing position. The second one has a uniform increase for the first half, however, has a sudden rise and a falling tendency for the second half, except from the hidden reference. The third sequence has a surprisingly high score for test case 45, compared to the\r  \nadjacent scores, followed by score fluctuation from test case 75, and ending in decreasing scores.\rexperiment. The knowledge on this cognitive distortion phenomenon could benefit from further, more exhaustive research.\rThe training phase prior to the subjective assessment of visualized 3D content on such displays evidently affects the performance of test subjects during quality evaluation. Future recommendations, benchmarks and standards particularly considering subjective assessment methods on autostereoscopic 3D displays could additionally benefit the research community by declaring standardized training phases with special concern for such new visual phenomena.\rThe research presented in this paper did not exclude the results of any test participant. Removing outliers may reduce the deviation of the score set and thus increase confidence, however, having numerous distorted score series from the same inconsistent pattern may result in discarding undistorted \u2013 or at least less distorted \u2013 evaluations, due to their statistical distances from other observations. Also, we kept all the acquired data in the analysis in order to demonstrate the effect of such score sets on the mean values of user experience.\rOne may argue that motion parallax is something that people are used to, as we encounter it in real life. That is indeed unquestionable, however, reductions in angular resolution and thus in the smoothness of motion parallax does not occur in everyday life, making this phenomenon new for those without any prior experience with such display technologies.\r5. CONCLUSIONS\rThe paper has presented the results of a subjective quality assessment where test participants rated visual contents displayed on a light field display with different angular resolutions. We found that user experience appears to correlate well with angular resolution, however, our results indicate severe inconsistencies in the ratings provided by the test participants, supporting our questioning of the validity of the results obtained by subjectively evaluating a phenomenon the test participant had not experienced prior to the experiment.\rThe continuation of this work shall focus particularly on video quality, as factors like frame rate may affect the perception of angular resolution. In the future, we also plan to address more subjective quality assessment validity issues that are exclusively present in autostereoscopic 3D displays.\rACKNOWLEDGMENTS\rThe work in this paper was funded from the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 643072, Network QoE-Net. The research leading to these results has received funding from the PROLIGHT-IAPP Marie Curie Action of the People programme of the\r Fig. 10. Examples of inconsistent scores for stimulus B.\rThe selected sequences of stimulus C (see Figure 11) show assessment inconsistency as well. The first one already reaches the maximal score at test case 45, and it also has the opposite scoring relation for test case 90 and 105, compared to the majority of ratings for this stimulus (see Figure 8). The second and the third evaluation sequence both have score fluctuation beyond test case 105 and the highest score was obtained by test case 75, but the second one has a relatively high score for test case 30, even higher than what was given for test case 90.\r Fig. 11. Examples of inconsistent scores for stimulus C.\rFrom these results, we can conclude that there are indeed returning patterns in inconsistencies \u2013 as also seen earlier in the paper, in the comparison of test case 75, 90 and 105 \u2013 yet we also need to add that the actual reasons for some specific distortions remain unclear to us after this\r\nEuropean Union\u2019s Seventh Framework Programme, REA grant agreement 32449. The work was also supported by the Department of Networked Systems and Services of the Budapest University of Technology and Economics.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823450","bibtex":"@INPROCEEDINGS{7823450, \nauthor={P. A. Kara and M. G. Martini and P. T. Kovacs and S. Imre and A. Barsi and K. Lackner and T. Balogh}, \nbooktitle={2016 International Conference on 3D Imaging (IC3D)}, \ntitle={Perceived quality of angular resolution for light field displays and the validy of subjective assessment}, \nyear={2016}, \npages={1-7}, \nkeywords={image resolution;stereo image processing;three-dimensional displays;angular resolution;autostereoscopic 3D displays;light field displays;motion parallax effect;subjective quality assessment;visual content perceptual quality;visual experience;Atmospheric measurements;Image color analysis;Particle measurements;Spatial resolution;Three-dimensional displays;Visualization;Angular Resolution;Image Quality;Light Field Display;Perceived Quality;Quality of Experience;Subjective Assessment of Visual Quality}, \ndoi={10.1109\/IC3D.2016.7823450}, \nmonth={Dec},}"},"paper10":{"title":"VARIATIONAL IMAGE-BASED RENDERING WITH GRADIENT CONSTRAINTS","authors":["Gr\u00e9goire Nieto","Fr\u00e9d\u00e9ric Devernay","James Crowley"],"conference":"2016 International Conference on 3D Imaging (IC3D)","abstract":"Multi-view image-based rendering consists in generating a novel view of a scene from a set of source views. In gen- eral, this works by first doing a coarse 3D reconstruction of the scene, and then using this reconstruction to establish cor- respondences between source and target views, followed by blending the warped views to get the final image. Unfortu- nately, discontinuities in the blending weights, due to scene geometry or camera placement, result in artifacts in the tar- get view. In this paper, we show how to avoid these arti- facts by imposing additional constraints on the image gradi- ents of the novel view. We propose a variational framework in which an energy functional is derived and optimized by itera- tively solving a linear system. We demonstrate this method on several structured and unstructured multi-view datasets, and show that it numerically outperforms state-of-the-art meth- ods, and eliminates artifacts that result from visibility discon- tinuities.","keywords":["Image-Based Rendering","Computational Photography","Computer Graphics"],"content":"1. INTRODUCTION\rMulti-view image-based rendering consists in generating a novel view of a scene from a set of source views. In general, this works by first doing a coarse 3D reconstruction of the scene, called a geometric proxy, then using this reconstruc- tion to establish correspondences between source and target views(Figure 1 (a)), followed by blending the warped views to obtain the final image. Recent work by Pujades et al [24] proposed a Bayesian formulation of the image-based render- ing problem, building on previous work by Wanner and Gold- luecke [11, 29]. They showed that the weight of each source image in the novel view could be formally deduced from the camera properties, image content, and accuracy of the geo- metric proxy, leading to a formalization of the heuristic blend- ing weights proposed initially by Buehler et al [3]. Most of the \u201cdesirable properties that [...] an ideal image-based ren- dering algorithm should have\u201d [3] were thus given a formal explanation, except for the continuity property. Therefore, discontinuities in the source image weights, which are mainly\rThanks to DGA for funding\rdue to scene geometry or camera placement, result in artifacts in the target view.\rIn this work, we show that a way to avoid these artifacts is to impose additional constraints on the image gradients of the novel view. These constraints come from a simple obser- vation: an image contour in the target image should also be present in source images where this part of the scene is vis- ible. An energy functional similar to the one in Pujades et alis derived, which is composed of the usual data term and smoothness term, but the data term has an additional term which takes into account these gradient constraints. The re- sulting energy is optimized in a variational framework, by it- eratively solving a linear system.\rOur method is composed of two stages: a 3D reconstruc- tion pipeline (section 4.1) followed by the target view render- ing (section 3). The depth maps we obtain from multi-view stereo are used to compute a warp function from each input image to the novel view. The target view is computed via a novel variational formulation of the image-based rendering problem. We demonstrate the method on several unstructured multi-view datasets (section 4) and show that not only it nu- merically outperforms state-of-the-art methods (section 4.2), but also it eliminates artifacts that originated from visibility discontinuities. We conclude that taking into account both intensities and gradients in image-based rendering methods offers an elegant solution to enforcing the continuity property initially devised by Buehler et al.\r2. PREVIOUSWORK\rImage-Based Rendering (IBR) has been extensively reviewed by Shum et al [25]. Most state-of-the-art [15, 26, 17, 18, 5] approaches use a coarse 3D reconstruction of the scene, called a geometric proxy, which may have various degrees of ac- curacy. Ortiz-Cayon et al [22] choose to over-segment the image and compute the quality of several IBR algorithms on each super-pixel. Then the output of the best IBR algorithm for each super-pixel is picked. These algorithms are largely inspired by the Unstructured Lumigraph [3], that performs a blending of the k-nearest views, weighted by angles and dis- tances to the target view, thus guarantying a smooth camera blending field. The continuity of the resulting blend in im- age domain is ensured by enforcing spatial smoothness on\r 978-1-5090-5743-6\/16\/$31.00 \u20ddc2016 IEEE\r Fig. 1: (a) We coarsely estimate the geometry of the scene to register input images. The warp functions \u03c4k link the input images vk to the target view u. (b) The target view u is reconstructed by blending the source images in intensities. (c) Our method consists in appending constraints on the gradient of the solution; we show that it is equivalent to a Laplacian blending and that it removes high frequency blending artifacts.\rthese weights, but temporal artifacts may still occur if the contributing cameras are too sparse. Davis et al [6] propose a viewpoint-subdivision rendering technique to create a better blending field. Nevertheless, blending weights are still heuris- tics and the choice of cameras for rendering is arbitrary.\rDisposing of heuristics and tunable parameters is a key objective in [29], who propose a physics-based Bayesian for- mulation of image-based rendering. The weight of each in- put view in the final blending is automatically deduced from the mathematical equations by deriving an energy functional. Pujades et al [24] went further by integrating geometric un- certainty in the Bayesian formalism. They then obtain new weights that favor cameras satisfying both epipole consistency and minimal angular deviation, two principles stated by Buehler et al [3] to describe the ideal IBR algorithm. They were unable, however, to provide a formal derivation that leads to the continuity principle, especially near the borders of each camera view field. We show that introducing an ad- ditional term in the energy functional, not only constrains the image intensities but also constrains the image gradients, pro- viding an elegant solution to the continuity principle.\rThe key idea of high-quality rendering is that the qual- ity of the image solution often relies on the constraints put on the search space. As a consequence, finding the right reg- ularization or image prior has been widely researched in or- der to obtain high-quality images. The main contribution of Fitzgibbon et al [7] is the use of texture priors computed from a large database of patches to constrain the solution, inde- pendently from the input scene data. This idea was recently extended by Flynn et al [8] who perform new view synthesis via a deep network architecture, trained by a huge database of real-world image sets. In contrast our method does not rest upon any strong prior knowledge about the new view to syn- thesize, but rather makes better use of the data provided by the input views to add more constraints on the solution. There-\rfore our algorithm does not require large numbers of picture sets from the world\u2019s imagery to create a high-quality images. Image fusion in the gradient domain has received much interest in recent years beginning with the seminal paper of Perez et al [23], for applications such as image editing [19], inpainting [16] or image stitching [30]. The closest work to ours for image-based rendering is probably [15], who gener- ate a new point of view by reconstructing the gradients in the target view, followed by an integration of these gradients on the GPU to recover the final image. However, their method is limited to interpolating between two views, and they nei- ther address the generic case where the input views may have very different viewpoints, fields of view, and resolution, nor produce super-resolved images. Our method addresses these issues and proposes a more generic framework for multi-view\rimage-based rendering.\r3. VARIATIONALIMAGE-BASEDRENDERING\rOur goal is to synthesize an optimal novel image u : \u0393 \u2282 R2 \u2192 R at the target viewpoint from the input images vk : \u03a9k \u2282 R2 \u2192 R. For the sake of simplicity, image values are taken as scalars, but this easily generalizes to color images vk : \u03a9k \u2192 R3. In all our experiments we process the images in the RGB color space. Source images are registered via the warps \u03c4k that transform any point xm = (xm , ym )\u22ba of the input view k into the corresponding point xp = (xp , yp )\u22ba in the target view:\r\u03c4k: \u03a9k \u2192 \u0393 xm  \u2192 xp\r3.1. ImageFormationModel\r(1)\rAs commonly assumed in the super-resolution literature [1, 14], we consider the intensity value vk (xm ) at a point xm\r  in the low-resolution observed image k to be the convolution of values in the super-resolved image with the point-spread function (PSF) b. Given an ideal super-resolved image u at the position of the target view defined over \u0393, and a warp \u03c4k that maps points from \u03a9k (low resolution domain) to \u0393 (high resolution), if we discard for now the visibility effects, the intensity in the observed image k can be written as:\rvk(xm)=  u\u25e6\u03c4k(x)b(x\u2212xm)dx, (2) \u03a9k\rorsimplyvk =b\u2217(u\u25e6\u03c4k).\rThe PSF b : \u03a9k \u2192 [0, 1] is a probability density function\rthat can be turned into bk : \u0393 \u2192 [0,1] by the change of variable x\u2032 = \u03c4k(x) so that\rFig. 2: Warp and blending weight discontinuities cause arti- facts (in red). Left: a close-up to a view rendered by min- imizing the energy 7. Right: a warp that presents visibility discontinuities that caused the artifacts.\r3.2. MaximumAPosteriori(MAP)Estimation\rThe goal of the variational approach is to estimate the high resolution image u from the data (vk\u2217)k\u2208[1..K], where K is the number of inputs views. The estimator of u maximizes the posterior which is the probability of finding u given the input data. One can show that this is equivalent to minimizing the energy\rE(u) = Eintensity(u) + \u03bbEprior(u), (7)\rEprior, often referred to as the smoothness term, comes from the image prior and prevents the emergence of high frequen- cies. \u03bb is a parameter that controls the smoothness of the final solution. In this work, we use a total variation prior [12], Eprior(u) =  \u0393 |\u2207u|, which has several advantages over other more complicated image priors: this approach preserves strong edges and image contours, and is convex. A proof of conver- gence is given by Chambolle [4].\rEintensity, often referred as the data term, is derived from the likelihood given the input image intensity [24]. This ac- counts for how well the current solution fits the data in the intensity domain:\rvk(xm) =  \ru(x\u2032)bk(x\u2032 \u2212 \u03c4k(xm)) dx\u2032 (3)\r\u0393\rThere are several ways of computing the warped PSF bk, depending on how we model the initial PSF b. The more com- mon assumption is to consider that b is a 2D Gaussian cen- tered at position xm. Since Gaussian filters have infinite sup- port, it is quite difficult to implement in practice. A simpler model of the PSF is to assume pixels are square and uniformly sensitive to light, so that the PSF is a uniform square density function. Noting A the area of a pixel centered on (0, 0) in a source view k, we get\r 1 b(x,y)= A2\r0\rif\u22121 \u2264x,y\u22641 A A\relsewhere.\r(4)\r   Under the assumption that the warp \u03c4k is locally linear, the warped PSF is a uniformly distributed parallelogram. In this case, we can make an even stronger assumption by suppos- ing that the warp preserve the pixels (their area and squared shape), which is actually untrue but largely simplify the im- plementation. From now on, we take a unit pixel area. Since the intensity is constant and equals u(p) over all the pixel area in the target view, the previous convolution can be written as in [14]:\r K 1   Eintensity(u) = 2\rk=1\r\u03c9k(u)((b \u2217 (u \u25e6 \u03c4k) \u2212 vk\u2217))2 dx. \u03a9k\r vk(xm) =   u(p)   bk(x\u2032 \u2212 \u03c4k(xm)) dx\u2032, p\u2208\u0393 p\rso the pixel intensity m in the source image is vk(m) =   Bk,m,pu(p),\rp\u2208\u0393\r(5)\r(6)\r(8) The terms \u03c9k(u) are the per-pixel contribution of each in- put view k. They depend on the gradient of the current solu- tion u and the geometric uncertainty of the 3D reconstruction.\r3.3. AppendingtheGradientTerm\rSince the geometry and the visibility may be discontinuous, the term \u03c9k(u) in 8 may be discontinuous too, resulting in artifacts in the synthesized image that may appear as spuri- ous edges or textures (Fig. 2). The IBR method should pre- vent these contours from appearing: actually, an image con- tour synthesized in the target image should also be present in source images where this part of the scene is visible.\rwhere Bk,m,p =  p bk(x\u2032 \u2212 \u03c4k(xm)) dx\u2032 is the area of inter- section between the projection of the pixel in the target view and a pixel p of this view. It is equivalent to bilinearly inter- polate the intensities of u.\rTo enforce this property, we add an extra term Egradient(u) to the previous energy (8) that forces the current solution to also fit the data in the gradient domain:\rEgradient (u)\rbe the vector of all the pixels of every input view put in one column (v0(0) v0(1) ... vK\u22121(M \u22121))\u22ba, U the column vector storing the current solution (u(0) ... u(N \u2212 1))\u22ba, and B the KM \u00d7N matrix that stores the Bk,m,p coefficients, we may naturally write V = BU. Consequently, we can express the energy (8) as a linear system:\rEintensity(U) = (BU \u2212 V\u2217)\u22baW(BU \u2212 V\u2217), (18) where W is a KM \u00d7 KM diagonal matrix that stores the\r(\u03b2 )|\u03c9 . To minimize this energy we derive the\r= \u2212log p(\u2207v0 . . . \u2207vK\u22121|\u2207u) K\u22121\r(9) = \u2212 logp(\u2207vk|\u2207u) (10)\r  \rk=0\r= (\u2207vk\u2212\u2207vk\u2217)2dx (11)\r \r\u2032\r\u03a9k \u03a9k\rweights |Jx k k\rlinear system, and obtain the normal equations that provide\r(\u2207(b \u2217 (u \u25e6 \u03c4k)) \u2212 \u2207vk\u2217)2 dx.(12) Finding u that minimizes this energy is equivalent to solv-\ring the Laplace equation:\r\u2206((b\u2217(u\u25e6\u03c4k)\u2212vk\u2217) = 0, (13)\rwhere \u2206 = \u2207.\u2207 denotes the Laplacian. We instantly deduce the derivative of the functional:\rdEgradient(u) = (|\u2202\u03c4k |\u22121  \u0304b \u2217 (\u2206(b \u2217 (u \u25e6 \u03c4k)) \u2212 \u2206vk\u2217)) \u25e6 \u03b2k.\r\u2202z\r(14) The \u03b2k are the backward warp that appear thank to the change of variable in the integral.  \u0304b is the adjoint of the blur kernel b. The warps \u03c4k are those which were estimated before- hand, and thus lack precision. This uncertainty has a drastic effect on the computation of \u2206(b\u2217(u\u25e6\u03c4k )). As a consequence, we chose to compute the Laplacian of u first, then warp it in the \u03a9k domain. Under the assumption that the warps \u03c4k can be locally linear, we neglect their second order derivatives and\r=\r\u02c6 an estimator for the solution U:\rB\u22baWBU\u02c6 = B\u22baWV\u2217. (19)\rThe matrix B\u22baWB is generally not invertible. This linear system can be solved by any linear least square solver.\rAkin to the data term on color, the data term on image gradient is\rEgrad(U) = (B\u2207U \u2212 \u2207V\u2217)\u22ba(B\u2207U \u2212 \u2207V\u2217) (20) and can be derived likewise.\r4. EXPERIMENTSANDVALIDATION 4.1. 3DReconstruction\rOur method takes as input an unstructured set of source views with no particular structure [3], as opposed to view interpola- tion methods which usually take stereo pairs, or methods that are based on a structured light field [29]. To be as generic as possible, it only requires the warp functions \u03c4k that we obtain via a classic 3D reconstruction pipeline [9]. It con- sists in camera calibration via bundle adjustment [20], fol- lowed by an MVS reconstruction [10] to get a depth map for each input view (Fig. 3). Knowing the camera parameters and the depth of each input pixel, we can deduce per-pixels correspondences between the source images and the target\rFig. 3: The depth map of an input view.\r obtain:\r\u2206(b\u2217(u\u25e6\u03c4 )) = b\u2217 k H k + k H k . (15)\r \u2202\u03c4 \u22ba \u2202\u03c4 \u2202\u03c4 \u22ba \u2202\u03c4   k \u2202x u \u2202x \u2202y u \u2202y\r    H = \u2202\u2207u is the Hessian matrix of u. Due to uncer- u \u2202x\rtain depth maps that cause strong discontinuities in warps, the Hessian matrix may be very unstable. For the computa- tion, and in this case only, we assume \u03c4k(x) \u2248 x + d so that\r\u2206(b\u2217(u\u25e6\u03c4k)) = b\u2217(trace(Hu)\u25e6\u03c4k) = b\u2217(\u2206u\u25e6\u03c4k). (16) The final form of the energy to minimize is thus:\rE(u) = \u03b1Eintensity(u) + \u03b3Egradient(u) + \u03bbEprior(u). (17)\rWe minimize the functional (17) via Fast Iterative Shrink- age Thresholding Algorithm (FISTA) [2].\r3.4. Discretization\rOne can wonder how we go from a continuous model of the problem to a numerical solution. For each pixel m of each input view k we obtain an equation similar to (6). Let V\u2217\r  \r  Fig. 4: Results on datasets fountain and herzjesu. The bottom row shows some of the input views. Note that the parts of the target view that are not visible by any of the inputs are filled by a push\/pull inpainting algorithm.\rview. We use the depth to derive the blending weights as it is demonstrated in [24]. For further information about the reconstruction pipeline, more specifically occlusion handling and the derivation of the blending weights, see [21].\r4.2. Results\rA set of experiments (Figure 5) is performed on real views taken from Strecha\u2019s dataset [27], fountain and herzjesu. Ge- ometric proxies are estimated according to the pipeline de- scribed in section 4.1. For both datasets we remove the cen- tral view, render it in the same dimensions as the input images and compare the result with the original for visual evaluation. All experiments were performed on GPU with an nVidia GTX Titan. Convergence is reached within 3 minutes for an input set of 11 input images of size 3072 \u00d7 2048. Since the conver- gence time strongly depends on the initialisation of the solu- tion, the performance could be significantly increased. How- ever, one drawback of our variational method is that it does not perform real-time rendering. To show that the quality of our results does not depend on the initialization, we start the optimization process from a null image.\rSince some parts of the target view are not visible from the input views due to self-occlusion, inpainting is needed to fill potential holes. To that end, we implemented the push\/pull algorithm as it is described in [13]. Firstly, the push stage decomposes the final image u into its Gaussian pyramid. At each level of the pyramid the image is filtered by a Gaussian- like 5 \u00d7 5 kernel and down-sampled by a factor 2. Only non- null value pixels contribute to create the upper-level image, so that at the coarsest level of the pyramid the image has no hole.\rNote that we do not need to down-sample to the 1\u00d71 pixel im- age; in our experiments the 6 \u00d7 4 pixel image does not contain any holes. Secondly the pull stage propagates missing infor- mation from the top of the pyramid downwards to the finest level. At each level, holes are filled with the corresponding pixels from the upper coarser image.\rAt first we tested for a null gradient term (\u03b3 = 0.0). Since the linear system to be solved is ill-constrained, high frequen- cies appear in areas that few cameras see. These artifacts are emphasized by a very noisy depth estimation near occlusion regions (around the fish on the fountain or the Jesus on the wall). To remove these artifacts and eliminate high frequen- cies, a stronger smoothness term is commonly used. Conse- quently we increased the \u03bb parameter that controls our To- tal Variation regularizer to 0.003. However the results are not that convincing: although most high frequencies are re- moved, some image features are lost compared to originals. To fix this problem, we achieved a third rendering where \u03bb is set to its initial value (0.002) and the gradient data term is appended to the equation (\u03b3 = 1.0). For the final solution to keep the original colors of the input image, we keep the intensity data term as a bias but choose a small controlling parameter (\u03b1 = 0.1). One can notice that artifacts are com- pletely removed, while preserving all of the image features. We conclude that appending the gradient data term prevents the emergence of spurious contours near visibility borders, hence guaranteeing the continuity property.\rSome numerical results are presented in table 1. To show that our rendering algorithm outperforms state-of-the-art meth- ods when synthesizing a specific view only, we generate sev- eral input views that we preliminarily removed from the input\r\u03b1 = 1.0,\u03b3 = 0.0,\u03bb = 0.002 ( [24, 29])\r\u03b1 = 1.0,\u03b3 = 0.0,\u03bb = 0.003 ( [24, 29])\r\u03b1 = 0.1,\u03b3 = 1.0,\u03bb = 0.002 (ours)\rfountain \u2013 view 2\r21.03 132\r21.09 120\r21.16 107\rfountain \u2013 view 5\r26.00 74\r26.14 64\r26.36 51\rfountain \u2013 view 8\r22.00 140\r22.08 125\r22.16 111\rherzjesu \u2013 view 2\r21.73 186\r21.96 153\r21.93 143\rherzjesu \u2013 view 4\r23.13 194\r23.81 130\r23.90 115\rherzjesu \u2013 view 6\r18.08 349\r18.26 287\r18.31 273\rTable 1: Numerical results on real-world datasets [27]. Our method is compared against state-of-the-art methods [24, 29], for which there is no gradient constraints (\u03b3 = 0.0). For each result, the first value is the PSNR (bigger is better), the second value is DSSIM in units of 10\u22124. DSSIM = 104(1 \u2212 SSIM) [28] (smaller is better). The best value is highlighted in bold. See text for a detailed description of the experiments.\rset and kept as reference views for quantitative comparison. Two numerical measures are computed with respect to the ref- erence view to evaluate our results: PSNR (the higher the bet- ter) and DSSIM = 104 (1 \u2212 SSIM) (the lower the better). Al- though the increase of the PSNR score is noticeable but not conclusive, the strong improvement of the DSSIM demon- strates a higher structural similarity with the reference view, which accounts for the elimination of most visual artifacts.\r5. CONCLUSION\rWe presented an image-based rendering method that renders a novel view from a generic and unstructured set of input views. This method is inspired by previous work by Pu- jades et al [24], which proposed a formulation for most of the \u201cdesirable properties\u201d that were listed in the seminal work by Buehler et al [3], using a Bayesian formulation, and opti- mizing the target image in a variational framework. The only property that could not be formally derived was the continu- ity property, which states that the contribution of each input view to the pixels of the target image should be a continuous function of the pixel coordinates.\rWe showed that an alternative approach for enforcing the continuity property is to state that edges, contours or textures should not be created in the target image if they are not present in the source images. This results in an additional data term, based on image gradients, which can be added to the energy functional. The energy can then be solve by iteratively solv- ing a linear system devised from the energy functional. The results show an improvement over previous intensity-based unstructured IBR methods, both in terms of objective image quality measurements, and in terms of subjective quality.\rLimitations: Despite the noticeable improvement of the image quality, our method does not fully eliminate all the vis- ible artifacts. It could be reworked to optimize directly the target image gradients, rather than intensities, and the target intensity could then be reconstructed by solving the Poisson equation, as is done in Kopf et al [15]. This should totally re- move any variations in the synthesized image that come from the discontinuity of the visibility functions, and are still visi-\rble, although attenuated, in our results.\rFuture work: As shown in section 3, appending gradient\rconstraints as a new energy data term is comparable to solv- ing the Poisson equation. In the stitching literature [30], it is known as Laplacian blending. Considering that the Lapla- cian of an image behaves like a band-pass filter, blending the Laplacian of the input images is analogous to blending the im- ages in the frequency domain for a specific band of frequen- cies that depends on the scale of the Laplacian. In our case the Laplacian is computed at the original scale of the image, level 0 of a Laplacian pyramid. Therefore we blend the im- ages for the band of highest frequencies, and high-frequency artifacts due to naive intensity blending are prevented. Our fu- ture work will focus on an extension to all scales via the use of Laplacian pyramids, enabling a complete Laplacian blend- ing to prevent not only high-frequency but also low-frequency artifacts. Fig. 5: Rendering the central view with different energy parameters. Each column shows the results on datasets fountain and herzjesu [27] by applying a special set of parameters (\u03b1, \u03b3, \u03bb) that control the amount of terms in the energy formula 17. Proposed approach is for \u03b3 \u0338= 0. State-of-the-art approach [24, 29] creates high-frequency artifacts due to blending intensities only. A higher smoothness parameter \u03bb = 0.003 partially removes these artifacts at the cost of detail loss. Our approach preserves detail and show finer results by appending constraints of the gradients of the solution, forcing a Laplacian blending. ","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823449","bibtex":"@INPROCEEDINGS{7823449, \rauthor={G. Nieto and F. Devernay and J. Crowley}, \rbooktitle={2016 International Conference on 3D Imaging (IC3D)}, \rtitle={Variational image-based rendering with gradient constraints}, \ryear={2016}, \rpages={1-8}, \rkeywords={geometrical optics;image reconstruction;object detection;optical images;variational techniques;camera placement;coarse 3D reconstruction;image gradient constraints;linear system;multiview image-based rendering consists;scene geometry reconstruction;target detection;variational image-based rendering;Cameras;Geometry;Image reconstruction;Image resolution;Laplace equations;Rendering (computer graphics);Three-dimensional displays;Computational Photography;Computer Graphics;Image-Based Rendering}, \rdoi={10.1109\/IC3D.2016.7823449}, \rmonth={Dec},}"},"paper11":{"title":"Quasi-Lumped Crystal-Like Reactive Elements","authors":["Pavlo Bidenko","Evgeniy Nelin","Anatolii Nazarko","Volodymyr Popsui"],"conference":"2015 13th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM)","abstract":"Quasi-lumped reactive elements on the base of electromagnetic-crystal inhomogeneities with a substantial increase of parameters compared to conventional microstrip structures are proposed. The parameter\u2019s error of quasi-lumped element is analyzed and the procedure of its minimization is offered. Theoretical and experimental results are presented.","keywords":["electromagnetic crystal","electromagnetic inhomogeneity","quasi-lumped reactive element"],"content":"I. INTRODUCTION\nIn the microstrip technique there were new directions for devices parameters improvement [1]. One of these directions is metamaterials which include crystal-like structures with zone properties similar to crystals. Microstrip crystal-like structures named electromagnetic crystals, as well as separate electromagnetic-crystal inhomogeneities (ECIs), are used in microstrip devices.\nConventional ECIs are two-dimensional structures formed as holes or slits of different shapes in the metallized surface or in the signal conductor [1-3].\nIn [4-5] high- and low-impedance three-dimensional (3D) ECIs were proposed. ECI includes inhomogeneities in metallized surface, in dielectric and in signal surface with much smaller and much larger values of equivalent impedance (Z l and Z h , respectively; indexes \u00abl\u00bb and \u00abh\u00bb denoting low and high impedance).\nSubstantial expansion of impedance range is a key advantage of 3D ECIs. Wave impedance characterizes reaction of a structure on wave perturbation and, in the same, efficiency of structure influence on a wave. With increasing Zh\/Zl ratio structure selectivity increases.\nIn the paper the application of 3D ECIs as quasi-lumped reactive elements with a substantially increase of reactive parameters compared to conventional microstrip structures is proposed and procedure of quasi-lumped parameter\u2019s error minimization is offered.\nII. CONVENTIONAL QUASI-LUMPED ELEMENTS\nConventional microstrip quasi-lumped reactive elements are formed by wide and narrow signal conductor sections with length l < \u03bb \/ 8 ( \u03bb \u2015 wavelength) and impedances Zl and Z h , respectively [1]. These sections are modeled by T- and \u03c0-equivalent circuits with elements defined by equations, respectively [1]:\nX = iZtg \u03c6 , B = iZ\u22121sin ; X = \u03c6iZ sin , B =\u03c6iZ\u22121tg \u03c6 , (1) 22\nwhere X and B are series and shunt elements of equivalent circuits, \u03c6 = \u03b2l , \u03b2 is the wavenumber.\nFor l<\u03bb\/8 weobtain \u03c6<\u03c0\/4,andEqs.(1)become: \u03c6\u22121 \u22121\u03c6\nX \u2248 iZ 2 , B \u2248 iZ \u03c6 ; X \u2248 iZ\u03c6 , B \u2248 iZ 2 . (2)\n  Under conditions\nZl << Z0 and Zh >> Z0 ,\n(3)\nwhere Z0 is a source impedance, series inductances and shunt capacitances in the T- and \u03c0-equivalent circuits, respectively, is not taken into account. As a result, it remains only shunt capacitance and series inductance with values defined as:\nC\u2248 l ,L\u2248Zhl, (4) Zlv v\nwhere v is a wave\u2019s speed.\nEqs. (4) follow also from the expression for input\nimpedance of transmission line (TL) section:\nz = 1+izg 1+iz\u03c6\u2248, (5)\n  in 1+ iz\u22121g 1+ iz\u22121\u03c6\nwhere small \u00abz\u00bb denoting normalized to Z impedances\n  0\nwhich are used for mathematical letups simplification;\ng=tg .\u03c6\nFor conditions (3) Eq. (5) yields\nyin \u22481+iz\u22121\u03c6, zin \u22481+iz\u03c6, (6) where y is input admittance. Second summands in\nin\nEqs. (6) correspond to B and X in T- and \u03c0-equivalent circuits in Eqs. (2).\nExpansion of the impedance range increases the rigour of conditions (3) as well as the achievable values of capacitance and inductance according to Eqs. (4).\nIII. DETAILS OF MODELING\nAs ECIs are 3D structures with complex geometry and boundary conditions it is necessary to use 3D electromagnetic simulators. In the paper simulation was held in software package \u0421ST Microwave Studio. This package can be considered as a \u00abvirtual\u00bb vector network analyzer with possibility of structure's fields\u2019 visualization.\nFor minimum transmission coefficient T0 and its frequency f0 found in Microwave Studio TL section model is created. The parameters of the model are defined by expressions:\n 1\uf06d 1\u2212T2 T 2 zl,h= 0 ,zl\u22480,zh\u2248\n\uf8eb c \uf8f62 ,\u03b5eff=\uf8ec   \uf8f7,(7)\n   T0 2 T0\n\uf8ed 4 f0l \uf8f8 where \u03b5eff is the effective dielectric constant, c is the speed of\nlight in a vacuum.\nThe expressions for zl,h and \u03b5eff are found from the formula\nfor TL section transmission coefficient and from relationship \u03b2l = \u03c0 \/ 4 for the frequency f0, respectively.\n  CADSM 2015, 24-27 February, 2015, Polyana-Svalyava (Zakarpattya), UKRAINE\n As shown in the next section, characteristics obtained by means of these two models are in a good agreement which suggests the possibility of the TL model using for the analysis of complex 3D ECI.\nTL models were created in the software package The MathWorks MATLAB. One of the main advantages of this package is easy and intuitive working with matrices and arrays. In the paper there was the need to work with large dimensions matrices and cyclic operations. Syntax of this package simplifies writing of cycles and sometimes allows replacing a cycle by an operator. This package was used also for experimental results treatment and minimizing parameter\u2019s error of quasi-lumped element.\nIV. CRYSTAL-LIKE QUASI-LUMPED ELEMENTS\nFig. 1 shows low-impedance (a-c) and high-impedance ECI d. ECI a is formed by the blind metallized hole from metallized surface side and signal conductor placed above it. Due increasing its area ECI b has a higher efficiency. ECI c is formed as the blind metallized hole from signal conductor side.\nab\ncd\nFig. 1 ECIs\nFigs. 2-3 show characteristics for low-impedance ECI\ncompared to conventional structure. ECIs parameters: signal conductor width w = 1.1 mm (with Z0 = 50 \u03a9), hole diameter d = 8 mm, gap between hole and signal conductor for ECIs a and b as well as the thickness of the dielectric under ECI c 0.28 mm. Conventional structure is fulfilled in the form of a square with the side equals diameter d. Substrate material \u2015 Rogers RO3010, thickness of dielectric 1.28 mm, relative dielectric constant 10.2, dielectric loss tangent 0.0023 at the frequency 10 GHz, metallization thickness 0.035 mm.\nFig. 2 shows the transmission coefficient characteristics of ECI c (curve 1) and conventional structure (curve 2). Values of f0 and T0 are 3.21 and 3.25 GHz, \u201316.24 and \u20135.48 dB, respectively. Vertical line limits frequency range by the frequency f0\/2 from the condition l = \u03bb\/ 8 . Characteristics 1 and 2 are calculated using 3D-simulation. Curves 3 and 4 are the transmission coefficients characteristics of lumped capacitances with parameters according to Eq. (4). Characteristic 5 corresponds to the TL model.\nIn the frequency range up to f0\/2 the characteristics 1 and 2 are close to the characteristics 3 and 4. The effectiveness of ECI characterized by the value of T0 is much higher compared to conventional structure.\nAccording to Eqs. (7) and [1] the equivalent values of Zl and \u03b5eff of ECI and conventional structure are equal 3.9 and\n14.4 \u03a9, 8.5 and 8.3, respectively. According to Eq. (4) the values of capacitance are equal 20.0 and 5.2 pF.\n10\nCADSM 2015, 24-27 February, 2015, Polyana-Svalyava (Zakarpattya), UKRAINE\n0\n-4 2 5 -8 4 1\n\u0421, pF T, dB\n-12 -16\n3\n0.1.2\n12 8 4\n2\n4 1\n0,5\n1,5\nf\/f0\nFig. 2 Transmission coefficient characteristics\nFig. 3 shows the capacitance dependences of ECIs a, b and\nc (curves 1, 2 and 3, respectively), as well as of conventional\nstructure (curve 4). 20\n16 3\n0345678 d, mm\nFig. 3 \u0421apacitance dependences\nEquivalent capacitance of ECI c is slightly larger than\ncapacitance of ECI b. The capacitance of ECI \u0430 with d=8mm and w=1.1mm is equal 3.8 pF. In conventional solution such capacitance corresponds to w = 5.4 mm.\nHigh-impedance ECI d is formed by through hole in metallized surface and dielectric with a small-diameter conductor above it.\nLet us compare ECI d and conventional structure as quasi- lumped inductances. ECI parameters: d = 8 mm, diameter of the mounted wire 0.1 mm. Conventional structure is a signal conductor section with length l = d and w = 0.1 mm. Substrate material \uf8e7 Rogers RO3010.\nAccording to the 3D-simulation the values of f0 and T0 for ECI and conventional structure are 7.91 and 3.78GHz, \u20139.25 and \u20131.95 dB, respectively. According to Eqs. (4) and (7) and [1] for ECI and conventional structure the following values of Zl, \u03b5eff and L are obtained: 281.2 and 100.2 \u03a9, 1.4 and 6.1, 8.9 and 6.6 nH.\nFrom the experimental transmission coefficient characteristics measured by Rodhe&Schwarz \u00ae FSH Handheld Spectrum Analyzer the following values of Zl and L are calculated: 286.9 and 103.1 \u03a9, 9.1 and 6.8 nH.\nFig. 4 shows the dependences of equivalent impedance for ECI (curve 1) and inductance for ECI (curve 2) and for\n conventional structure (curve 3).\nConventional structure impedance does not depend on\nlength. ECI impedance increases with diameter increasing. As a result steepness of dependence 2 more as compared to\nEqs. (4). This multiplier will be also in subtrahend denominators in Eqs. (9). The condition (10) is met by an optimal value k o .\nFig. 5 shows the dependences of |\u03b4|m without (curve 1) and with (curve 2) error minimization, as well as the dependence of ko (curve 3) for the frequency range 0...f0\/2. Minimization notedly reduces error of reactive parameters realization by quasi-lumped elements.\ndependence 3. 300\n250\n200\n150\n100\n503 4 5 6 7 82\nd, mm\nFig. 4 Impedance and inductance dependences\nBecause of value \u03b5eff decreasing increase of inductance value by ECI compared to a capacitance less, however frequency range increases substantially. The simultaneous estimation of inductance and frequency range increase is given by product P = Lf0\/2. According to Eqs. (4) and (7) we have P=Zh\/8. The increase of P for ECI with d=3 and 8 mm is 1.5 and 2.8, respectively.\nV. ERROR MINIMIZATION\nLet us analyse the error of reactive parameter realization by quasi-lumped element (ECI or conventional) and consider the possibility of its minimization.\nInput admittance and impedance of lumped capacitance and inductance is determined by Eqs (6) with the sign of \u00ab=\u00bb. According to Eq. (5) the input admittance and impedance of quasi-lumped elements are determined by expressions\ny\u2032 =1+g2+i(z\u22121\u2212z)g,z\u2032 =1+g2+i(z\u2212z\u22121)g.(8) in 1+ z2g2 in 1+ z\u22122g2\nReal parts in Eqs. (6) and (8) are equal only at zero frequency. In contrast of Eqs. (6) real parts in Eqs. (8) increase with frequency.\nAccording to Eqs. (6) and (8) relative errors of reactive parameters realization by quasi-lumped elements with error sign account are determined by expressions\n(z\u22121 \u2212 z)g (z \u2212 z\u22121)g \u03b4C=1\u2212\u221212,\u03b4L=1\u2212\u221212,(9) (z +zg )\u03c6 (z+z g )\u03c6\nwhere indexes \u00abC\u00bb and \u00abL\u00bb correspond to the capacitance and inductance. When zC = zL-1 we have \u03b4C = \u03b4L = \u03b4.\nExcept for one value of z C = z L -1 in frequency range\n0...f0\/2 we have \u03b4+ \u2260 \u03b4\u2212 , where \u03b4\u00b1 are maximum positive\nand minimum negative values of \u03b4.\nFor maximum error |\u03b4|m minimizing it is necessary to\n10 18\n25 20 15 10\n5\n0 2 4 6 8 zh\n0.5 0.25 0.167 0.125 z Fig. 5 Dependences of \u03b4 m and k\u043e\nVI. CONCLUSION\n2 3\n6 4\n3\n1\n2\n1,1 1.0\n11\n.\n|\u03b4|m, %\nko\nZ, \u03a9\nL, nH\nfulfill condition\nTo that aim we will enter a multiplier k in the right side of\n\u03b4=\u03b4=\u03b4 . (10) \u2212+m\nCADSM 2015, 24-27 February, 2015, Polyana-Svalyava (Zakarpattya), UKRAINE\nECIs allow quasi-lumped reactive elements realization with a substantially greater parameter values compared to conventional microstrip structures with the same size. For required parameter values such increase corresponds to sizes reduction.\nOptimization of the relationship between reactivity parameter value and equivalent impedance of quasi-lumped element allows to minimize quasi-lumped element realization error.","link":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7230781","bibtex":"@INPROCEEDINGS{7230781, \nauthor={P. Bidenko and E. Nelin and A. Nazarko and V. Popsui}, \nbooktitle={The Experience of Designing and Application of CAD Systems in Microelectronics}, \ntitle={Quasi-lumped crystal-like reactive elements}, \nyear={2015}, \npages={9-11}, \nkeywords={crystal structure;inhomogeneous media;electromagnetic-crystal inhomogeneity;parameter error;quasilumped crystal-like reactive elements;Capacitance;Conductors;Impedance;Inductance;Mathematical model;Microstrip;Minimization;electromagnetic crystal;electromagnetic inhomogeneity;quasi-lumped reactive element}, \ndoi={10.1109\/CADSM.2015.7230781}, \nmonth={Feb},}\n"}}