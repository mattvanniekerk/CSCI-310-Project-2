(title)VARIATIONAL IMAGE-BASED RENDERING WITH GRADIENT CONSTRAINTS(/title)
(authors)Grégoire Nieto, Frédéric Devernay, James Crowley(/authors)
(conference)2016 International Conference on 3D Imaging (IC3D)(/conference)
(abstract)Multi-view image-based rendering consists in generating a novel view of a scene from a set of source views. In gen- eral, this works by first doing a coarse 3D reconstruction of the scene, and then using this reconstruction to establish cor- respondences between source and target views, followed by blending the warped views to get the final image. Unfortu- nately, discontinuities in the blending weights, due to scene geometry or camera placement, result in artifacts in the tar- get view. In this paper, we show how to avoid these arti- facts by imposing additional constraints on the image gradi- ents of the novel view. We propose a variational framework in which an energy functional is derived and optimized by itera- tively solving a linear system. We demonstrate this method on several structured and unstructured multi-view datasets, and show that it numerically outperforms state-of-the-art meth- ods, and eliminates artifacts that result from visibility discon- tinuities.(/abstract)
(content)1. INTRODUCTIONMulti-view image-based rendering consists in generating a novel view of a scene from a set of source views. In general, this works by first doing a coarse 3D reconstruction of the scene, called a geometric proxy, then using this reconstruc- tion to establish correspondences between source and target views(Figure 1 (a)), followed by blending the warped views to obtain the final image. Recent work by Pujades et al [24] proposed a Bayesian formulation of the image-based render- ing problem, building on previous work by Wanner and Gold- luecke [11, 29]. They showed that the weight of each source image in the novel view could be formally deduced from the camera properties, image content, and accuracy of the geo- metric proxy, leading to a formalization of the heuristic blend- ing weights proposed initially by Buehler et al [3]. Most of the “desirable properties that [...] an ideal image-based ren- dering algorithm should have” [3] were thus given a formal explanation, except for the continuity property. Therefore, discontinuities in the source image weights, which are mainlyThanks to DGA for fundingdue to scene geometry or camera placement, result in artifacts in the target view.In this work, we show that a way to avoid these artifacts is to impose additional constraints on the image gradients of the novel view. These constraints come from a simple obser- vation: an image contour in the target image should also be present in source images where this part of the scene is vis- ible. An energy functional similar to the one in Pujades et alis derived, which is composed of the usual data term and smoothness term, but the data term has an additional term which takes into account these gradient constraints. The re- sulting energy is optimized in a variational framework, by it- eratively solving a linear system.Our method is composed of two stages: a 3D reconstruc- tion pipeline (section 4.1) followed by the target view render- ing (section 3). The depth maps we obtain from multi-view stereo are used to compute a warp function from each input image to the novel view. The target view is computed via a novel variational formulation of the image-based rendering problem. We demonstrate the method on several unstructured multi-view datasets (section 4) and show that not only it nu- merically outperforms state-of-the-art methods (section 4.2), but also it eliminates artifacts that originated from visibility discontinuities. We conclude that taking into account both intensities and gradients in image-based rendering methods offers an elegant solution to enforcing the continuity property initially devised by Buehler et al.2. PREVIOUSWORKImage-Based Rendering (IBR) has been extensively reviewed by Shum et al [25]. Most state-of-the-art [15, 26, 17, 18, 5] approaches use a coarse 3D reconstruction of the scene, called a geometric proxy, which may have various degrees of ac- curacy. Ortiz-Cayon et al [22] choose to over-segment the image and compute the quality of several IBR algorithms on each super-pixel. Then the output of the best IBR algorithm for each super-pixel is picked. These algorithms are largely inspired by the Unstructured Lumigraph [3], that performs a blending of the k-nearest views, weighted by angles and dis- tances to the target view, thus guarantying a smooth camera blending field. The continuity of the resulting blend in im- age domain is ensured by enforcing spatial smoothness on 978-1-5090-5743-6/16/$31.00 ⃝c2016 IEEE
 Fig. 1: (a) We coarsely estimate the geometry of the scene to register input images. The warp functions τk link the input images vk to the target view u. (b) The target view u is reconstructed by blending the source images in intensities. (c) Our method consists in appending constraints on the gradient of the solution; we show that it is equivalent to a Laplacian blending and that it removes high frequency blending artifacts.these weights, but temporal artifacts may still occur if the contributing cameras are too sparse. Davis et al [6] propose a viewpoint-subdivision rendering technique to create a better blending field. Nevertheless, blending weights are still heuris- tics and the choice of cameras for rendering is arbitrary.Disposing of heuristics and tunable parameters is a key objective in [29], who propose a physics-based Bayesian for- mulation of image-based rendering. The weight of each in- put view in the final blending is automatically deduced from the mathematical equations by deriving an energy functional. Pujades et al [24] went further by integrating geometric un- certainty in the Bayesian formalism. They then obtain new weights that favor cameras satisfying both epipole consistency and minimal angular deviation, two principles stated by Buehler et al [3] to describe the ideal IBR algorithm. They were unable, however, to provide a formal derivation that leads to the continuity principle, especially near the borders of each camera view field. We show that introducing an ad- ditional term in the energy functional, not only constrains the image intensities but also constrains the image gradients, pro- viding an elegant solution to the continuity principle.The key idea of high-quality rendering is that the qual- ity of the image solution often relies on the constraints put on the search space. As a consequence, finding the right reg- ularization or image prior has been widely researched in or- der to obtain high-quality images. The main contribution of Fitzgibbon et al [7] is the use of texture priors computed from a large database of patches to constrain the solution, inde- pendently from the input scene data. This idea was recently extended by Flynn et al [8] who perform new view synthesis via a deep network architecture, trained by a huge database of real-world image sets. In contrast our method does not rest upon any strong prior knowledge about the new view to syn- thesize, but rather makes better use of the data provided by the input views to add more constraints on the solution. There-fore our algorithm does not require large numbers of picture sets from the world’s imagery to create a high-quality images. Image fusion in the gradient domain has received much interest in recent years beginning with the seminal paper of Perez et al [23], for applications such as image editing [19], inpainting [16] or image stitching [30]. The closest work to ours for image-based rendering is probably [15], who gener- ate a new point of view by reconstructing the gradients in the target view, followed by an integration of these gradients on the GPU to recover the final image. However, their method is limited to interpolating between two views, and they nei- ther address the generic case where the input views may have very different viewpoints, fields of view, and resolution, nor produce super-resolved images. Our method addresses these issues and proposes a more generic framework for multi-viewimage-based rendering.3. VARIATIONALIMAGE-BASEDRENDERINGOur goal is to synthesize an optimal novel image u : Γ ⊂ R2 → R at the target viewpoint from the input images vk : Ωk ⊂ R2 → R. For the sake of simplicity, image values are taken as scalars, but this easily generalizes to color images vk : Ωk → R3. In all our experiments we process the images in the RGB color space. Source images are registered via the warps τk that transform any point xm = (xm , ym )⊺ of the input view k into the corresponding point xp = (xp , yp )⊺ in the target view:τk: Ωk → Γ xm  → xp3.1. ImageFormationModel(1)As commonly assumed in the super-resolution literature [1, 14], we consider the intensity value vk (xm ) at a point xm
  in the low-resolution observed image k to be the convolution of values in the super-resolved image with the point-spread function (PSF) b. Given an ideal super-resolved image u at the position of the target view defined over Γ, and a warp τk that maps points from Ωk (low resolution domain) to Γ (high resolution), if we discard for now the visibility effects, the intensity in the observed image k can be written as:vk(xm)=  u◦τk(x)b(x−xm)dx, (2) Ωkorsimplyvk =b∗(u◦τk).The PSF b : Ωk → [0, 1] is a probability density functionthat can be turned into bk : Γ → [0,1] by the change of variable x′ = τk(x) so thatFig. 2: Warp and blending weight discontinuities cause arti- facts (in red). Left: a close-up to a view rendered by min- imizing the energy 7. Right: a warp that presents visibility discontinuities that caused the artifacts.3.2. MaximumAPosteriori(MAP)EstimationThe goal of the variational approach is to estimate the high resolution image u from the data (vk∗)k∈[1..K], where K is the number of inputs views. The estimator of u maximizes the posterior which is the probability of finding u given the input data. One can show that this is equivalent to minimizing the energyE(u) = Eintensity(u) + λEprior(u), (7)Eprior, often referred to as the smoothness term, comes from the image prior and prevents the emergence of high frequen- cies. λ is a parameter that controls the smoothness of the final solution. In this work, we use a total variation prior [12], Eprior(u) =  Γ |∇u|, which has several advantages over other more complicated image priors: this approach preserves strong edges and image contours, and is convex. A proof of conver- gence is given by Chambolle [4].Eintensity, often referred as the data term, is derived from the likelihood given the input image intensity [24]. This ac- counts for how well the current solution fits the data in the intensity domain:vk(xm) =  u(x′)bk(x′ − τk(xm)) dx′ (3)ΓThere are several ways of computing the warped PSF bk, depending on how we model the initial PSF b. The more com- mon assumption is to consider that b is a 2D Gaussian cen- tered at position xm. Since Gaussian filters have infinite sup- port, it is quite difficult to implement in practice. A simpler model of the PSF is to assume pixels are square and uniformly sensitive to light, so that the PSF is a uniform square density function. Noting A the area of a pixel centered on (0, 0) in a source view k, we get 1 b(x,y)= A20if−1 ≤x,y≤1 A Aelsewhere.(4)   Under the assumption that the warp τk is locally linear, the warped PSF is a uniformly distributed parallelogram. In this case, we can make an even stronger assumption by suppos- ing that the warp preserve the pixels (their area and squared shape), which is actually untrue but largely simplify the im- plementation. From now on, we take a unit pixel area. Since the intensity is constant and equals u(p) over all the pixel area in the target view, the previous convolution can be written as in [14]: K 1   Eintensity(u) = 2k=1ωk(u)((b ∗ (u ◦ τk) − vk∗))2 dx. Ωk vk(xm) =   u(p)   bk(x′ − τk(xm)) dx′, p∈Γ pso the pixel intensity m in the source image is vk(m) =   Bk,m,pu(p),p∈Γ(5)(6)(8) The terms ωk(u) are the per-pixel contribution of each in- put view k. They depend on the gradient of the current solu- tion u and the geometric uncertainty of the 3D reconstruction.3.3. AppendingtheGradientTermSince the geometry and the visibility may be discontinuous, the term ωk(u) in 8 may be discontinuous too, resulting in artifacts in the synthesized image that may appear as spuri- ous edges or textures (Fig. 2). The IBR method should pre- vent these contours from appearing: actually, an image con- tour synthesized in the target image should also be present in source images where this part of the scene is visible.where Bk,m,p =  p bk(x′ − τk(xm)) dx′ is the area of inter- section between the projection of the pixel in the target view and a pixel p of this view. It is equivalent to bilinearly inter- polate the intensities of u.
To enforce this property, we add an extra term Egradient(u) to the previous energy (8) that forces the current solution to also fit the data in the gradient domain:Egradient (u)be the vector of all the pixels of every input view put in one column (v0(0) v0(1) ... vK−1(M −1))⊺, U the column vector storing the current solution (u(0) ... u(N − 1))⊺, and B the KM ×N matrix that stores the Bk,m,p coefficients, we may naturally write V = BU. Consequently, we can express the energy (8) as a linear system:Eintensity(U) = (BU − V∗)⊺W(BU − V∗), (18) where W is a KM × KM diagonal matrix that stores the(β )|ω . To minimize this energy we derive the= −log p(∇v0 . . . ∇vK−1|∇u) K−1(9) = − logp(∇vk|∇u) (10)  k=0= (∇vk−∇vk∗)2dx (11) ′Ωk Ωkweights |Jx k klinear system, and obtain the normal equations that provide(∇(b ∗ (u ◦ τk)) − ∇vk∗)2 dx.(12) Finding u that minimizes this energy is equivalent to solv-ing the Laplace equation:∆((b∗(u◦τk)−vk∗) = 0, (13)where ∆ = ∇.∇ denotes the Laplacian. We instantly deduce the derivative of the functional:dEgradient(u) = (|∂τk |−1  ̄b ∗ (∆(b ∗ (u ◦ τk)) − ∆vk∗)) ◦ βk.∂z(14) The βk are the backward warp that appear thank to the change of variable in the integral.  ̄b is the adjoint of the blur kernel b. The warps τk are those which were estimated before- hand, and thus lack precision. This uncertainty has a drastic effect on the computation of ∆(b∗(u◦τk )). As a consequence, we chose to compute the Laplacian of u first, then warp it in the Ωk domain. Under the assumption that the warps τk can be locally linear, we neglect their second order derivatives and=ˆ an estimator for the solution U:B⊺WBUˆ = B⊺WV∗. (19)The matrix B⊺WB is generally not invertible. This linear system can be solved by any linear least square solver.Akin to the data term on color, the data term on image gradient isEgrad(U) = (B∇U − ∇V∗)⊺(B∇U − ∇V∗) (20) and can be derived likewise.4. EXPERIMENTSANDVALIDATION 4.1. 3DReconstructionOur method takes as input an unstructured set of source views with no particular structure [3], as opposed to view interpola- tion methods which usually take stereo pairs, or methods that are based on a structured light field [29]. To be as generic as possible, it only requires the warp functions τk that we obtain via a classic 3D reconstruction pipeline [9]. It con- sists in camera calibration via bundle adjustment [20], fol- lowed by an MVS reconstruction [10] to get a depth map for each input view (Fig. 3). Knowing the camera parameters and the depth of each input pixel, we can deduce per-pixels correspondences between the source images and the targetFig. 3: The depth map of an input view. obtain:∆(b∗(u◦τ )) = b∗ k H k + k H k . (15) ∂τ ⊺ ∂τ ∂τ ⊺ ∂τ   k ∂x u ∂x ∂y u ∂y    H = ∂∇u is the Hessian matrix of u. Due to uncer- u ∂xtain depth maps that cause strong discontinuities in warps, the Hessian matrix may be very unstable. For the computa- tion, and in this case only, we assume τk(x) ≈ x + d so that∆(b∗(u◦τk)) = b∗(trace(Hu)◦τk) = b∗(∆u◦τk). (16) The final form of the energy to minimize is thus:E(u) = αEintensity(u) + γEgradient(u) + λEprior(u). (17)We minimize the functional (17) via Fast Iterative Shrink- age Thresholding Algorithm (FISTA) [2].3.4. DiscretizationOne can wonder how we go from a continuous model of the problem to a numerical solution. For each pixel m of each input view k we obtain an equation similar to (6). Let V∗  
  Fig. 4: Results on datasets fountain and herzjesu. The bottom row shows some of the input views. Note that the parts of the target view that are not visible by any of the inputs are filled by a push/pull inpainting algorithm.view. We use the depth to derive the blending weights as it is demonstrated in [24]. For further information about the reconstruction pipeline, more specifically occlusion handling and the derivation of the blending weights, see [21].4.2. ResultsA set of experiments (Figure 5) is performed on real views taken from Strecha’s dataset [27], fountain and herzjesu. Ge- ometric proxies are estimated according to the pipeline de- scribed in section 4.1. For both datasets we remove the cen- tral view, render it in the same dimensions as the input images and compare the result with the original for visual evaluation. All experiments were performed on GPU with an nVidia GTX Titan. Convergence is reached within 3 minutes for an input set of 11 input images of size 3072 × 2048. Since the conver- gence time strongly depends on the initialisation of the solu- tion, the performance could be significantly increased. How- ever, one drawback of our variational method is that it does not perform real-time rendering. To show that the quality of our results does not depend on the initialization, we start the optimization process from a null image.Since some parts of the target view are not visible from the input views due to self-occlusion, inpainting is needed to fill potential holes. To that end, we implemented the push/pull algorithm as it is described in [13]. Firstly, the push stage decomposes the final image u into its Gaussian pyramid. At each level of the pyramid the image is filtered by a Gaussian- like 5 × 5 kernel and down-sampled by a factor 2. Only non- null value pixels contribute to create the upper-level image, so that at the coarsest level of the pyramid the image has no hole.Note that we do not need to down-sample to the 1×1 pixel im- age; in our experiments the 6 × 4 pixel image does not contain any holes. Secondly the pull stage propagates missing infor- mation from the top of the pyramid downwards to the finest level. At each level, holes are filled with the corresponding pixels from the upper coarser image.At first we tested for a null gradient term (γ = 0.0). Since the linear system to be solved is ill-constrained, high frequen- cies appear in areas that few cameras see. These artifacts are emphasized by a very noisy depth estimation near occlusion regions (around the fish on the fountain or the Jesus on the wall). To remove these artifacts and eliminate high frequen- cies, a stronger smoothness term is commonly used. Conse- quently we increased the λ parameter that controls our To- tal Variation regularizer to 0.003. However the results are not that convincing: although most high frequencies are re- moved, some image features are lost compared to originals. To fix this problem, we achieved a third rendering where λ is set to its initial value (0.002) and the gradient data term is appended to the equation (γ = 1.0). For the final solution to keep the original colors of the input image, we keep the intensity data term as a bias but choose a small controlling parameter (α = 0.1). One can notice that artifacts are com- pletely removed, while preserving all of the image features. We conclude that appending the gradient data term prevents the emergence of spurious contours near visibility borders, hence guaranteeing the continuity property.Some numerical results are presented in table 1. To show that our rendering algorithm outperforms state-of-the-art meth- ods when synthesizing a specific view only, we generate sev- eral input views that we preliminarily removed from the input
α = 1.0,γ = 0.0,λ = 0.002 ( [24, 29])α = 1.0,γ = 0.0,λ = 0.003 ( [24, 29])α = 0.1,γ = 1.0,λ = 0.002 (ours)fountain – view 221.03 13221.09 12021.16 107fountain – view 526.00 7426.14 6426.36 51fountain – view 822.00 14022.08 12522.16 111herzjesu – view 221.73 18621.96 15321.93 143herzjesu – view 423.13 19423.81 13023.90 115herzjesu – view 618.08 34918.26 28718.31 273Table 1: Numerical results on real-world datasets [27]. Our method is compared against state-of-the-art methods [24, 29], for which there is no gradient constraints (γ = 0.0). For each result, the first value is the PSNR (bigger is better), the second value is DSSIM in units of 10−4. DSSIM = 104(1 − SSIM) [28] (smaller is better). The best value is highlighted in bold. See text for a detailed description of the experiments.set and kept as reference views for quantitative comparison. Two numerical measures are computed with respect to the ref- erence view to evaluate our results: PSNR (the higher the bet- ter) and DSSIM = 104 (1 − SSIM) (the lower the better). Al- though the increase of the PSNR score is noticeable but not conclusive, the strong improvement of the DSSIM demon- strates a higher structural similarity with the reference view, which accounts for the elimination of most visual artifacts.5. CONCLUSIONWe presented an image-based rendering method that renders a novel view from a generic and unstructured set of input views. This method is inspired by previous work by Pu- jades et al [24], which proposed a formulation for most of the “desirable properties” that were listed in the seminal work by Buehler et al [3], using a Bayesian formulation, and opti- mizing the target image in a variational framework. The only property that could not be formally derived was the continu- ity property, which states that the contribution of each input view to the pixels of the target image should be a continuous function of the pixel coordinates.We showed that an alternative approach for enforcing the continuity property is to state that edges, contours or textures should not be created in the target image if they are not present in the source images. This results in an additional data term, based on image gradients, which can be added to the energy functional. The energy can then be solve by iteratively solv- ing a linear system devised from the energy functional. The results show an improvement over previous intensity-based unstructured IBR methods, both in terms of objective image quality measurements, and in terms of subjective quality.Limitations: Despite the noticeable improvement of the image quality, our method does not fully eliminate all the vis- ible artifacts. It could be reworked to optimize directly the target image gradients, rather than intensities, and the target intensity could then be reconstructed by solving the Poisson equation, as is done in Kopf et al [15]. This should totally re- move any variations in the synthesized image that come from the discontinuity of the visibility functions, and are still visi-ble, although attenuated, in our results.Future work: As shown in section 3, appending gradientconstraints as a new energy data term is comparable to solv- ing the Poisson equation. In the stitching literature [30], it is known as Laplacian blending. Considering that the Lapla- cian of an image behaves like a band-pass filter, blending the Laplacian of the input images is analogous to blending the im- ages in the frequency domain for a specific band of frequen- cies that depends on the scale of the Laplacian. In our case the Laplacian is computed at the original scale of the image, level 0 of a Laplacian pyramid. Therefore we blend the im- ages for the band of highest frequencies, and high-frequency artifacts due to naive intensity blending are prevented. Our fu- ture work will focus on an extension to all scales via the use of Laplacian pyramids, enabling a complete Laplacian blend- ing to prevent not only high-frequency but also low-frequency artifacts. Fig. 5: Rendering the central view with different energy parameters. Each column shows the results on datasets fountain and herzjesu [27] by applying a special set of parameters (α, γ, λ) that control the amount of terms in the energy formula 17. Proposed approach is for γ ̸= 0. State-of-the-art approach [24, 29] creates high-frequency artifacts due to blending intensities only. A higher smoothness parameter λ = 0.003 partially removes these artifacts at the cost of detail loss. Our approach preserves detail and show finer results by appending constraints of the gradients of the solution, forcing a Laplacian blending. (/content)
(link)http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823449(/link)
(bibtex)@INPROCEEDINGS{7823449, 
author={G. Nieto and F. Devernay and J. Crowley}, 
booktitle={2016 International Conference on 3D Imaging (IC3D)}, 
title={Variational image-based rendering with gradient constraints}, 
year={2016}, 
pages={1-8}, 
keywords={geometrical optics;image reconstruction;object detection;optical images;variational techniques;camera placement;coarse 3D reconstruction;image gradient constraints;linear system;multiview image-based rendering consists;scene geometry reconstruction;target detection;variational image-based rendering;Cameras;Geometry;Image reconstruction;Image resolution;Laplace equations;Rendering (computer graphics);Three-dimensional displays;Computational Photography;Computer Graphics;Image-Based Rendering}, 
doi={10.1109/IC3D.2016.7823449}, 
month={Dec},}(/bibtex)