(title)BODY EXPRESSION RECOGNITION FROM ANIMATED 3D SKELETON(/title)
(authors)Arthur Crenn, Rizwan Ahmed Khan, Alexandre Meyer, Saida Bouakaz(/authors)
(conference)2016 International Conference on 3D Imaging (IC3D)(/conference)
(abstract)We present a novel and generic framework for the recogni- tion of body expressions using human postures. Motivated by the state of the art from the domain of psychology, our ap- proach recognizes expression by analyzing sequence of pose. Features proposed in this article are computationally simple and intuitive to understand. They are based on visual cues and provide in-depth understanding of body postures required to recognize body expressions. We have evaluated our ap- proach on different databases with heterogeneous movements and body expressions. Our recognition results exceeds state of the art for some database and for others we obtain results at par with state of the art.(/abstract)
(content)1. INTRODUCTIONMany applications would benefit from the ability to under- stand human emotional state in order to provide more natu- ral interaction e.g video games, video surveillance, human- computer interaction, artistic creation, etc. Emotion is a com- plex phenomena. Expression of an emotion could be personal and subjective as two persons could perceive and interpret dif- ferently the same expression. Its perception changes from one culture to another. Furthermore, human expresses emotional information through several channels, like facial expression, body movement and sound. Several studies from various dis- ciplines have shown that body expressions are as powerful as those of the face to express emotion [16]. Unfortunately, to the best of our knowledge, few publications have focused on this issues.This paper presents a method to detect and classify ex- pression through a sequence of 3D skeleton-based poses. The challenge is to find common informations between all the movements of the same expression. And this, while the ex- pression is embedded in the action performed by the per- son, e.g. nervous person walking pattern contains the ac- tion of walking and the emotional state to be nervous. With the growth and ease of accessibility of devices that track 3- dimensional body like the Kinect [21, 3] or accelerometer-based motion capture system [23], it is easy to get data. Thus many applications will be benefited from real-time analysis of body movements. There are lots of method proposed in literature for facial expressions [5, 15, 12, 14], however, as mentioned above, there is a lack of research for the analysis of body movement to recognize expressions. Early proposed methods for body movements analysis [13, 18] were limited to specific movements or expressions. Taking an approach proposed by choreographer seeking to describe dance move- ments, Truong et al.[24] proposed descriptors calculated from the movements in order to recognize various gestures and ex- pressions. Inspired by Truong’s work and results of psycho- logical research [16], we have looked to quantify the space posture of the body, during an action, by introducing new set of features to characterize an expression. Besides, its com- putational simplicity allowing the recognition of expression on heterogeneous body movements in real-time. Our method has the advantage to be reusable in many domains as move- ment synthesis processes for computer graphics and anima- tion. The paper is organized as follows. Section 2 presents the state of the art of the emotion analysis. Section 3 details the set of features we proposed. Section 4 shows the results obtained on different databases and the comparison with other state of the art’s method. Finally, Section 5 concludes the pa- per.2. RELATEDWORKIn recent years, researchers have considerably focused on the automatic facial expression recognition (FER) [5, 15, 12]. How- ever, computer vision domain is lacking research on the de- tection of emotion based on human’s posture. In analogy af- terward we use the term ”Body Expressions” to refer to emo- tion expressed by body posture. While, psychological studies show that the human’s posture is as powerful as facial expres- sions in conveying emotions [20]. That’s why, psychologists have sought to understand what bodily information is neces- sary for recognizing the affective state of one person. Psycho- logical studies [2, 10] have investigated the part of the body form versus movement in affect perception. These studies conclude that both form and motion information are impor- tant for perceiving emotions from body expressions. Finally, after the form and movement features, psychologists have in-978-1-5090-5743-6/16/$31.00 ⃝c2016 IEEE
troduced two main levels of bodily details which are high and low-level descriptions [16]. The high level description is of- ten based on the Laban approach which describes body ex- pressions in a global way. The low description is another ap- proach to describe body expression by providing more precise features like distance between joints and angle between body segments. Some techniques related to psychology studies fo- cus on very specific actions. Bernhardt and Robinson [4] pro- posed a framework to detect implicitly communicated affect of knocking actions. Their method is based on segmentation to divide complex motions into a set of automatically derived motion primitives. Then, they analyzed the parsed motion in terms of dynamic features, i.e. velocity, acceleration and jerk. They obtained a range of 50% to 81% correct classification rate and we are also going to compare our method with their results (refer Section 4 for comparison). Karg et al.[13] pro- posed method for gait patterns recognition. They investigated the capability of gait to reveal a person’s affective state. Ac- cording to their study, speed, cadence and stride length are important factors to correctly discriminate different expres- sions of a human gait. Paterson et al.[22] confirms the role of velocity for the discrimination of affect. They performed visual experiments and concluded that speed plays vital role in the perception of affect.Below mentioned are few state of the art articles that pro- posed heterogeneous body movement analysis. Castellano et al.[7] proposed a method for automated video analysis of body movement and gesture expressiveness. They used move- ment expressiveness to infer emotions. They also present two methods for the classification. The first one is based on a direct classification of time series whereas the second one uses a meta-features approach. A meta-feature is the statis- tical calculation on a set of features in order to abstract the time. They obtained correct classification accuracy of 61% on their database which contains four expressions (Anger, joy, pleasure and sadness). Kleinsmith et al.[18] proposed a sys- tem that incrementally learns to recognize the body expres- sion. Their system is based on form features (i.e. distance from one joint to another one) and motion features. They ob- tained a correct classification rate of 79%, we will compare our method with their results (refer Section 4 for compar- ison). Truong et al.[24] proposed a new set of 3D gesture descriptors based on the laban movement analysis model for gestures expressiveness. They obtained high recognition rates for action recognition (F-Score: 97%) on Microsoft Research Cambridge-12 dataset [9]. They also tested their classifica- tion approach on their own proprietary database which con- tains 882 gestures and achieved best F-Score of 56.9%.We can take inspiration from the domain of animation in computer graphics as they also analyze body movement and gesture expressiveness. The researchers working in the domain of animation generation proposed different methods for modification of animation using skeleton transformations. This modification of expression can be achieved by chang-ing the timing, speed and the spatial amplitude of the motions of body part joints. [1, 11]. Some researchers [6, 25] have also used methods from signal processing (Fourier transfor- mation, motion wave-shaping, time-warping, etc.) in order to modify the expression of animations. Recently, with the same goal of synthesis expressive animations, Forger and Takala [8] proposed an approach based on the motion signals by using frequency components. Fig. 1: Overview of our framework.3. PROPOSEDFRAMEWORKOur motivation was to propose an efficient recognition method of body expressions on heterogeneous movements that rivals state of the art and in real time on a standard computer. Fol- lowing the foot prints of several previous works [18, 24, 8], we propose a set of novel descriptors based on geometry, mo- tion and frequency-based of body part joints. The overview of our approach is given in Figure 1 and in the following three steps. The rest of this Section is dedicated to the presentation of the features proposed by our method.1. Our framework computes low-level features for each frame of the motion capture data. We decomposed our features in three types: geometric features, motion fea- tures and Fourier features. The details of these features are described below.2. Startingfromalltheselow-levelfeaturesobtainedfrom a time sequence, we compute the meta-features i.e. mean and standard deviations for each feature. Since meta- features are independent of the time, we avoid the com- putationally complex time-warping step for synchro- nizing two animations.3. The resulting values of these meta-features are fed to the classifier which provides the expression classifica- tion of the input motion capture data.
The low-level features we propose are based on distances between body joints, area of triangle defined by specific joints, angles, velocity, acceleration and frequency of movement of different joints. Table 1 describes the set of all our basic fea- tures. We have a total of 68 features. Then, for each feature we compute the meta-features. Thus, our approach leads to a total number of 136 features in order to recognize the body expression. Figure 2 shows the set of meta-features for three expressions. For visual purpose, we are only showing first 76 features from the total of 136 dimensional feature vector. It can be observed in the referred figure that our features have a discriminative trend for these expressions. This discrimi- native ability is used in the proposed framework to robustly classify body expressions.(a) Histogram of our features extracted from happy knockingstudies, i.e., form and movement, high and low-level descrip- tions of bodily detail.The first feature V, is the global space occupy by the skele- ton. We use the size of the bounding box of the skeleton in the three directions. The second feature θ is the angle defined between the vertical axis y and the axis binding the center of the hip to the neck. According to studies in psychology, a per- son tends to extend his body for positive expression whereas for negative expressions a person takes a more compact and forward tilt posture.The following features are dedicated to correctly capture the configuration of each body part. Distance D between two joints is important feature to analyze body expression. For in- stance, we use the distance between hand to the shoulder on the same side. This distance gives indirect information about the elbow. We use distances between hands to hips, hands to shoulders and elbows to hips. Furthermore, a new idea intro- duced by our approach is the use of triangles A to correctly discriminate multiple expressions. The body being symmet- ric, we take joints on each side of the body, right and left side. The last point of the triangle is chosen on the axis of the body, i.e. either neck or hips. Since the majority of move- ments is anti-symmetric, the triangle gives lot of information about the expression. For each triangle, we compute its area and the three angles formed by these different triangles. With these four values, we extract information related to shape of the body. For instance one of these triangle-based feature is the area and the angles of the triangle defined by the neck and shoulders. Figure 3 shows the variation of triangles for two different body expressions taken at the same timing of walk- ing. We compute the area and the angle of triangles defined by the hands and the neck; the shoulders and the neck; the elbows and the neck; the hands and the hips.As mentioned in the state of the art, psychologists have showed that motion is an important characteristic to discrim- inate expressions for different gestures. Thus, we add to the feature vector the motion features: the velocity ⃗v and the ac- celeration ⃗a of the hands, shoulders, hips, head and elbows joints. The velocity is the first derivative of the position of the current joint. Acceleration being the second derivative of this position.Finally, we use the fast Fourier transformation F to ob- tained the frequency component of our selected joints. We compute the discrete Fourier transform with the fast Fourier transform algorithm on the signal of the joint angle. Section 4 compares the results by using only geometric features, only motions feature or only the Fourier features on the different databases.4. RESULTSANDANALYSISWe have tested our method on three databases (refer table 2 for summary of databases) which are presented below. Two of them are acted databases, while the last database consists   (b) Histogram of our features extracted from sad knocking(c) Histogram of our features extracted from angry knockingFig. 2: Features used for different expressions. These his- tograms show the discriminative property of our features.According to the studies of [13] and [4], we have decided to use the following joints for feature extraction: head, pelvis, elbows, shoulders and hands. Experimental results obtained by [13] showed that head, pelvis, elbow and shoulder joints embed most of the emotion of a movement. Results in [4] proved that hands are also important in conveying expressions for several action. Thus, analysis of these five joints is most important to recognize body expressions. All the features ex- tracted for body expression analysis are presented in Table 1 and are described below. We used many features to correctly fit to aspects of body expression described by psychological
   Id. Type of feature Description  V Space of the skeleton Size of the bounding box of the skeleton  θ   Angle  The three angles induces by the triangle formed by both shoulders and neckAngle between the vertical direction y and the axis binding the center of the hip and the head   D           Distance      Right hand to the hipsLeft hand to the hipsRight hand to the right shoulder Left hand to the left shoulder Right elbow to the hipsLeft elbow to the hips       A       Area    Triangle defined by both hands and neck Triangle defined by both shoulders and neck Triangle defined by both hands and hips Triangle defined by both elbows and neck     ⃗v         Velocity     Hands Shoulders Hips Head Elbows      ⃗a         Acceleration     Hands Shoulders Hips Head Elbows      F           Frequency      Change of angle with respect to time of the following joints : HandsShouldersHipsHead Elbows             Table 1: List of set of features that we propose to extract from the human postures for the classification of body expressions.  DataBase  Number of movements Number of expressions UCLIC [17]  183 4 Biological [19]  1356 4 SIGGRAPH [26]  572 8of synthetic animations generated by the method of Xia et al.[26]. We will refer the last database as SIGGRAPH database in the rest of this paper.1. Biological Motion [19]: this database consists of 1356 motions, especially knocking actions for 4 expressions (angry, neutral, happy, sad).2. UCLIC Affective Body Posture and Motion [17]: this acted database contains 183 acted animations for 4 ex- pressions (fear, sad, happy, angry).3. SIGGRAPH database [26]: the SIGGRAPH database is synthetic database generated from 11 minutes of mo- tion capture data. It contains 572 animations with 8 expressions or style (angry, childlike, depressed, neu- tral, old, proud, sexy, strutting). In above mentioned databases, the SIGGRAPH database includes the largest range of movements: jump, run, kick, walk.Table 2: Description of the databases used for the test of our method.Summary of results obtained by our methods using the different sets of features are presented in Table 3. Our pro- posed approach can run in real-time as it runs at  ̃50 fps (on PC running on i7-4710MQ with 8GB of RAM). Extraction of proposed features, except Fourier features, takes 10ms for each frame while the extraction of Fourier features takes 500ms for a sequence of 27 seconds with 1657 frames. Before the classification, we are scaling each attribute of our features to the range [1,+1] to avoid numerical difficulties during the cal-    
   DataBaseSet of features used  Results  UCLICAll features  78%  UCLICOnly geometric features  66%  UCLICOnly motion features  52%  UCLICOnly Fourier features  61%  SIGGRAPHAll features  93%  SIGGRAPHOnly geometric features  92%  SIGGRAPHOnly motion features  75%  SIGGRAPHOnly Fourier features  90%  BiologicalAll features  57%  BiologicalOnly geometric features  56%  BiologicalOnly motion features  48%  BiologicalOnly Fourier features  46%               (a) First frame of the depressed walking animation.(b) First frame of the proud walking animation.Table 3: Our results on the different databases and with dif- ferent set of features. Classification method is SVM.scenario for machine learning algorithm. Proposed frame- work achieved best results on the synthetic database (SIG- GRAPH) which is due to the fact that synthetic animations presents exaggerate expressions with high inter-class varia- tions.Table 4 shows that our method is competitive against state of the art on the UCLIC database with similar recognition rate. In the Biological Motion Database, movements are mainly knocking at a door (≈ 1200 animations out of 1356 anima- tions). The state of the art approach [4] uses this particularity to compute the average movement of knocking at a door and then subtracting this movement before running the recogni- tion in order to emphasis the expression. Their recognition rate for this biased method is 81%. Nevertheless, this trick is possible for very specific movements, when you assume that all movements are similar. They learn a movement’s specific bias. Since purpose of our proposed framework is to be ro- bust against heterogeneous movements, we can not apply this assumption. We believe that to evaluate [4] and our approach, their unbiased recognition rate of 50% is to be compared with our approach that obtained correct classification accuracy of 57%. Finally, to the best of our knowledge, no method in liter- ature on body expression analysis has tested the SIGGRAPH database.Table 4: Comparison of our methods using all features men- tioned in this paper to the state of the art methods.Fig. 3: Frames of the same action but with different body expressions. This figure show the variations of the triangle area formed by the both shoulders and the neck. Figure is color coded for the ease of visualization i.e. triangle formed by same joints in two figures are shown in similar color.culation. Presented results have been obtained using the sup- port vector machine with a radial basis function kernel (SVM - RBF kernel) method for the classification using a 10-fold cross validation technique. The parameters of the classifier were determined empirically. Proposed features (refer Sec- tion 3 for the detailed discussion on proposed features) can be categorized in the following sub-categories.1. Geometric features: distances, area and angles of trian- gles.2. Motion features: velocity and accelerations.3. Fourier features: magnitude of the spectra for different joints.4. All features: combines all features mentioned above.The best result is obtained with the combination of the different proposed features (see Table 3). It is interesting to observe the fact that, correct classification accuracy does not degrade significantly when we used only geometric features.On the two databases, SIGGRAPH and Biological Mo- tion, geometric features achieved only one percent less cor- rect classification accuracy than the best achieved results. On UCLIC database the recognition rate for the geometric fea- tures is 12% less than the combination of all features. This is probably due to the fact that this database is significantly smaller in sample size (183 actions) and presents worst case   DataBaseBest results from state-of-the-art  Our results  UCLIC79% [18]  78%  Biological50% unbias [4]  57%  SIGGRAPH–  93%    
5. CONCLUSIONWe have presented novel approach for automatic recognition of body expressions through 3D skeleton provided by mo- tion capture data. Taking inspiration from psychology do- main state of the art, we have proposed simple and represen- tative features to detect body expression from temporal 3D postures, even in complex cases: jump, run, kick etc. We have evaluated our approach on three databases that contain heterogeneous movements and expressions and obtained re- sults that exceeds state of the art. Secondly, our proposed approach runs in real time due to its computation simplicity. Thus, opening up possibilities for human-computer interac- tion applications. One such application example is new gen- eration of video games that can benefit from real time body expression analysis to adapt its content on run time.As future work, we will aim at extending the proposed method to use more semantic meta-features reinforcing the recognition of body expressions. For instance, we will seek to fit the low-level features on analytic curves in order to use the curve parameters in the classification. And, we will com- pute incrementally the low-level features to achieve continu- ous recognition over time. An improvement of the validation will be to test our approach on multi-simultaneous actions and by cross-validating our method on several databases.(/content)
(link)http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823448(/link)
(bibtex)@INPROCEEDINGS{7823448, 
author={A. Crenn and R. A. Khan and A. Meyer and S. Bouakaz}, 
booktitle={2016 International Conference on 3D Imaging (IC3D)}, 
title={Body expression recognition from animated 3D skeleton}, 
year={2016}, 
pages={1-7}, 
keywords={computer animation;emotion recognition;feature extraction;pose estimation;psychology;stereo image processing;animated 3D skeleton;body expression recognition;body postures;heterogeneous movements;human postures;pose sequence;psychology domain;visual cues;Databases;Elbow;Feature extraction;Hip;Neck;Psychology;Shoulder;3D Skeleton;Animation;Body Expression Recognition;Features Extraction}, 
doi={10.1109/IC3D.2016.7823448}, 
month={Dec},}(/bibtex)